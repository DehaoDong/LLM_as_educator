[
  [
    {
      "role": "system",
      "content": "You are a professional educator. \nYour target is to assess students' assignments based on the provided rubric (between <rubric> and <rubric> labels).\n<rubric>\nAssignment: Investigating and Analysing Ethical Risks of a Digital Engagement application \n\nThis is an INDIVIDUAL assignment on Ethical Risk Assessment for the TEU00062 module. This assessment is worth 30% of the total TEU 00062 module mark. The deadline for submission is 12.00. noon Friday 31st March 2024. \n \nLearning outcomes: Upon completion of this assignment students should:\nBe able to identify the different stakeholders and organisational roles involved in developing and using AI-driven digital engagement application, and motivations. \nBe able to identify and analyse ethical risks in a digital engagement application.\nBe equipped to identify technical and governance mitigations of these risks\n \nSteps:\n \nSelect ONE real world application that embodies one or more AI-driven digital engagement techniques you have learnt about on this course, e.g. e.g. digital engagement techniques used in one applications such as social media, newsfeeds, search, eCommerce, elearning or digital humanities., or in an application you are familiar with e.g. interactive Game, Virtual Presence (i.e. VR /AR) etc. \nIdentify the different value chain stakeholder roles involved in this application scenario and where possible the actors (i.e. companies, organisations) carrying out those roles. including those involved in: data collection; AI model development; development and use of the application. \nUse social responsibility principles (see ethic lecture week 2) to identify and analyse real or potential ethical risks of the application, including classification of the affected stakeholders (hint: see also risks classified by Latzer et al 2016 \u2013 ethics lecture week 1), how severe the risk would be and how likely it is to occur.\nIdentify one or more mitigation measures that specific value-chain stakeholders may undertake to minimise the impact of the risk you think is most harmful. These could be technical measures, organisational governance measures or measure involving additional engagement with affected individuals or groups.\n \nStructure of Assignment Report. \nStructure the Report (4000 word max excluding figures and references), under the following heading [indicative percentage of assignment mark per section]. Please provide references\n \nDescription of Application [20%]\n<describe your selected application, explaining what it does, the role AI and data collection plays in its digital engagement features, and the benefits it provides>\n\nIdentification of Stakeholder Roles involved in the application and its governance [10%]\n<describe the stakeholder roles undertaken in the development, use and governance of the application, and which organization take those roles and the benefits they derive from their involvement. Include roles that may be indirectly affected by the application. >\n\nIdentification of Ethical Risks [40%]\n<Identify the ethical risks that may be raised by your application under the social responsibility categories below (more details in ethics lecture week 2). For each risk identify which stakeholder may be affected, and assess how severe the impact would be for each stakeholder (Low, Medium, High) and the likelihood of that risk being incurred (Low, Medium, High), justifying your assessment, even if you think there is no risk under that heading.>\nHuman Rights\nLabour Practices\nThe Environment\nFair Operating Procedures\nConsumer Issues\nCommunity Involvement and Development\n \nDiscussion of Mitigations Measures for Risk [20%]\n< For one of the risks you consider more severe, describe mitigation measures that could be taken. Explain how the measure would reduce or eliminate the likelihood and impact of the risk. Explain which stakeholders would be involved, or would have to interact in implementing the measure, identifying if this is a governance measure an organization can undertake itself or in collaboration with other parties, e.g. regulators or external stakeholder..>\n\nReferences [10%]\nInclude references to source you use for you analysis, which could be academic or policy papers, blogs, information from company web site or news stories.\n\n</rubric>\nThe assignment content will be provided by user (not in system prompt)\nDo not say anything else except the marking, following the format of:\n{\n    \"feedback\": \"feedback content\",\n    \"(rubric part 1) / (rubric part 1 weight)\": score,\n    \"(rubric part 2) / (rubric part 2 weight)\": score,\n    ......other rubric parts\n    \"total score\": weighted average score of all parts\n}"
    },
    {
      "role": "user",
      "content": "Assignment: Investigating and Analysing Ethical Risks of a Digital Engagement application\nI\u2019m going to be talking about twitter as it\u2019s a digital platform with huge reach and has had big changes made to its ethics in recent years after its sale to Elon musk.\nIntroduction:\nTwitter is widely recognized as a leading social media platform worldwide, playing a pivotal role in facilitating instantaneous communication, sharing of information, and public discussions. Through its concise and influential format of brief messages, referred to as tweets, Twitter has transformed the dynamics of online interaction for individuals, groups, and governmental entities alike.\nIn this assignment, I choose Twitter as the focal point for my assignment, due to its enormity and significance in digital engagement. With over 330 million monthly active users worldwide, Twitter has become a vital tool for sharing news, expressing opinions, creating communities, and starting a public dialogue on a lot of topics ranging from politics and social issues to entertainment and sports.\nBeyond its user base, Twitter's influence extends to shaping public discourse, a way for pushing your own agenda, and home to a lot of citizen journalism. Also, Twitter serves as a breeding ground for AI-driven technologies, using machine learning algorithms from content the user has watched to create personalized recommendations, sentiment analysis, and trend detection.\nUnderstanding Twitter's role in digital engagement necessitates a comprehensive examination of its impact on society, its utilization of AI technologies, and the ethical considerations inherent in its operations. By delving into these aspects, in this assignment I aim to show the ethical Risks twitter has by being a social media which has in recent times made a point on the need for free speech and other risks which I shall explain later. Also, I will talk about Twitter as a real-world application and its implications for ethical AI-driven digital engagement.\n\nDescription of Twitter\nTwitter is a prominent American communications company headquartered in San Francisco, California, renowned for its microblogging and social networking platform of the same name. The platform enables users to post and interact with messages, known as \"tweets,\" which are limited to 280 characters. These tweets can include text, images, videos, and links, fostering real-time communication, information sharing, and social interaction.\nUsers can follow other accounts to receive their tweets on their timeline, engage with tweets through likes, retweets, and replies, and explore trending topics through hashtags. Twitter serves as a digital hub for a diverse range of content, including news updates, personal anecdotes, promotional material, and cultural commentary.\nUtilization of AI-driven Technologies\nTwitter leverages artificial intelligence (AI) across various facets of its platform to enhance user experience, optimize content delivery, and analyse user engagement. Some key areas where AI-driven technologies are utilized include:\nContent Recommendation: Twitter employs AI algorithms to recommend relevant content to users based on their interests, engagement history, and trending topics. These recommendations help users discover new accounts to follow and engage with, thereby enriching their Twitter experience. For example, Twitter's recommendation system suggests tweets, accounts, and topics tailored to individual user preferences, fostering personalized engagement.\nTrend Analysis: AI algorithms analyse vast amounts of tweet data to identify emerging trends and popular topics in real-time. Twitter's Trending Topics feature highlights the most-discussed subjects on the platform, allowing users to stay informed about current events and conversations. AI-powered trend analysis enables Twitter to adapt its content delivery and prioritize timely and relevant information for users.\nSentiment Analysis: Twitter utilizes AI-powered sentiment analysis to assess the emotional tone and sentiment expressed in tweets. By analysing language patterns and contextual cues, AI algorithms can categorize tweets as positive, negative, or neutral, providing insights into public opinion and sentiment trends. Sentiment analysis helps users gauge public perception, track brand sentiment, and identify emerging issues or crises.\nRole of AI in Enhancing Digital Engagement:\nAI plays a pivotal role in enhancing the digital engagement experience on Twitter by facilitating personalized content discovery, real-time trend monitoring, and sentiment analysis. Here are real-world examples showcasing how AI enhances digital engagement on Twitter:\nPersonalized Content Recommendations: When users sign up for Twitter or follow new accounts, the platform's AI algorithms analyse their interests, engagement history, and interactions to suggest relevant tweets, accounts, and topics tailored to their preferences. For example, if a user frequently engages with tweets about cooking and follows several food-related accounts, Twitter's recommendation system might suggest tweets from chefs, cooking enthusiasts, or trending recipes.\nReal-time Trend Identification: Twitter\u2019s Trending Topics feature showcases the most-discussed subjects on the platform in real-time. AI algorithms continuously analyse tweet data to identify emerging trends and popular conversations. For instance, during major events like sports games or award shows, Twitter's AI can quickly identify trending topics related to those events, allowing users to stay informed and engage with relevant content as it unfolds.\nBrand Sentiment Analysis: Many brands use Twitter's AI-powered sentiment analysis tools to monitor public perception and track sentiment trends regarding their products or services. For example, a company might track mentions of their brand on Twitter and analyse the sentiment expressed in those tweets (positive, negative, or neutral). By doing so, the brand can gain insights into customer satisfaction, identify areas for improvement, and respond promptly to customer feedback or concerns. Additionally, sentiment analysis can help brands gauge the effectiveness of their marketing campaigns and adjust their strategies accordingly.\nIn summary, Twitter harnesses the power of AI-driven technologies to deliver personalized content recommendations, monitor real-time trends, and analyse sentiment, thereby enhancing the digital engagement experience for users and brands alike. Through AI-enabled features and functionalities, Twitter continues to innovate and evolve its platform to meet the dynamic needs of its global user base.\n\n\nStakeholders\n\nUsers: \nCreators: Users are not only consumers but also creators of content on Twitter. They contribute by sharing tweets, posting updates, sharing multimedia content like photos and videos, and engaging in conversations.\nEngagers: Users engage with content through actions such as likes, retweets, replies, and mentions. These interactions shape the visibility and spread of tweets, influencing trends and discussions.\nCommunity Builders: Users also play a role in building communities and fostering connections on Twitter. They participate in discussions, form networks based on shared interests, and collaborate on various initiatives.\nFeedback Providers: Users provide feedback to Twitter through various channels, including direct messages, tweets, and surveys. This feedback helps Twitter understand user preferences, identify issues, and improve the platform.\n\nDevelopers: \nTool and App Creators: Developers create third-party tools, applications, and integrations that enhance the Twitter experience. These can include analytics tools, scheduling apps, social media management platforms, and more.\nAPI Users: Developers utilize Twitter's API (Application Programming Interface) to access platform data and functionality for their apps and services. They build upon Twitter's infrastructure to create innovative solutions.\nContributors to Platform Innovation: Developers contribute to platform innovation by experimenting with new features, creating plugins, and proposing enhancements. Their contributions can influence Twitter's product roadmap and user experience.\n\nAdvertisers:\nPromoters: Advertisers use Twitter's advertising platform to promote their products, services, events, or brand messaging. They create targeted ad campaigns to reach specific audience segments and achieve marketing objectives.\nRevenue Contributors: Advertisers contribute to Twitter's revenue stream through advertising spend. Their investments sustain the platform financially and support ongoing development and operations.\nContent Sponsors: Advertisers may also sponsor content or collaborate with influencers and content creators to reach wider audiences. Sponsored content adds diversity to the content ecosystem and can drive engagement.\n\nRegulators: \nLegal Compliance Enforcers: Regulators ensure that Twitter complies with relevant laws, regulations, and industry standards governing online platforms. They monitor content moderation practices, privacy policies, data protection measures, and consumer rights.\nPolicy Makers: Regulators develop policies and guidelines that govern social media platforms' operations, including Twitter. They may introduce new regulations or updates to address emerging challenges such as misinformation, hate speech, and user safety although with free speech being a huge point made by Elon Musk when he bought twitter some policies have aren\u2019t enforced as much as other.\nOversight Bodies: Regulators establish oversight mechanisms and enforcement agencies to monitor Twitter's activities and address complaints or violations. They investigate issues related to user rights, data privacy, and platform integrity.\nBy understanding the roles of these stakeholders, Twitter can better engage with them, address their needs and concerns, and foster a collaborative and sustainable ecosystem. Each stakeholder group contributes to Twitter's success and influences its development, making their engagement and collaboration crucial for the platform's evolution.\nIdentification of Ethical Risks\nThe rise of social media platforms like Twitter has revolutionized global communication, providing a forum for individuals to express themselves and engage with others. However, this newfound freedom of expression comes with ethical responsibilities and challenges. The ethical risks associated with Twitter's censorship policies, labour practices, fair operating procedures, consumer issues, and community involvement and development. \nTwitter has ushered in an era of unprecedented connectivity and communication, facilitating global interactions and the exchange of ideas on an unprecedented scale. However, the proliferation of free speech on these platforms has brought to light a myriad of ethical challenges that demand attention and consideration. As individuals exercise their right to express themselves freely, the consequences of unchecked speech become increasingly apparent, raising concerns about the spread of misinformation, hate speech, and social discord. Elon Musk, as a prominent figure in the tech industry and owner of the platform Twitter, plays a significant role in shaping the discourse and policies surrounding free speech online. His hands off approach to free speech has enabled the expansion of divisive rhetoric and harmful content, highlighting the need for responsible platform governance and ethical oversight. The ethical risks associated with Musk's approach to free speech on Twitter and the implications of his decision to fire the ethical AI team are huge while he is allowing a basic human right of free speech he is also opening opportunities for hurtful/hateful speech and complete misinformation to be passed around freely and as I said in the beginning of my essay twitter has over 330 million monthly users so it has a global impact on society this all  sheds light on the complex interplay between freedom of expression, ethical responsibility, and platform governance in the digital age.\n\nHuman Rights Implications: Twitter's censorship policies raise concerns about potential infringements on basic human rights, particularly the freedom of speech and expression. In regions where censorship is enforced, users may face limitations on their ability to participate in public discourse, impacting individual users and activists striving for social change. This poses a high-severity risk, with a moderate likelihood of occurrence due to Twitter's global reach and potential government intervention.\nLabour Practices and Ethical Oversight: The recent termination of Twitter's ML Ethics, Transparency, and Accountability team raises concerns about compromised ethical oversight and labour practices within the company. The elimination of key teams focused on ethics and transparency heightens the risk of biased algorithm development and unethical behaviour. This poses a risk, with a high likelihood given the complexity of content moderation and the pressure to balance competing interests.\nFair Operating Procedures: Twitter's censorship policies may result in inconsistencies in content moderation and unfair treatment of users based on geographical location or political influence, undermining trust in the platform. This poses a risk, with a moderate likelihood of occurrence due to varying legal requirements across borders and evolving content moderation practices.\nConsumer Issues and User Satisfaction: Before Twitters sale restrictions on content to Twitter's censorship policies may have led to dissatisfaction among users who value freedom of expression and unrestricted access to information with a high likelihood given the complex differing landscapes and competing interests that social media platforms navigate. This is where freedom of speech has equalled the field for a lot of users in countries that allow it. Earlier I talked about the risks of freedom of speech, but this is one of the positives that we have gained when using twitter.\nCommunity Involvement and Development: Twitter's new censorship policies have the potential to help community involvement and development by giving diverse voices and unlimiting the exchange of ideas. This poses a high risk, with a high likelihood of spreading harmful content and misinformation. Given the challenge of maintaining an open platform while addressing concerns about this sort of content Twitter must learn to balance the good and the bad \nIn conclusion, Twitter's censorship policies present significant ethical concerns across various social responsibility categories. From potential violations of basic human rights to compromised ethical oversight and consumer dissatisfaction, the ethical risks associated with Twitter's operations are complex. Addressing these risks requires a balanced approach that respects freedom of expression while promoting transparency, accountability, and inclusivity in the social media landscape. By addressing these challenges, stakeholders can work towards creating a more ethical and responsible online environment.\nDiscussion of Mitigations Measures for Risk\nThe termination of Twitter's ML Ethics, Transparency, and Accountability team has raised significant concerns about compromised labour practices and ethical oversight within the company. To address this risk, Twitter can implement several mitigation measures aimed at creating better ethical oversight and transparency.\nOne mitigation measure is the establishment of an independent ethics board composed of external experts, industry professionals, and representatives from civil society organizations. This board would be responsible for reviewing and evaluating Twitter's algorithmic decision-making processes, ensuring that the ethical guidelines and principles are upheld. By involving external stakeholders, such as academics, ethicists, and human rights advocates, Twitter can benefit from diverse perspectives and expertise in addressing ethical challenges. This measure would reduce the likelihood of biased algorithm behaviour by providing an independent body to oversee and review Twitter's practices.\nAnother mitigation measure is the implementation of regular audits and assessments of Twitter's algorithmic systems by independent third-party organizations and internal teams dedicated to ethical oversight. These audits would identify and mitigate potential biases, discriminatory practices, or ethical lapses in Twitter's algorithmic decision-making processes. The internal audits would fix parts of the algorithmic systems that have biases and discriminatory practices, but internal audits would have their own biases as these employees would work for twitter. The external audits could look at the working of twitter as a company fixing the unseen biases which have built up in the company while also looking at the algorithm to ensure the internal fixes don\u2019t have new unseen biases. By proactively monitoring and evaluating these processes, Twitter can identify and address ethical concerns before they escalate into significant issues. \nTwitter can prioritize employee training and education on ethical considerations and best practices in algorithmic decision-making. Twitter can cultivate a culture of ethical conduct and responsibility within the organization to develop comprehensive training programs tailored to the unique challenges of content moderation and algorithmic decision-making. By ensuring employees have the necessary knowledge and skills to identify and address ethical challenges. This measure would reduce the likelihood of unethical behaviour and biased algorithm.\nOverall, these mitigation measures aim to reduce the likelihood and impact of the compromised ethics which gave been left compromised by allowing free speech, figuring out which tweets or posts go too far, is a job twitter must do and fast as leaving such problematic tweets up even for a few moments can have substantial affect on a person or group.\n\n\n\n\nReferences \nManku, C. (2021, June 21). How Twitter Is Embracing Artificial Intelligence. LinkedIn. https://www.linkedin.com/pulse/how-twitter-embracing-artificial-intelligence-chetna-manku\nIFTTT. (n.d.). AI-Generated Tweets. IFTTT. https://ifttt.com/explore/ai-generated-tweets#:~:text=AI%2Dgenerated%20tweets%20are%20tweets,patterns%20found%20in%20the%20data.\nEmerj. (n.d.). Artificial Intelligence at Twitter. Emerj. https://emerj.com/ai-sector-overviews/artificial-intelligence-at-twitter/\nWikipedia. (n.d.). Twitter. Wikipedia. https://en.wikipedia.org/wiki/Twitter\nAll Tech is Human. (n.d.). Elon Musk's Impact on Twitter's Stakeholders: A Candidate's Perspective. All Tech is Human. https://alltechishuman.org/community-voices/blog-post-elon-musks-impact-on-twitters-stakeholders-a-candidates-perspective\nSolis, B. (2016, June 22). The Twitter Conflict: Twitter's New Algorithm and the Battle Between Shareholders and Stakeholders. Medium. https://medium.com/@briansolis/the-twitter-conflict-twitter-s-new-algorithm-and-the-battle-between-shareholders-and-stakeholders-b9b400fbd0\nHall, B., & Tait, J. (2022). Twitter's Ethical AI Team: Response to Ethical and Governance Risks. MDPI. https://www.mdpi.com/2071-1050/14/24/16769\nLaFrance, A. (2019, May 2). Inside Twitter's Attempt to Automate Away Abuse. Wired. https://www.wired.com/story/twitter-ethical-ai-team/\nPark, M., McDonald, D. W., Cha, M., & Waber, B. N. (2014). Activity Life Cycles of Mobile Applications. Journal of Medical Internet Research, 16(12), e290. https://www.jmir.org/2014/12/e290/\n(2020, January 16). The Business Ethics of Elon Musk, Tesla, Twitter, and the Tech Industry. Harvard Law School. https://hls.harvard.edu/today/the-business-ethics-of-elon-musk-tesla-twitter-and-the-tech-industry/\nSherry, B. (2022, January 6). Elon Musk's Twitter Firings: What the Ethical AI Team Told Us About Artificial Intelligence. Inc. https://www.inc.com/ben-sherry/elon-musk-twitter-firings-ethical-ai-artificial-intelligence.html\nEdubirdie. (n.d.). Social and Ethical Implications of Twitter Censorship. Edubirdie. https://edubirdie.com/examples/social-and-ethical-implications-of-twitter-censorship/\nChatgpt was used to help format the better organise the essay."
    },
    {
      "role": "assistant",
      "content": "{\n    \"feedback\": \"Your assessment of the ethical issues around twitter focusses correctly on Elon Musk's seeming prioritisation of the right to free speech over other rights, whereas in reality protection of these rights interact in complex way which companies like twitter must take responsibility for balancing.\",\n    \"description / 20\": 65,\n    \"roles / 10\": 65,\n    \"risks / 40\": 60,\n    \"mitigation / 20\": 65,\n    \"refs / 10\": 60,\n    \"total score\": 62.5\n}"
    }
  ],
  [
    {
      "role": "system",
      "content": "You are a professional educator. \nYour target is to assess students' assignments based on the provided rubric (between <rubric> and <rubric> labels).\n<rubric>\nAssignment: Investigating and Analysing Ethical Risks of a Digital Engagement application \n\nThis is an INDIVIDUAL assignment on Ethical Risk Assessment for the TEU00062 module. This assessment is worth 30% of the total TEU 00062 module mark. The deadline for submission is 12.00. noon Friday 31st March 2024. \n \nLearning outcomes: Upon completion of this assignment students should:\nBe able to identify the different stakeholders and organisational roles involved in developing and using AI-driven digital engagement application, and motivations. \nBe able to identify and analyse ethical risks in a digital engagement application.\nBe equipped to identify technical and governance mitigations of these risks\n \nSteps:\n \nSelect ONE real world application that embodies one or more AI-driven digital engagement techniques you have learnt about on this course, e.g. e.g. digital engagement techniques used in one applications such as social media, newsfeeds, search, eCommerce, elearning or digital humanities., or in an application you are familiar with e.g. interactive Game, Virtual Presence (i.e. VR /AR) etc. \nIdentify the different value chain stakeholder roles involved in this application scenario and where possible the actors (i.e. companies, organisations) carrying out those roles. including those involved in: data collection; AI model development; development and use of the application. \nUse social responsibility principles (see ethic lecture week 2) to identify and analyse real or potential ethical risks of the application, including classification of the affected stakeholders (hint: see also risks classified by Latzer et al 2016 \u2013 ethics lecture week 1), how severe the risk would be and how likely it is to occur.\nIdentify one or more mitigation measures that specific value-chain stakeholders may undertake to minimise the impact of the risk you think is most harmful. These could be technical measures, organisational governance measures or measure involving additional engagement with affected individuals or groups.\n \nStructure of Assignment Report. \nStructure the Report (4000 word max excluding figures and references), under the following heading [indicative percentage of assignment mark per section]. Please provide references\n \nDescription of Application [20%]\n<describe your selected application, explaining what it does, the role AI and data collection plays in its digital engagement features, and the benefits it provides>\n\nIdentification of Stakeholder Roles involved in the application and its governance [10%]\n<describe the stakeholder roles undertaken in the development, use and governance of the application, and which organization take those roles and the benefits they derive from their involvement. Include roles that may be indirectly affected by the application. >\n\nIdentification of Ethical Risks [40%]\n<Identify the ethical risks that may be raised by your application under the social responsibility categories below (more details in ethics lecture week 2). For each risk identify which stakeholder may be affected, and assess how severe the impact would be for each stakeholder (Low, Medium, High) and the likelihood of that risk being incurred (Low, Medium, High), justifying your assessment, even if you think there is no risk under that heading.>\nHuman Rights\nLabour Practices\nThe Environment\nFair Operating Procedures\nConsumer Issues\nCommunity Involvement and Development\n \nDiscussion of Mitigations Measures for Risk [20%]\n< For one of the risks you consider more severe, describe mitigation measures that could be taken. Explain how the measure would reduce or eliminate the likelihood and impact of the risk. Explain which stakeholders would be involved, or would have to interact in implementing the measure, identifying if this is a governance measure an organization can undertake itself or in collaboration with other parties, e.g. regulators or external stakeholder..>\n\nReferences [10%]\nInclude references to source you use for you analysis, which could be academic or policy papers, blogs, information from company web site or news stories.\n\n</rubric>\nThe assignment content will be provided by user (not in system prompt)\nDo not say anything else except the marking, following the format of:\n{\n    \"feedback\": \"feedback content\",\n    \"(rubric part 1) / (rubric part 1 weight)\": score,\n    \"(rubric part 2) / (rubric part 2 weight)\": score,\n    ......other rubric parts\n    \"total score\": weighted average score of all parts\n}"
    },
    {
      "role": "user",
      "content": "Report Investigating and Analyzing Ethical Risks of Disney+\n    \nIntroduction\nThis piece will review the streaming service Disney+ in terms of reviewing its AI-driven digital engagement techniques, identifying the different value chain stakeholders and their roles involved, identify, and analyze ethical risks that may arise, and lastly identify mitigation measures that value chain stakeholders may undertake to minimize the risk of a data and security breach. \nAI-Driven Digital Engagement Techniques\nDisney+ uses digital engagement strategies to improve user engagement and interaction through AI-driven digital engagement strategies. Through the reviewing of Disney+\u2019s social media engagement tactics on platforms such as Instagram and Twitter (Quevedo et al., 2023) reveals the important role that AI plays when tailoring one\u2019s content and recommendations in which they are exposed to and in turn increases ones appeal and engagement. Despite posting less frequently than Amazon Prime, Rahman (2021) discovered that Disney+ generates significant levels of interaction on Instagram. Ding (2022) emphasised the value of social media marketing, a tactic that Disney+ likely uses, in creating emotional ties with its users.\u00a0This personalized approach fits with the much larger trend of AI integration within digital platforms (Morley et al., 2022). \n\nDisney\u2019s digital strategy is centered on leveraging AI to build a futuristic image which appeals historical brand loyalty, as articulated in the context of their brand futurity (Pitre, 2022). This futuristic strategy captivated views and highlights Disney\u2019s dedication to embracing technology developments for increased user engagement. Through the incorporation if AI into digital processes, including digital processes, including digital pathology (McKay et al., 2022), emphasizes the significance of ethical and trustworthy considerations when implementing AI-driven solutions. \n\nIn order to preserve its leadership within the entertainment sector, Li (2023) emphasizes the strategic use of AI. Disney+ maximizes its digital transformation initiatives and adds value for consumers by implementing AI-powered service operations (Vijayakumar, 2023). \nFurthermore, by automatically responding to user inquiries and offering prompt support, AI significantly improves customer service on Disney+ (Wang et al., 2023). As a result of this automation, service operations are streamlined, which speeds up the resolution of user problems and eventually raises customer satisfaction (Wang et al., 2023).\n\nValue Chain Stakeholder Roles and Performance of Roles\nAs value-chain stakeholders in the streaming service, Disney+ creators are extremely important to the platform's success since they create and produce content that draws and keeps users interested. Disney+ producers add to the perceived value of the service by producing interesting and varied content (Deng, 2022). In addition to providing entertainment, the material they provide affects viewers' decisions to engage and subscribe (Hilvert-Bruce et al., 2018). \n\nUsers of Disney+ are vital stakeholders in the streaming service's value chain. They have a significant impact on the platform's overall satisfaction, engagement levels, and content selections, all of which have an immediate effect on the platforms subscriber retention and acquisition rates. Studies have indicated that comprehension of user behaviour and preferences is critical to the expansion of streaming services such as Disney+ (Gupta & Singharia, 2021). Disney+ must make sure that users' needs and expectations are met by including them in decision-making processes and taking their input into account (Riazi et al., 2017). \n\nNielson, an entity involved in the data collection for Disney+, collect the Disney+ users preferences, viewing patterns, and other related data to personalise content suggestions on the platform, hench improving user experience and engagement (Ding, 2022).  Platforms like Disney+ are crucially required to personalise their content recommendations in order to keep their users interested and improve their overall experience. Research has indicated that cooperative innovation with subscribers, as seen in the data gathering efforts by Nielson, may result in favourable outcomes concerning user engagement and satisfaction (Silva et al., 2021). Studies on how users interact with streaming services highlight how crucial it is to comprehend user preferences and behaviours in order to be capable of maximising long-term engagement (Teperek et al., 2017). Platforms may utilise techniques like reinforcement learning to increase user engagement over time by gathering and evaluating user data (Birch et al., 2021). \n\nAnother significant component is AI model development, in which companies such as IBM and Google work on algorithms which analyse user data to forecast preferences and optimise content delivery, resulting in a personalised entertainment experience for the Disney+ subscribers (Smadja & Muel, 2021). \n\nAccenture and Infosys are among the companies involved in the creation and usage of the Disney+ application, which focuses on building a smooth and user-friendly interface, ensuring that the application is responsive, intuitive, and offers a variety of function which in turn aids to improve a user\u2019s satisfaction (Wilfinger, 2020). \n\nIn keeping within the trend of customer centric design in digital applications, this collaboration seeks to provide a wide range of functionalities catered to user requirements and preferences allowing Disney+ to boost user engagement and retention on the application by emphasising the responsiveness and user-friendly design, which ultimately benefits Disney+ in the long term to remain competitive successful across the streaming industry (Wang et, al., 2020). \n\nThe involvement of organisations such as Accenture and Infosys in enhancing user satisfaction through Disney+ indicates towards a wider trend within the digital world shifting towards improving the users experience (Xu et al., 2020). By including features to better accessibility and usability, these organisations help achieve the objective of providing viewers with a positive and customised experience. Accenture and Infosys work alongside Disney to ensure that the application meeting the continuously changing user expectations and needs by taking advantage of their software development expertise. This in turn highlights the significance of cross industry collaborations in adapting innovation and producing high-quality digital experiences (Webster et al., 2021)\n\nAdditionally, stakeholders associated with marketing, brand management, and strategic alliances make a substantial contribution to Disney+\u2019s success. Companies such as Ogilvy and Mather focus on brand positioning, price tactics, and partnerships to attract and maintain subscribers (Lee & Cho, 2022). \n\nCompanies like Disney depend on strategic collaborations in order to increase their market reach and improve what they are offering to users. The better strategic alliances are built upon a foundation of trust, common strategic goals, and the offering of required sources where either party involved may have lacked (Whipple & Frankel, 2000). Establishing these forms of connections are highly important to the success of strategic partnerships (Zhou,2022). \n\nShafiq et al. (2014) have emphasised the utilisation of gamification aspects in streaming services as a tactic to boost user engagement and encourage continuous platform usage. Disney+ incorporates elements of gaming, such achievements, awards, and progress monitoring, into non-gaming environments in an effort to encourage and involve its members (Gibson,2015).\n\n\nOverall, Disney+\u2019s success stems from the joint effort of several stakeholders involved in creating the application, data collecting, AI model development, application development, marketing, brand management, and the users themselves. By combining all efforts of the value-chain stakeholders together, these stakeholders help to create a compelling and engaging streaming platform for Disney\u2019s consumers. \n\nEthical Risks\nOne crucial ethical danger is associated with data privacy and security. Disney+ utilizes user data for personalization and content suggestions, therefore there is a danger of data breaches or misuse, which might compromise one\u2019s user privacy. The risk impacts the user because their personal information may be exposed, resulting in serious implications such as unauthorized access to sensitive data or identity theft. Given the rising frequency of data breach in modern day and this point within the digital age, this form of danger is somewhat likely to occur (Bonnafous-Boucher & Rendtorff, 2016).\nAnother ethical risk for Disney+ involved content moderating and regulation within the platform. Disney+ may encounter difficulties when ensuring that the content in which they have provided is consistent with their ethical standards and aligned diverse societal norms. Failure in handling this risk accordingly can result in disputes, alienating specific user groups and stakeholders. The seriousness of this ethical risk varies according to the cultural background and sensitivity of the various Disney+ subscribers. Given that the application provides the service globally, the potential of content-related ethical risks would be regarded as hugely significant (Archer, 2023). \nFurthermore, using AI algorithms to propose material carries an ethical risk. Biases in AI models may result in biased outcomes, affecting user experiences and perpetuating social imbalances. This danger applies to both users and marginalized populations, which may be disproportionately harmed by biased suggestions. This danger is severe because it has the potential to propagate negative perceptions and support institutional prejudice. Given the intricacy of algorithmic bias and the difficulties in properly reducing it, the possibility of AI-related ethical issues emerging can be considered to be high (Lobato & Meese, 2014). \nIn summary, the use of social responsibility principles has enabled the identification and analysis of significant ethical issues related to Disney+. These risks include biases in AI algorithm, data privacy and security, and content moderation (Seyyed-Kalantari et al., 2021; Manuel et al., 2023). These hazards have varying degrees of severity and probability of occurrence, and they affect different stakeholders. Disney+ must proactively handle these ethical concerns if it hopes to respect moral principles, keep users\u2019 confidence, and operate with a sense of social responsibility (Manuel et al., 2023).\nMitigation Measures Minimizing the Impact of Data and Security Risks\nDeveloping a security risk mitigation plan specifically for the streaming platform is one mitigation action that specific value-chain stakeholders involved within Disney+ may undertake to reduce the impact of data and security risks. Zahid et al. (2019) suggests that this approach should concentrate on limitations including secrecy, nonrepudiation, data integrity, authentication, and data freshness. By incorporating such security measures, stakeholders can strengthen the protection of Disney+ users, prevent any unauthorized access, and mitigate any potential security breaches within the platform (Zahid et al, 2019). \nAdditionally, Support Vector Machine (SVM) regression serves as one of the latest innovations that stakeholders may use to participate in continuous security monitoring and threat identification (Alshunaifi et al., 2022). SVM cyberattack detection and mitigation enable stakeholders to proactively detect and react to security attacks in real-time, protecting the privacy and integrity of user data for the Disney+ application (Alshunaifi et al., 2022).\nIn order to educate employees and users about the best possible security practices and possible risks that may arise, stakeholders can also fund information security awareness initiatives (Chan & Mubarek, 2012). Stakeholders may foster a culture of security consciousness, reducing the possibility of errors leading to a security breach as a result of human error in where there are data vulnerabilities for the Disney+ platform by raising information of security knowledge within the organization. \nIn conclusion, specific value-chain stakeholders involved within Disney+ can effectively minimize the impact of data and security risks, safeguard user data, and uphold the applications security standards by implementing a security risk mitigation framework, utilizing cutting-edge threat detection technologies such as SVM regression, and raising information for security awareness amongst employees and users. \nConclusion \nTo overall conclude, Disney+ has strategically incorporated AI-driven techniques and digital engagement methods to improve user satisfaction and interaction\u00a0(Quevedo et al., 2023). Disney+ has developed a compelling streaming platform that accommodates a wide range of user preferences through collaborations with several stakeholders, such as data collectors, application developers, AI developers, and marketing agencies (Deng, 2022; Ding, 2022; Smadja & Muel, 2021). However, there are ethical dangers associated with the use of such cutting-edge technology, such as issues with content moderation, privacy and security of data, and biases in AI algorithms (Bonnafous-Boucher & Rendtorff, 2016; Archer, 2023; Lobato & Meese, 2014). Disney+ must take proactive measures to overcome these moral dilemmas in order to fulfil its social obligation and preserve user trust (Seyyed-Kalantari et al., 2021; Manuel et al., 2023).\n\nDisney+ stakeholders have several options for reducing the impact of data and security risks, including putting security risk mitigation frameworks into place, using advanced threat detection technologies like SVM regression, and raising user and employee awareness of information security (Zahid et al., 2019; Alshunaifi et al., 2022; Chan & Mubarek, 2012). By implementing these safeguards, Disney+ can preserve user information, mitigate against security lapses, and maintain its obligation to give its members a reliable and secure streaming experience.\n\nDisney+ is a leading streaming platform in the quickly changing digital world because it embraces technology while putting ethical issues first (Quevedo et al., 2023). Disney+ can sustain its standing as a prominent participant in the streaming business and develop trust and loyalty among its user base by persisting in its collaboration with stakeholders, taking proactive measures to address ethical issues, and giving priority to user privacy and security.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBibliography \n\nAlshunaifi, S. et al. (2022). Cyber-Attack Detection and Mitigation Using SVM for 5g Network. Intelligent Automation & Amp; Soft Computing, 31(1), 13-28. https://doi/10.32604/iasc.2022.019121. \nArcher C. (2023). Play is a Child\u2019s Work (on Instagram), M/C Journal, 26(2). https://doi.org/10.5204/mcj.2952. \nBirch et al. (2021). Data as Asset? The Measurement, Governance, and Valuation of Digital Personal Data by Big Tech. Big Data & Society, 8(1_, 205395172110173. https://doi.org/10.1177/20539517211017308. \nBonnafous-Boucher,M. and Rendtorff, J. (2016). Stakeholder Theory and Ethics., 67-78. \nhttps://doi.org/10.1007/978-3-319-44356-0_5. \nDeng, Z. (2022). Analysis of Disney\u2019s Marketing Strategy and Competitiveness \u2013 Based on 4ps Theory and SWOT Analysis. BCP Business & Management, 34, 449-445. \nhttps://doi.org/10.54691/bcpbm.v34i.3048. \nDing, Y. (2022). The Effect of Social Media Marketing on Customer Relationship Management at Disney in USA. Highlights in Business Economics and Management, 2, 348-352.\n https://doi.org/10.54097/hbem.v2i.2385. \nHilvert-Bruce et al. (2018). Social Motivations of Live-Streaming Viewer Engagement on Twitch. Computers in Human Behaviour, 84, 58-67. https://doi.org.10.1016/j.chb.2018.02.013. \nGibson, R, (2015). Four Strategies for Remote Workforce Training, Development, and Certification., 770-785. \nhttps://doi.org/10.4018/978-1-4666-8200-9.ch038. \nGupta, G. and Singharia, K. (2021). Consumption of OTT Media Streaming in COVID-19 Lockdown: Insights from pls Analysis. Vision Journal of Business Perspective, 25(1), 36-34. \nhttps://doi.org/10/1177/0972262921989118. \nLee, J. and Cho, J. (2022). The Effects of a Disney Masstige Brand Collaboration on Perceptions of Brand Luxury: Vertical Versus Horizontal Product Line Extension Strategies. Journal of Product & Brand Management, 32(1), 138-156. https://doi.org/10.1108/jpbm-01-2022-3833. \nLi, H. (2023). Disney Market Positioning Analysis and Competitive Strategy Research., 301-313. \nhttps://doi.org/10.2991/978-2-494069-97-8_38. \nLobato, R. and Meese, J. (2014). Kittens All the Way Down: Cute in Context. M/C Journal, 17(2). \nhttps://doi.org/10.5204/mcj.807. \nManuel, A. et al. (2023). Ethical Assessments and Mitigation Strategies for Biases in AI-Systems Used During the COVID-19 Pandemic. Big Data & Society, 10(1). https://doi.org/10.1177/20539517231179199. \nMcKay et al., (2022). The Ethical Challenges of Artificial Intelligence-Driven Digital Pathology. The Journal of Pathology Clinical Research, 8(3), 209-216. https://doi.org/10.1002/cjp2.263. \nMorley et al., (2022). Governing data and artificial intelligence for health care: Developing an International Understanding. Jmir Formative Research, 6(1), e31623.\n https://doi.org/10.2196/31623 \nPitre, J. (2022). The Magical Work of Brand Futurity: The Mythmaking of Disney+. Television & New Media, 24(6), 712-729. https://doi.org/10.1177/15272764221128923. \nQuevedo et al, (2023). \u201cCOVID-19 Lockdown and Disney+ strategy on social networks on its launch during the State of alarm in Spain.\u201d Communication & Society, 36(2), 151- 169.\nhttp://doi.org/10.15581/003.36.2.151-169. \nRahman, K. (2021). Driving Engagement on Instagram: A Comparative Analysis of Amazon Prime and Disney+. Seisense Business Review, 1(3), 1-11. \nhttps://doi.org/10.33215/sbr.v1i3.676. \nRiazi et al. (2017). Canadian 24-hour Movement Guidelines for Early Years (0-4years): Exploring the Perceptions of Stakeholders and End Users Regarding Their Acceptability, Barriers to Uptake, and Dissemination. BMC Public Health, 17(S5). \nhttps://doi.org/10.1186/s12889-017-4853-z. \nSeyyed-Kalantari, L. et al. (2021). Underdiagnosis Bias of Artificial Intelligence Algorithms Applied to Chest Radiographs in Under-Served Patient Populations. Nature Medicine, 27(12), 2176-2182. \nhttps://doi.org/10.1038/s41591-021-01595-0. \nShafiq et al. (2014). Understanding the Impact of Network Dynamics on Mobile Video User Engagement. ACM Sigmetrics Performance Evaluation Review, 42(1), 367-379.\nhttps://doi.org/10.1145/2637364.2591975. \nSilva et al. (2021). Mobile vs. Non-Mobile Live-streaming: A Comparative Analysis of Users Engagement and Interruption Using Big Data from Large CDN Perspectives. Sensors, 21(16), 5616. \nhttps://doi.org/10.3390/s21165616. \nSmadja, T. and Muel, F. (2021). Analysis of EU Legume Value Chains from the h2020 Legvalue Project: What insights for Organic Value Chains? OCL, 28,15. https://doi.org/10.1051/ocl/2021005. \nTeperek et al. (2017). Is Democracy the Right System? Collaborative Approaches to Building an Engaged RDM Community.. \nhttps://doi.org/10.1101/103895. \nVijayakumar, H. (2023). Business Value Impact of AI-Powered Service Operations (aiserviceops).. \nhttps://doi.org/10.5121/csit.2023.130502. \nWang et al., (2020) Lewoa: A Universal Leaf-Woof Classification Method to Facilitate the 3D Modelling of Large tropical Trees Using Terrestrial Lidar. Methods in Ecology and Evolution, 11(3), 376-389. https://doi.org/10.1111/2041-210x.13342. \nWang et al., (2021). Understanding the Implications of Artificial Intelligence on Field Service Operations: A Case Study of BT. Production & Control, 33(16), 1591-1607. https://doi.org/10.1080/09537287.2021.1882694. \nWebster et al., (2021). Review: Posed vs. Genuine Facial Emotion Recognition and Expression in Autism and Implications for Invention. Frontiers in Psychology, 12. \nhttps://doi.org/10.3389/fpsy.2021.653112. \nWhipple J. and Frankel, R. (2000). Strategic Alliance Success Factors. Journal of Supply Chain Management, 36(2), 21-28. \nhttps://doi.org.10.1111/j.1745-493x.2000.tb00248.x. \nWilfinger, D. (2020). Digital Transformation in the Value Chain \u2013 Effects on Major Stakeholders.. \nhttps://doi.org/10.33422/6thomeaconf.2020.09.205. \nXu et al., (2020). RNA Structures in Alternative Splicing and Back-Splicing. Wiley Interdisciplinary Reviews \u2013 RNA, 12(1). \nhttps://doi.org/10.1002/wrna.1626. \nZahid, M. et al. (2019). A Security Risk Mitigation Framework for Cyber Physical Systems. Journal of Software Evolution and Process, 32(2). https://doi.org/10.1002/smr.2219. \nZhou, Y. (2022). Research on the Marketing Strategy Under the Castle \u2013 taking Disney as an Example., 53-57. \nhttps://doi.org/10.2991/978-94-6463-052-7_7."
    },
    {
      "role": "assistant",
      "content": "{\n    \"feedback\": \"Well researched summary of application and the stakeholders. However the risks could have more comprehensively addressed the needs of different groups, e.g. children vs adult and the role of censorship.\",\n    \"description / 20\": 65,\n    \"roles / 10\": 55,\n    \"risks / 40\": 60,\n    \"mitigation / 20\": 65,\n    \"refs / 10\": 75,\n    \"total score\": 63.0\n}"
    }
  ],
  [
    {
      "role": "system",
      "content": "You are a professional educator. \nYour target is to assess students' assignments based on the provided rubric (between <rubric> and <rubric> labels).\n<rubric>\nAssignment: Investigating and Analysing Ethical Risks of a Digital Engagement application \n\nThis is an INDIVIDUAL assignment on Ethical Risk Assessment for the TEU00062 module. This assessment is worth 30% of the total TEU 00062 module mark. The deadline for submission is 12.00. noon Friday 31st March 2024. \n \nLearning outcomes: Upon completion of this assignment students should:\nBe able to identify the different stakeholders and organisational roles involved in developing and using AI-driven digital engagement application, and motivations. \nBe able to identify and analyse ethical risks in a digital engagement application.\nBe equipped to identify technical and governance mitigations of these risks\n \nSteps:\n \nSelect ONE real world application that embodies one or more AI-driven digital engagement techniques you have learnt about on this course, e.g. e.g. digital engagement techniques used in one applications such as social media, newsfeeds, search, eCommerce, elearning or digital humanities., or in an application you are familiar with e.g. interactive Game, Virtual Presence (i.e. VR /AR) etc. \nIdentify the different value chain stakeholder roles involved in this application scenario and where possible the actors (i.e. companies, organisations) carrying out those roles. including those involved in: data collection; AI model development; development and use of the application. \nUse social responsibility principles (see ethic lecture week 2) to identify and analyse real or potential ethical risks of the application, including classification of the affected stakeholders (hint: see also risks classified by Latzer et al 2016 \u2013 ethics lecture week 1), how severe the risk would be and how likely it is to occur.\nIdentify one or more mitigation measures that specific value-chain stakeholders may undertake to minimise the impact of the risk you think is most harmful. These could be technical measures, organisational governance measures or measure involving additional engagement with affected individuals or groups.\n \nStructure of Assignment Report. \nStructure the Report (4000 word max excluding figures and references), under the following heading [indicative percentage of assignment mark per section]. Please provide references\n \nDescription of Application [20%]\n<describe your selected application, explaining what it does, the role AI and data collection plays in its digital engagement features, and the benefits it provides>\n\nIdentification of Stakeholder Roles involved in the application and its governance [10%]\n<describe the stakeholder roles undertaken in the development, use and governance of the application, and which organization take those roles and the benefits they derive from their involvement. Include roles that may be indirectly affected by the application. >\n\nIdentification of Ethical Risks [40%]\n<Identify the ethical risks that may be raised by your application under the social responsibility categories below (more details in ethics lecture week 2). For each risk identify which stakeholder may be affected, and assess how severe the impact would be for each stakeholder (Low, Medium, High) and the likelihood of that risk being incurred (Low, Medium, High), justifying your assessment, even if you think there is no risk under that heading.>\nHuman Rights\nLabour Practices\nThe Environment\nFair Operating Procedures\nConsumer Issues\nCommunity Involvement and Development\n \nDiscussion of Mitigations Measures for Risk [20%]\n< For one of the risks you consider more severe, describe mitigation measures that could be taken. Explain how the measure would reduce or eliminate the likelihood and impact of the risk. Explain which stakeholders would be involved, or would have to interact in implementing the measure, identifying if this is a governance measure an organization can undertake itself or in collaboration with other parties, e.g. regulators or external stakeholder..>\n\nReferences [10%]\nInclude references to source you use for you analysis, which could be academic or policy papers, blogs, information from company web site or news stories.\n\n</rubric>\nThe assignment content will be provided by user (not in system prompt)\nDo not say anything else except the marking, following the format of:\n{\n    \"feedback\": \"feedback content\",\n    \"(rubric part 1) / (rubric part 1 weight)\": score,\n    \"(rubric part 2) / (rubric part 2 weight)\": score,\n    ......other rubric parts\n    \"total score\": weighted average score of all parts\n}"
    },
    {
      "role": "user",
      "content": "In the digital age, applications utilising artificial intelligence (A.I.) to enhance user engagement are becoming increasingly prevalent. One such application, Duolingo, exemplifies how A.I.-driven techniques can revolutionize language learning. This essay explores Duolingo's application, identifies the roles of various stakeholders involved, assesses potential ethical risks, and discusses mitigation measures to address these risks.\n\nDuolingo is a language-based learning application that has re-invented the way in which people learn languages. It was invented by Luis von Ahn and his PhD student Severin Hacker in 2011, out of the Carnegie Mellon University, Pittsburgh, Pennsylvania, United States. Duolingo offers over 45 different languages currently, whilst also offering Math and Music lessons too (Blanco, 2022). \n\nWith over 500 million registered users, it is one of the most popular education apps in the world.  Its goal is \u201cto develop the best education in the world and make it universally available\u201d - (Duolingo Support, 2022). It achieves this by offering a user-friendly experience, gamification of learning and personalised learning experience powered by artificial intelligence.  \n\nDuolingo\u2019s business is based on a Freemium model, much like other large platforms such as Spotify. The user can use the app for free with a few advertisements, or they can pay for additional features alongside the removal of ads. The app breaks the languages into small 2-minute-long lessons, that cover a vast range of topics from basic vocabulary to complex translation of dialogues. The user can progress through the lessons at their own pace. If the topic is too simple for you, you can even take a small test to skip to the next topic. Users earn XP (experience points), unlock new levels that increase in difficulty, operating on a 5-life system (5 mistakes can be made) gamifies the studying experience and allows friends to compete. Through gamification Duolingo can keep users engaged and motivated to continue learning every day. \nA.I.  plays a large role in the success of Duolingo. Algorithms are employed to customise the learning experience for each user. By analysing a user\u2019s performance on their lessons, the system can identify their strong and weak points, and adjusting the difficulty of lessons accordingly. This approach keeps the level of difficulty appropriate to the user, ensuring no lesson is too difficult or too easy, keeping the user in a goldilocks zone. Duolingo utilizes A.I. to space the content for revision as well, maximizing the user\u2019s ability to retain the vocabulary.\n\n\n\nThe ecosystem around Duolingo involves a diverse array of stakeholders, each playing a crucial role in the application's development, use, and governance. These stakeholders include developers, users (learners), language experts, investors, educational institutions, and even governments, all of whom derive specific benefits from their involvement with the application.\n\nThe developers and engineers who design, build, and maintain the application. This group includes software engineers, data scientists, and A.I. specialists who work directly for Duolingo Inc., the organisation behind the app. Their role is instrumental in implementing A.I. technologies and ensuring the platform remains engaging, effective, and user-friendly. The benefit to these stakeholders is professional development and compensation, as well as the fulfilment that comes from contributing to a globally impactful educational tool.\n\nUsers of Duolingo, are perhaps the most direct stakeholders. They use the app to learn new languages, improve their skills, and engage with different cultures. The benefit for users is the acquisition of valuable language skills at no or low cost, enhanced employability, and personal enrichment. The users as stakeholders are the source of data that the A.I. program is collecting. Collecting various data points related to their lessons, is gathered to help improve the difficulty of the lesson and the user\u2019s overall progress.\nLinguists and language educators contribute to the development of course content, ensuring that linguistic information is accurate and is following good methods of teaching. While some of these experts may be in-house, Duolingo also collaborates with universities and language institutes. The benefit to these stakeholders includes professional recognition, research opportunities, and the chance to influence how languages are taught on a massive scale.  Linguistics interact with the A.I. program to make sure the lessons are correct, and to help provide cultural context, even though A.I. is a powerful tool, it has shortcomings in the areas of human interpretation and cultural influences.\nVenture capitalists and shareholders who invest in Duolingo Inc. are crucial for providing the financial resources needed for research, development, and expansion. Their benefit is financial return on investment, particularly as Duolingo continues to grow and potentially increases its market value. The shareholders\u2019 investment would\u2019ve enabled Duolingo to acquire the necessary finances to obtain successful A.I resources, as they can have a large initial cost, however the use of A.I. in Duolingo has given it the cutting edge against competitors as it can output lessons at a rapid pace. \n\nSchools and universities increasingly incorporate Duolingo into their curricula as a supplementary tool for language learning. Many teachers implement the classroom function into their methods of teaching a language. In the classroom section on Duolingo a teacher can create a class, where which they can view the progress for each student. This collection of data is completed by A.I., and it enables the teacher to identify the weak points of a student\u2019s vocabulary, enabling the teachers to create specialised learning plans for each student. The seamless integration of A.I into the classroom via Duolingo, has helped revolutionise the way in which languages have been used in the classroom.\n\nThe tourism industry and businesses who trade internationally indirectly benefit from Duolingo. The increase in multilingual people in a community boosts tourism and international business trading as it helps bridge the gap, the language barrier between locals and international people visiting. International communities also indirectly benefit from Duolingo, due to the education of a language may spark interest into someone wanting to learn more about a countries culture, i.e. trying out different cuisines in restaurants.\n\nThe governance and development of Duolingo involve a complex network of stakeholders, each contributing/ receiving unique benefits to/from the application. This collaborative ecosystem not only fuels the app's continuous improvement and expansion but also amplifies its impact on global language education.\nHuman Rights\nPrivacy and Data Protection\nHuman Rights has been the centre of ethical practices for many years and with the digital age there is no change. It comes in the form of Privacy and Data Protection of the user. Our personal information is valuable by many companies who use targeted ads. For this reason, privacy and data protection is of utmost importance, to protect users from harmful content. There is a high risk in this section as Duolingo collects a large range of data from their users. Some articles even dubbing Duolingo as, \u2018The Champion of Tracking,\u2019 this is due to it collecting almost 4x the average amount of data points compared to its competitors (Lapientye, 2023).  This data is used to create more personalised lessons for a more enjoyable experience however this data must be stored. Duolingo on their support page have stated they do not sell your data (Duolingo Support, 2024), but data has been leaked through hackers before, with approximately 2.6 million users\u2019 data been sold to phishing companies (Mascellino, 2023). Whilst the impact is rated high, there is only a medium level risk associated as Duolingo employs strict security measures and is constantly investing in its cybersecurity.\nDuolingo operates on an inclusive principle of providing education of languages to all who seek it (Duolingo, 2021). Here they enable people to practice their right to education, as Duolingo offers their educational services for free. \n\nLabour Practices\nThe tech industry is known for its demanding work culture, which may lead to burnout and dissatisfaction of employees. There is a medium level impact on labour practices for employees, as they are susceptible to burn out. There is not a significant amount of information available online for their labour practices, however due to their competitive positioning in the language education market, it would suggest the company is also keeping up with their competitors in this section too. \nThe rise of freelance workers in the tech industry has brought many opportunities for work at companies such as Duolingo, with \u2018Glassdoor\u2019 suggesting a freelance translator would be paid up to 45 thousand dollar per annum (N.D., 2021). Even though freelance culture has been seen to improve flexibility, job security is not the same. The freelance workers are not considered employees, but more so contractors.\n\nCarbon Footprint of A.I. Training\nA.I. and machine learning, integral to Duolingo\u2019s adaptive learning model, require significant computational power, contributing to the company\u2019s carbon footprint. This risk, with a medium impact and high likelihood, affects all stakeholders involved due to the broader environmental implications of global warming. By optimizing data centre efficiency through investing into newer technology that can utilise power more efficiently, using renewable energy sources, and investing in carbon offset projects, Duolingo can work towards reducing its carbon impact potentially becoming a carbon neutral company. In their company report to the shareholders in 2023, Duolingo had reported of offsetting carbon through (Terrapass) 2022 and Pachama (2023), they had also reported a donation of 8000lbs of food that would\u2019ve spoiled in 2022, aiming to reduce their waste and carbon output (Von Ahn, 2023).\n\nFair Operating Procedures\nIntellectual Property Rights\nDeveloping language courses that respect intellectual property rights is crucial, especially for less common languages or specific cultural content. The risk of infringing upon **Language Experts and Cultural Groups**\u2019 intellectual property is low in both impact and likelihood, given Duolingo\u2019s reliance on licensed content and public domain resources. Continued diligence in content sourcing and collaboration with cultural representatives is key to avoiding potential infringements.\n\nTransparency in Algorithms\nThe algorithms that adapt learning paths for users are central to Duolingo\u2019s success but also a source of potential ethical concern. The impact on Users is low, given the primarily educational context, but the medium likelihood of concern arises from increasing demands for algorithmic transparency across tech platforms. Enhancing transparency regarding how learning paths are determined could bolster user trust and facilitate more personalized learning experiences.\n\nConsumer Issues\nAccessibility\nEnsuring the Duolingo app and website are accessible to individuals with disabilities is not just a legal requirement but an ethical obligation. The high impact on Users with Disabilities reflects the significance of accessibility in enabling equitable learning opportunities. The risk likelihood is low, thanks to Duolingo\u2019s ongoing efforts to meet and exceed accessibility standards, yet continuous improvement and user feedback are vital to ensure accessibility remains a priority. Duolingo ensures they accommodate disabilities by allowing the user to submit an extra accommodation form, flagging the account to be in receipt of different accommodation, like extra time per question (N.D., 2024). \n\nAccuracy and Effectiveness of Learning Material\nThe possibility for misinformation or ineffective teaching methods presents a medium impact and likelihood risk to users. Inaccurate or culturally insensitive content can undermine learning outcomes and user trust. Engaging language experts and continuously updating content based on the latest development in teaching practices and user feedback can mitigate this risk, ensuring that Duolingo remains a reliable and effective tool for language learners.\n\nCultural Sensitivity and Representation\nDuolingo\u2019s broad language offering includes courses in endangered or minority languages, highlighting the platform's role in cultural preservation. However, the medium impact and likelihood of inadequately representing the cultural diversity within these languages pose a risk to users and cultural groups. Collaborating with native speakers and cultural experts in course development is essential to ensure content is culturally sensitive and inclusive. Through their relentless work alongside the native speakers, Duolingo is helping revive certain languages, for example ever since Irish has been added to Duolingo, there a been an increase of speakers leading to Ireland\u2019s president publicly thanking the company\u2019s involvement with the local language (Wen, 2019). \n\nEconomic Impact on Traditional Education Providers\nThe disruption caused by Duolingo\u2019s free model can have significant economic implications for traditional language schools and educators, marked by a high impact and medium likelihood. Language learning institutions may see a decline in participation for paid courses as learners opt for Duolingo\u2019s free alternative. However, I believe this is more likely to be of lower impact, as Duolingo is not meant to be a replacement for classes but more as a supplementary tool to learn the language. Transforming this challenge into an opportunity involves Duolingo partnering with educational institutions to supplement traditional teaching methods, as previously mentioned through the Duolingo classroom function.\n\nIn addressing these ethical risks, Duolingo must navigate a complex landscape of stakeholder interests, technological challenges, and social responsibilities. Proactive measures, including robust privacy protections, fair labour practices, environmental sustainability efforts, and continuous engagement with the educational community, are essential. By maintaining high ethical standards, Duolingo can continue to advance its mission of making language learning accessible and enjoyable for all.\n\n\nThe ethical issue of most in importance in the analysis of Duolingo\u2019s landscape is the privacy and data protection of users. This risk holds significant implications due to the high sensitivity of personal information and the potential consequences of data breaches. The mitigation of this risk involves comprehensive measures that address both the likelihood and impact of data privacy and security incidents. The stakeholders involved in these measures include Duolingo, Users, Regulators, and potentially Third-party Security Experts. The implementation of these measures can be both an internal governance action by Duolingo and a collaborative effort with external parties.\n\nMitigation Measures\nSafeguarding of user data and privacy is of utmost importance for applications like Duolingo, which gather significant amount of personal information to provide educational services. The effectiveness of Duolingo's data protection strategies relies on a comprehensive approach that interlinks technological innovations, adherence to legal standards, and active user participation. \n\nDuolingo's approach to keeping user data safe is built on several key strategies that work together to protect against unauthorised access and data breaches. At the heart of these strategies is the use of advanced encryption techniques. This means that Duolingo makes sure any data it stores or sends is coded in such a way that only authorised people can read it. If someone tries to hack this data, they won't be able to understand it. This step greatly reduces the danger of data breaches by making stolen data almost worthless to thieves. Developers and security experts from outside the company play a vital role in setting up and managing these encryption methods, highlighting the need for skilled professionals and external checks to keep data safe. These encryption methods are tested regularly through penetration tests from third party companies, to ensure they are working and to a high standard (Baig, 2023).\n\nTo strengthen its defences even further, Duolingo regularly checks its systems for weaknesses that hackers could take advantage of and amends these issues right away. The teamwork between Duolingo's in-house developers and external security experts is essential. External security brings an unbiased view that helps find and solve security problems more effectively. This careful checking and fixing of vulnerabilities make it much harder for hackers to break in.\n\n\nBeing open about how user data is used is crucial for building trust and following the rules. Duolingo is committed to making its data use policies easy to understand and making sure users agree to how their data is collected, used, and shared. This transparency not only keeps users informed but also gets them involved in the privacy process, with the legal team, management, and users themselves all playing significant roles.\n\nKeeping in line with international data protection laws, like the GDPR in Europe and the CCPA in California, is key part of Duolingo's privacy strategy. This compliance helps Duolingo meet strict privacy standards, avoiding legal and financial issues. The legal team, regulatory authorities, and management work together to ensure Duolingo's operations stay within these laws, protecting user privacy.\n\nTeaching users about data privacy and security is also vital. Duolingo gives users the tools and information they need to keep their personal information safe. This effort shows how both users and developers share the responsibility for data protection.\n\nHaving an up-to-date plan for responding to data breaches is essential. This plan means Duolingo can quickly deal with any data breaches, reducing harm and quickly rebuilding users' trust. Management, developers, the legal team, and external security experts all contribute to this plan, showing the importance of teamwork in responding to data breaches.\nIn conclusion, Duolingo's integration of A.I. into language learning highlights the transformative potential of technology in education. By tailoring lessons to individual user needs, gamifying the learning experience, and providing accessible education to millions worldwide, Duolingo has redefined the approach to language acquisition. The application operates within a complex ecosystem involving a diverse group of stakeholders, including developers, users, educational institutions, and investors, each contributing to and benefiting from Duolingo's success.\n\nHowever, the deployment of A.I. and the collection of vast amounts of user data introduce significant ethical considerations, particularly regarding privacy and data protection. The potential risks associated with these practices, including data breaches, the carbon footprint of A.I. training, labour practices, and the impact on traditional education providers, require a proactive approach towards its mitigation.\n\nDuolingo's efforts to safeguard user data through encryption, regular security audits, transparent data use policies, compliance with international data protection laws, user education on privacy, and a robust incident response plan illustrate a commitment to addressing these ethical concerns. By continuously evaluating and improving these measures, Duolingo not only enhances the security and privacy of its platform but also sets a precedent for ethical responsibility in the tech industry.\n\nAs Duolingo continues to innovate and expand its offerings, the application's ability to adapt to ethical considerations will be crucial in sustaining its mission to make high-quality education universally available. The intersection of technology, education, and ethical responsibility in Duolingo's model provides valuable insights into the potential for A.I. to enhance learning experiences while highlighting the and responsibility in managing the accompanying challenges.\nReference:\n\nBlanco, C. (2022) Duolingo 101: How to learn a language on duolingo, Duolingo Blog. Available at: https://blog.duolingo.com/duolingo-101-how-to-learn-a-language-on-duolingo/#:~:text=We%20offer%20courses%20in%2045,education%20possible%2C%20all%20for%20free. (Accessed: 31 March 2024). \nDuolingo (ed.) (2021) About Us, University Programs Duolingo. Available at: https://university.duolingo.com/our-story-1 (Accessed: 30 March 2024). \nDuolingo Support (2022) What is duolingo?, Duolingo Support. Available at: https://support.duolingo.com/hc/en-us/articles/204829090-What-is-Duolingo (Accessed: 31 March 2024). \nDuolingo Support (ed.) (2024) Data Privacy & Duolingo: Welcome to the duolingo data vault \u2013 ..., Duolingo Support. Available at: https://support.duolingo.com/hc/en-us/articles/360004256711-Data-Privacy-Duolingo-Welcome-to-the-Duolingo-Data-Vault (Accessed: 30 March 2024). \nLapientye, J. (ed.) (2023) Duolingo crowned as \u201cUndisputed champion of tracking\u201d, Cybernews. Available at: https://cybernews.com/news/duolingo-champion-of-tracking (Accessed: 30 March 2024). \nMascellino, A. (2023) Data of 2.6M duolingo users leaked on Hacking Forum, Infosecurity Magazine. Available at: https://www.infosecurity-magazine.com/news/data-26m-duolingo-users-leaked/ (Accessed: 30 March 2024). \nN.D. (2021) Duolingo Freelance Translator Salaries, Glassdoor. Available at: https://www.glassdoor.ie/Salary/Duolingo-Freelance-Translator-Salaries-E629348_D_KO9,29.htm. \nN.D. (2024) Duolingo English test, Duolingo English Test. Available at: https://englishtest.duolingo.com/test_takers/accommodations (Accessed: 31 March 2024). \nVon Ahn, L. (ed.) (2023) Form DEF 14A for Duolingo Inc filed 04/28/2023, Proxy Statement Pursuant to Section 14(a) of the Securities Exchange Act of 1934. Available at: https://investors.duolingo.com/static-files/f0e6a687-e63a-40b2-8bdf-4eb3d727e8db (Accessed: 30 March 2024). \nWen, A. (2019) Duolingo preserves endangered languages, Business Today Online Journal. Available at: https://journal.businesstoday.org/bt-online/2019/duolingo-preserves-endangered-languages (Accessed: 31 March 2024)."
    },
    {
      "role": "assistant",
      "content": "{\n    \"feedback\": \"A good analysis of Duolingo and the data protection risks. Did you consider whether it support for accessibility might reveal if a user had learning difficulties, and if this was flagged and compensated for this would also represent a class of sensitive information that needed particular protections?\",\n    \"description / 20\": 67,\n    \"roles / 10\": 67,\n    \"risks / 40\": 65,\n    \"mitigation / 20\": 65,\n    \"refs / 10\": 60,\n    \"total score\": 65.1\n}"
    }
  ],
  [
    {
      "role": "system",
      "content": "You are a professional educator. \nYour target is to assess students' assignments based on the provided rubric (between <rubric> and <rubric> labels).\n<rubric>\nAssignment: Investigating and Analysing Ethical Risks of a Digital Engagement application \n\nThis is an INDIVIDUAL assignment on Ethical Risk Assessment for the TEU00062 module. This assessment is worth 30% of the total TEU 00062 module mark. The deadline for submission is 12.00. noon Friday 31st March 2024. \n \nLearning outcomes: Upon completion of this assignment students should:\nBe able to identify the different stakeholders and organisational roles involved in developing and using AI-driven digital engagement application, and motivations. \nBe able to identify and analyse ethical risks in a digital engagement application.\nBe equipped to identify technical and governance mitigations of these risks\n \nSteps:\n \nSelect ONE real world application that embodies one or more AI-driven digital engagement techniques you have learnt about on this course, e.g. e.g. digital engagement techniques used in one applications such as social media, newsfeeds, search, eCommerce, elearning or digital humanities., or in an application you are familiar with e.g. interactive Game, Virtual Presence (i.e. VR /AR) etc. \nIdentify the different value chain stakeholder roles involved in this application scenario and where possible the actors (i.e. companies, organisations) carrying out those roles. including those involved in: data collection; AI model development; development and use of the application. \nUse social responsibility principles (see ethic lecture week 2) to identify and analyse real or potential ethical risks of the application, including classification of the affected stakeholders (hint: see also risks classified by Latzer et al 2016 \u2013 ethics lecture week 1), how severe the risk would be and how likely it is to occur.\nIdentify one or more mitigation measures that specific value-chain stakeholders may undertake to minimise the impact of the risk you think is most harmful. These could be technical measures, organisational governance measures or measure involving additional engagement with affected individuals or groups.\n \nStructure of Assignment Report. \nStructure the Report (4000 word max excluding figures and references), under the following heading [indicative percentage of assignment mark per section]. Please provide references\n \nDescription of Application [20%]\n<describe your selected application, explaining what it does, the role AI and data collection plays in its digital engagement features, and the benefits it provides>\n\nIdentification of Stakeholder Roles involved in the application and its governance [10%]\n<describe the stakeholder roles undertaken in the development, use and governance of the application, and which organization take those roles and the benefits they derive from their involvement. Include roles that may be indirectly affected by the application. >\n\nIdentification of Ethical Risks [40%]\n<Identify the ethical risks that may be raised by your application under the social responsibility categories below (more details in ethics lecture week 2). For each risk identify which stakeholder may be affected, and assess how severe the impact would be for each stakeholder (Low, Medium, High) and the likelihood of that risk being incurred (Low, Medium, High), justifying your assessment, even if you think there is no risk under that heading.>\nHuman Rights\nLabour Practices\nThe Environment\nFair Operating Procedures\nConsumer Issues\nCommunity Involvement and Development\n \nDiscussion of Mitigations Measures for Risk [20%]\n< For one of the risks you consider more severe, describe mitigation measures that could be taken. Explain how the measure would reduce or eliminate the likelihood and impact of the risk. Explain which stakeholders would be involved, or would have to interact in implementing the measure, identifying if this is a governance measure an organization can undertake itself or in collaboration with other parties, e.g. regulators or external stakeholder..>\n\nReferences [10%]\nInclude references to source you use for you analysis, which could be academic or policy papers, blogs, information from company web site or news stories.\n\n</rubric>\nThe assignment content will be provided by user (not in system prompt)\nDo not say anything else except the marking, following the format of:\n{\n    \"feedback\": \"feedback content\",\n    \"(rubric part 1) / (rubric part 1 weight)\": score,\n    \"(rubric part 2) / (rubric part 2 weight)\": score,\n    ......other rubric parts\n    \"total score\": weighted average score of all parts\n}"
    },
    {
      "role": "user",
      "content": "Netflix                             \n(section 1)\nNetflix is a popular streaming platform that offers a wide range of movies, tv shows, documentaries, and original content to subscribers worldwide. Netflix employs around 13,000 people. Netflix provides three streaming plans for its customers. The basic plan costs 8.99/month and offers good video quality in HD(720) and allows content to be downloaded on a single device. The standard plan costs 14.99/month and offers great video quality in full HD (1080p) and allows downloads on up to 2 devices. The premium plan offers Netflix\u2019s best video quality in ultra HD (4K) and HDR, provides spatial audio and allows customers to download on 6 devices. All subscription types allow customers to watch advert free on any phone, tablet, computer or TV. Users can access content on demand, stream across various devices and receive personalized recommendations based on their viewing history and preferences. Netflix has over 260million active worldwide subscribers. The unique selling point for Netflix is its \u201cNetflix originals\u201d, content that can only be streamed on its platform. There are over 2400 original titles on Netflix. Netflix is worth $271billion.\n\n\nThe role of AI in Netflix\u2019s engagement features\nOne of the most prominent features of Netflix is its personalized recommendation system which is generated by AI. AI algorithms analyse vast amounts of user data including viewing history, genre preferences, ratings, time of day and browsing behaviour. All of this information is gathered by Netflix to generate personalised recommendations for each individual user. These recommendations are displayed on the homepage, in dedicated sections under the headings \u201crecommended for you\u201d and \u201cbecause you watched\u201d The AI systems in place are designed to continuously learn and adapt to the users interactions on Netflix with the goal of ensuring that recommendations remain relevant and accurate overtime. This level of personalisation enhances user engagement by facilitating content discovery and making it easier for users to find content that they might like to watch. \n\n\n\nAI algorithms are utilized to categorize and tag content on Netflix. AI analyses the content of movies and TV shows and identifies key themes, genres, actors, directors and other relevant metadata. By accurately and conveniently categorizing content, Netflix can provide more accurate recommendations and improve search functionality. For example, users can search for specific genres or actors, and AI helps ensure that relevant content is surfaced in search results. This makes it easier for users to search for movies and TV shows that they may like as the data is conveniently sorted for them.\n\n\nAI algorithms are also employed to create dynamic thumbnails and previews for content on Netflix. These thumbnails are designed to grab users attention and entice them to click on the content. Thumbnails are rotated regularly on Netflix and give a fresh look to the Netflix content catalogue. AI analyses the content of movies and tv shows and selects representative images that are more likely to resonate with users. Additionally, Netflix utilizes AI to generate personalized video previews that auto play when users hover over a title, which provides a glimpse of the content and further encourages users engagement. Netflix also uses AI to provide audio and subtitle options in multiple languages, making its content accessible to a global audience. These AI driven techniques enhance user engagement, satisfaction, and retention on the platform by providing a seamless and personalized streaming experience tailored to each users preferences.\n\n(Section 2)\nProvider of the Service(Netflix)\nNetflix is responsible for delivering a wide range of entertainment content to subscribers worldwide. Netflix has a strong foothold in the market and is the most successful and dominant streaming platform on the market. Netflix invests in content acquisition, development and platform infrastructure to maintain its position in the market. Its many competitors such as Disney+, HULU, apple tv, Amazon prime and Now TV have put pressure on Netflix to improve the quality of their service. Netflix\u2019s unique selling point are its own original series, content that can only be found on Netflix. This includes shows such as Stranger things and Wednesday along with block buster movies such as Bird Box and Enola Holmes.\n\nUsers\nSubscribers: These are individuals who pay for a Netflix subscription to access the platforms content library. Users have the choice of 3 different payment plans such as basic, standard and premium. Subscribers expect a seamless streaming experience with high quality content and personalized recommendations. Subscribers rely on Netflix to provide a diverse selection of movies and TV shows suited to their tastes and interests. Netflix provides content that is suitable for all ages which include children, teenagers, students and Adults. Parents can pay for a subscription and use parental controls to ensure that their children can only access and watch appropriate age related content.\n\n\nNon-subscribers: Although these individuals are not direct users of the platform, non-subscribers are still impacted by Netflix\u2019s content distribution and cultural influence. For example, popular Netflix series like outer banks and stranger things generate buzz and discussion among non-subscribers on social media and in the broader cultural landscape.\n\nContent creators: Filmmakers, directors, production studios and actors are essential stakeholders in the Netflix ecosystem. Content creators rely on Netflix to distribute their work to a global audience and expect fair compensation for their contributions. Netflix  negotiates deals with content providers to acquire licensing rights for movies, TV shows, documentaries, and other content. This involves entering into agreements with major studios, production companies, and distributors. For instance, Netflix secured the rights to stream the hit series \"Friends\" by striking a deal with Warner Bros. Major Hollywood studios like universal pictures produce blockbuster movies and shows which are licenced to Netflix. Independent filmmakers and production companies also contribute to a significant portion of Netflix\u2019s content library. For example, Netflix\u2019s original series \u201cstranger Things\u201d is produced by 21 Laps Entertainment and Monkey Massacre productions.\n\nCompeting Streaming Services: Other streaming platforms, such as Amazon Prime Video, Disney+, and Hulu, compete with Netflix for subscribers and content licensing deals. These competitors influence the pricing, content selection, and user experience strategies of Netflix. For example, Disney's launch of Disney+ prompted Netflix to invest more heavily in original content to differentiate its offerings. This is because Disney+ were able to acquire the rights to Marvel and Star Wars, which are very successful franchises. Since Netflix don\u2019t own the rights to these major franchises, they had to improvise and come up with new strategies.\n\nLocal cable and broadcast Networks: Traditional media outlets face disruption from the rise of online streaming platforms like Netflix. Cable and broadcast networks may lose viewership and advertising revenue as consumers increasingly opt for on-demand streaming services. For example, the popularity of Netflix contributed to the decline of traditional TV viewership among younger demographics.\n\nSocial responsibility\nNetflix to subscribers: Netflix has a social responsibility to provide a safe and enjoyable streaming experience for its subscribers. This includes protecting user privacy and data, addressing harmful content and combating misinformation.  Netflix employs content moderation policies and practices to mitigate the presence of harmful or inappropriate content on its platform. This includes implementing age appropriate content ratings, content warnings and parental controls to help users make informed viewing decisions and to protect vulnerable audiences. For example 13 reasons why is a Netflix exclusive that discusses sensitive matters such as suicide. Users are informed that the show contains graphic content that could be upsetting to some viewers. In the image below, the 82% match feature in green is generated by AI. It means based on the previous content I have watched, I am 82% likely to enjoy the show \u201c13 reasons why\u201d Since this is a sensitive show that touches on the subject of suicide, Netflix provides a content warning to inform subscribers of the content they are about to watch.\n\n\nNetflix to content creators: Netflix must ensure fair compensation and transparent licensing agreements with content creators. This involves negotiating royalties and residuals to fairly compensate artists for their work. For example, Netflix signed a landmark deal with the Writers Guild of America to increase writers' compensation for streaming content.\n\nNetflix to Society: Netflix has a broader social responsibility to promote diversity, inclusion, and representation in its content offerings. This includes supporting diverse storytellers and amplifying underrepresented voices. For example, Netflix produced the critically acclaimed film \"Roma,\" directed by Oscar winner Alfonso Cuar\u00f3n, which highlighted issues of class and ethnicity in a 1970\u2019s Mexican society.\n\nTrust relationships\nSubscribers Trust Netflix: Subscribers trust Netflix to deliver a reliable streaming service with high-quality content. This trust is built on factors such as content availability, user friendly interface and features, streaming performance and reliability, and user privacy protections. For example, Netflix encrypts user data and adheres to strict privacy policies to protect subscriber information. Netflix maintains open communication with subscribers, providing timely update on service enhancements, content releases and platform changes. Subscribers trust Netflix to prioritize their satisfaction and provide reliable assistance whenever needed. For example, the hunger games franchise recently left Netflix as the contract had expired. Subscribers were informed of the date it would be removed from the platform allowing them sufficient time to watch it before it left the streaming platform.\n\nContent Creators Trust Netflix: Content creators trust Netflix to uphold their artistic vision and provide fair compensation for their work. This trust is essential for fostering long-term partnerships and attracting top talent to the platform. Netflix has gained a reputation for offering content creators artistic freedom and control over their projects. Netflix\u2019s trust in creators\u2019 visions and abilities encourages established filmmakers, writers, and actors to collaborate with Netflix, knowing that their work will be respected and supported. With over 200 million subscribers worldwide, Netflix provides content creators with guaranteed access to a global audience. Netflix offers creators the opportunity to share their stories with viewers around the world, overcoming geographical and cultural barriers.\n\nSociety Trusts Netflix: Society trusts Netflix to act responsibly and ethically as a cultural influence. This trust is based on Netflix's commitment to diversity, inclusion, and social impact. By producing and acquiring a wide range of films, TV shows and documentaries featuring diverse casts, stories, and perspectives, Netflix ensures that audiences see themselves reflected on the screen. For example, Netflix\u2019s original series heartstopper created by an independent author Alice Osman represents the LGBTQ community and Netflix\u2019s original series orange is the new black is celebrated for its diverse ensemble cast and exploration of various cultural and societal issues.\n\n(Section 3)\n\nPotential ethical risks associated with Netflix\n\nHuman rights\nRisk: Privacy violations\nStakeholders affected: Subscribers, content creators, Netflix employees\nSeverity: High\nLikelihood: Medium\n\nSubscribers: Privacy violations pose significant risks to affected stakeholders. Unauthorised access to personal data such as viewing history, payment information and demographic details can result in identity theft, financial fraud or reputational harm. Subscribers expect Netflix to handle their data securely and respect their privacy rights. Any breach of this trust can lead to severe consequences such as loss of confidence in the platform and potential legal action against Netflix. \n\nContent creators: Privacy breaches concerning content creators can jeopardize their proprietary information, intellectual property, or confident communications associated with their projects. For example , unauthorized access to scripts, production budgets, or contract negotiations could weaken content creators capacity to safeguard their creative assets and sustain competitive advantages within the industry. Additionally breaches of confidentiality might diminish the trust between Netflix and content creators, potentially affecting forthcoming collaborations and contractual ties.\n\nNetflix employees: Privacy violations related to employee records, performance evaluations or personal communications can encroach upon employees rights to privacy and confidentiality within the workplace. Unauthorised disclosure of employee health records or disciplinary actions could result in legal liabilities for Netflix and inflict emotional distress or harm to the reputation of affected employees. Breaches of employee privacy may contravene labour laws or employment regulations, leaving Netflix vulnerable to potential legal and financial consequences.\n\nLabour Practices\nRisk: Employee Exploitation\nStakeholders affected: Workers\nSeverity: Medium\nLikelihood: Low\n\nDespite Netflix\u2019s commitment to corporate social responsibility and compliance with labour laws and regulations, many factors may contribute to the occurrence of employee exploitation. The fast paced nature of the entertainment industry as well as tight production schedules and high expectations for content delivery can lead to production pressures that can lead to employee exploitation. In the pursuit of meeting deadlines and maintaining quality standards, production crews may face demands for longer than usual working hours, insufficient breaks and irregular working hours which may potentially compromise their well-being and their work life balance. Although Netflix directly employs a large portion of its workforce it also relies on a network of contractors and third parties to support its content production operations. The use of subcontractors or temporary workers may introduce complexities in labour relationships, making it challenging to ensure consistent adherence to fair labour practices across the entirety of the Netflix industry.\n\nThe environment\nRisk: Carbon footprint from data centres\nStakeholders affected: future generations\nImpact severity: Low\nLikelihood: medium\n\nStreaming services such as Netflix contribute to increases energy consumption, as power is needed to run various devices that you can stream Netflix on such as phones, tablets, laptops and TVs.  Netflix continues to expand its data centre infrastructure and streaming operations which contributes to the overall carbon footprint.  However Netflix\u2019s impact on the environment is relatively low compared to industries with more tangible environmental footprints.\n\nFair operating procedures:\nRisk: unfair treatment of content creators\nSeverity: Medium\nLikelihood: Medium\n\nNetflix heavily relies on content creators including filmmakers, directors, producers, writer and actors to produce the diverse array of content available on its platform.\nIt is a possibility that content creators could be treated unfairly in terms of compensation, contract agreements or creative control. If compensation is lower than industry standards or doesn\u2019t reflect the success of the movie or show, it could lead to tensions and legal disputes between Netflix and the content creator. For instance, content creators may feel their work is undervalued or exploited if they perceive that Netflix is prioritizing its own original productions over licensed content. Addressing these concerns and ensuing transparent partnerships with content creators is essential for maintaining ethical operating procedures. Intellectual property rights must be considered when working with Netflix. For instance, dispute may arise over who retains the rights to merchandising or future adaptations of the original content. Clear and transparent agreements regarding intellectual property rights are essential to mitigate such risks.\n\nConsumer issues: \nRisk: Content inaccuracy or misrepresentation\nStakeholders affected: Subscribers\nSeverity: Medium\nLikelihood: Low\n\nAlthough Netflix strives for accuracy and authenticity in its content, there may be inaccuracies or misrepresentation in their content, leading to viewer dissatisfaction and confusion. For example, inappropriate portrayals of specific ethnicities, inaccurate depictions of cultural traditions or insensitive handling of sensitive topics may result in backlash from viewers and damage Netflix\u2019s reputation for cultural diversity and inclusion. Netflix also needs to ensure that they fact check information they provide in educational content that they release. For example if a documentary series on a scientific or social issue presents information in a biased or misleading manor, it can lead to viewers developing misconceptions or false beliefs. Such instances of misinformation or bias can undermine the educational value of the content and diminish trust in Netflix\u2019s content.\n\nCommunity involvement and development\nRisk: disruption for local residents\nStakeholders affected: Local communities, residents, small businesses\nSeverity: Medium\nLikelihood: Medium\n\nWhen large production crews enter small towns and villages, it can cause an increase in traffic congestion in the area. The presence of a production crew, equipment, and filming activities can disrupt the daily routines of residents in small towns or rural areas. Road closures, traffic diversions, and restricted access to public spaces may cause inconvenience to locals, disrupt businesses and impact essential services such as transportation that many locals depend on. Filming activities often require heavy equipment, such as trucks and vans as well as special effects systems that generate noise that can disturb quite rural communities. Filming may have a negative impact on the environment and can lead to soil erosion, habitat disturbance and pollution particularly in ecologically sensitive areas or protected landscapes. Small towns and rural communities may lack the infrastructure and resources to accommodate large scale filming operations. The influx of crew members, actors and support staff can strain local amenities such as accommodation and utilities leading to shortages, overcrowding and increased demand for services in the area. \n\n\n\n(Section 4)\n\nMitigation measures for privacy violations\n\nEnhanced data security measures\nNetflix should implement advanced data security measures to safeguard subscriber data, content creator information and employee records. This can include encryption protocols, two step factor authentication, and regular security audits to detect and mitigate potential vulnerabilities or breaches . Netflix has recently investigated the issue of password sharing, and has employed various techniques to detect password sharing including monitoring IP addresses, device usage patterns and login locations. In order to share your password with friends from other households, you must pay extra and buy an extra member slot on your account. Password sharing increases the risk of subscribers data and personal information becoming leaked. By enhancing data security measures, Netflix can reduce the likelihood of privacy violations such as unauthorized access to personal data or confidential information. This helps to mitigate the risk of financial fraud and maintain trust between Netflix and its subscribers. By ensuring the security of personal data, Netflix can mitigate the risk of reputational damage, financial losses and legal damage associated with privacy breaches.\n\nStakeholders involved that play a role in these mitigation measures are Netflix\u2019s cybersecurity team, IT department, data protection officers and regulatory authorities. Compliance with regulatory bodies is necessary to ensure compliance with data security standards and regulations. Stakeholders effected include subscribers, content creators and Netflix employees.\n\nContent creators often share sensitive information with Netflix during the production process including scripts, production budgets and contract negotiations. Mitigation measures to address data privacy concerns between creators and Netflix include providing training and awareness programs for employees involved in content production to ensure adherence to data privacy protocols. Comprehensive privacy and awareness programs help raise employees awareness of privacy risks and educate them with the knowledge and skills needed to prevent privacy violations. By promoting a culture of privacy awareness and accountability, Netflix can reduce the likelihood of human errors that could lead to privacy breaches and enhance overall data protection. Netflix could also implement data security measures to protect sensitive information shared during the production process and establish clear contractual agreements that outline data privacy expectation and obligations for both parties. It is essential to conduct regular audits and assessments to monitor compliance with data privacy regulations and identify any potential vulnerabilities or risks. \n\nNetflix can enforce strict access controls to limit access to sensitive user data only to authorized personnel on a need to know basis. Strict access controls reduce the likelihood of privacy violations by minimizing the risk of unauthorized access to user data. By limiting access to only those employees who require it for their job acts as an additional layer of authentication and help Netflix prevent unauthorized individuals from accessing or tampering with sensitive information.\n\n\n\n\n\n\nReferences\n\n2nd, 3rd and 5th images sourced from my own Netflix account\nhttps://www.netflix.com/browse\n\nLyndon Seitz. (2023) \u201c24 fascinating facts about Netflix\u201dhttps://www.broadbandsearch.net/blog/fascinating-facts-statistics-netflix\n\nhttps://research.netflix.com/research-area/machine-learning\n\nhttps://www.argoid.ai/blog/netflix-ai\n\nAIthority analyst . (2023) https://aithority.com/machine-learning/five-ways-netflix-is-using-ai-to-improve-customer-experience/\n\nhttps://jobs.netflix.com/culture\n\nPatty McCord .(2014) \u201chow Netflix reinvented HR\u201d https://hbr.org/2014/01/how-netflix-reinvented-hr\n\nhttps://www.wannatalkaboutit.com/ie/13-reasons-why/#hero\n\nMehera Bonner .(2022) https://www.cosmopolitan.com/entertainment/celebs/a39794855/stranger-things-season-4-budget/\n\nCatherine Reed. (2023) \u201c Netflix Data breaches: Full timeline through 2023\u201d https://firewalltimes.com/netflix-data-breach-timeline/"
    },
    {
      "role": "assistant",
      "content": "{\n    \"feedback\": \"A decent assessment of Netflix\\u2019s ethical issues, though more on their overall business model than their use of AI specifically. Note that much of its actual production is subcontracted, so those are not direct Netflix employees or direct impact on filming locations, that would be the production companies so these issues would be under fair operating procedures.\",\n    \"description / 20\": 62,\n    \"roles / 10\": 62,\n    \"risks / 40\": 65,\n    \"mitigation / 20\": 65,\n    \"refs / 10\": 62,\n    \"total score\": 63.800000000000004\n}"
    }
  ],
  [
    {
      "role": "system",
      "content": "You are a professional educator. \nYour target is to assess students' assignments based on the provided rubric (between <rubric> and <rubric> labels).\n<rubric>\nAssignment: Investigating and Analysing Ethical Risks of a Digital Engagement application \n\nThis is an INDIVIDUAL assignment on Ethical Risk Assessment for the TEU00062 module. This assessment is worth 30% of the total TEU 00062 module mark. The deadline for submission is 12.00. noon Friday 31st March 2024. \n \nLearning outcomes: Upon completion of this assignment students should:\nBe able to identify the different stakeholders and organisational roles involved in developing and using AI-driven digital engagement application, and motivations. \nBe able to identify and analyse ethical risks in a digital engagement application.\nBe equipped to identify technical and governance mitigations of these risks\n \nSteps:\n \nSelect ONE real world application that embodies one or more AI-driven digital engagement techniques you have learnt about on this course, e.g. e.g. digital engagement techniques used in one applications such as social media, newsfeeds, search, eCommerce, elearning or digital humanities., or in an application you are familiar with e.g. interactive Game, Virtual Presence (i.e. VR /AR) etc. \nIdentify the different value chain stakeholder roles involved in this application scenario and where possible the actors (i.e. companies, organisations) carrying out those roles. including those involved in: data collection; AI model development; development and use of the application. \nUse social responsibility principles (see ethic lecture week 2) to identify and analyse real or potential ethical risks of the application, including classification of the affected stakeholders (hint: see also risks classified by Latzer et al 2016 \u2013 ethics lecture week 1), how severe the risk would be and how likely it is to occur.\nIdentify one or more mitigation measures that specific value-chain stakeholders may undertake to minimise the impact of the risk you think is most harmful. These could be technical measures, organisational governance measures or measure involving additional engagement with affected individuals or groups.\n \nStructure of Assignment Report. \nStructure the Report (4000 word max excluding figures and references), under the following heading [indicative percentage of assignment mark per section]. Please provide references\n \nDescription of Application [20%]\n<describe your selected application, explaining what it does, the role AI and data collection plays in its digital engagement features, and the benefits it provides>\n\nIdentification of Stakeholder Roles involved in the application and its governance [10%]\n<describe the stakeholder roles undertaken in the development, use and governance of the application, and which organization take those roles and the benefits they derive from their involvement. Include roles that may be indirectly affected by the application. >\n\nIdentification of Ethical Risks [40%]\n<Identify the ethical risks that may be raised by your application under the social responsibility categories below (more details in ethics lecture week 2). For each risk identify which stakeholder may be affected, and assess how severe the impact would be for each stakeholder (Low, Medium, High) and the likelihood of that risk being incurred (Low, Medium, High), justifying your assessment, even if you think there is no risk under that heading.>\nHuman Rights\nLabour Practices\nThe Environment\nFair Operating Procedures\nConsumer Issues\nCommunity Involvement and Development\n \nDiscussion of Mitigations Measures for Risk [20%]\n< For one of the risks you consider more severe, describe mitigation measures that could be taken. Explain how the measure would reduce or eliminate the likelihood and impact of the risk. Explain which stakeholders would be involved, or would have to interact in implementing the measure, identifying if this is a governance measure an organization can undertake itself or in collaboration with other parties, e.g. regulators or external stakeholder..>\n\nReferences [10%]\nInclude references to source you use for you analysis, which could be academic or policy papers, blogs, information from company web site or news stories.\n\n</rubric>\nThe assignment content will be provided by user (not in system prompt)\nDo not say anything else except the marking, following the format of:\n{\n    \"feedback\": \"feedback content\",\n    \"(rubric part 1) / (rubric part 1 weight)\": score,\n    \"(rubric part 2) / (rubric part 2 weight)\": score,\n    ......other rubric parts\n    \"total score\": weighted average score of all parts\n}"
    },
    {
      "role": "user",
      "content": "MyFitnessPal\n\nDescription\nMyFitnessPal is one of the most widely used health and fitness applications that helps millions of users worldwide optimize their health, whether it be weightloss, nutrition goals, fitness goals or tracking sleep. Users can also gain support from an online community. MyFitnessPal was founded in 2005 by Mike and Albert Lee but was acquired by Under Armour in 2015. MyFitnessPal is a platform that users can utilize to track their calories, nutrition, exercise, and sleep. Not only does MyFitnessPal allow the user to track calories, but it can also be used to track macros, vitamins, and other micro -                                                  vitamins. Everything that is logged into MyFitnessPal allows support of the user\u2019s                                                     individual goals whether it be a high protein diet, low carbohydrate, or fat, etc.\n MyFitnessPal has a huge database of foods, with almost 14 million foods on the \napp. The app is available for both Android and iOS devices.   The user can scan a barcode of a food product or search for it on the app, they will then be able to see all the nutritional information and log it into their \u2018diary\u2019.  AI has revolutionised the logging of food products into MyFitnessPal. MyFitnessPal uses AI to learn about the foods you eat, and what recipes you cook this saves time and makes tracking calories easier. \nThe standard subscription will only allow users to track calories, weight, steps, and exercise. A premium subscription offers additional features for users aiming to unlock more advanced functionalities such as custom macronutrient tracking, heart health information, as well as zero ads. Research shows that members are 65% more likely to reach their goals with a premium subscription. \nMacronutrients are often abbreviated to macros, and the three most important macronutrients are carbohydrates, proteins, and fats. Depending on your age, sex, and activity level your body will require certain amounts of these macros. Depending on the goal of the user whether it be to weight loss, muscle gain, improvement of cardiovascular fitness or general healthy eating habits the app will aid users in goal creation and track progress over time, MyFitnessPal uses an AI Macro Calculator to gain valuable insights into the user\u2019s macronutrient goals, depending on their own goal. For example, a person wishing to follow a ketogenic diet will have 70%-80% of their calorie intake for the day coming from fat sources, 10%-20% of their protein intake coming from proteins and 5%-10% coming from carbohydrates. Users can set a daily calorie goal, as well as specific calorie goals for each meal and snack, this will help the user to stay on track of their goal. Throughout the day there will be notifications to log food.  \nIn 2021, MyFitnessPal introduced a new AI-powered scanning feature that plays a role in identifying and tracking the ingredients in the food items that the user logs into the app. The company partnered with Passio, to create the MealScan feature, which is powered by \u201cstate of the art Nutrition AI SDK\u201d The app uses image recognition to identify foods and portion sizes to log your daily calorie and nutrient intake accurately. To successfully diet nutrition tracking is paramount, traditional methods of looking at labels were tedious, and users would lose motivation after a while. AI can also be used to provide tailored nutritional recommendations for the user to reach their desired goal. The use of AI has helped users to adopt a healthier lifestyle. Tracking foods can be a monotonous process and users are likely to lose motivation after a week or two. This new feature helps users avoid having to search for food and ingredients within the database so tracking foods has never been more convenient. \n\nFigure 3: Meal Scan feature on MyFitnessPal\n\n As well as the meal scanning feature on MyFitnessPal, there is also an AI-driven community feature \u2013 where users can interact and compete with each other whether it is exercise or weight logging. The community feature uses AI to \u2018match\u2019 people with similar health and fitness goals. The MyFitnessPal community helps users to support each other along their fitness and health journey. Users can ask questions and get help from other members on anything health and weight loss related. Users can also post and share tips, and nutritional advice and discuss ways of eating. As well as this, users can share their favourite recipes and share fitness tips and exercise suggestions. \nMyFitnessPal has the ability to synchronize with wearable technology such as the Apple Watch and other tracking apps to allow information to be uploaded from multiple sources. This is useful because information on weight, health, calorie intake and activity are all kept in one place. \n\nIdentification of stakeholder roles involved in the application and its governance.\nUnder Armour acquired MyFitnessPal in 2015 but sold it to investment firm Francisco Partners in 2020. The main stakeholders are the users, data scientists, product managers, privacy experts, engineers, and healthcare professionals. \n\nUsers\nPremium subscription users are important to the upkeep of MyFitnessPal. A premium subscription has additional features to increase the app\u2019s customizability and flexibility. A premium subscription costs \u20ac20 per month or \u20ac80 per year. Users interact with the app to track their diet, nutrition and activity. As well as this users are interacting with each other using the community feature. Features provided by a premium subscription include food analysis ( to see which foods are good and bad for you), quick add nutrients (adds to convenience), priority customer support, ad-free, Meal Scan, workout routines and a barcode scanner. These features help to keep the user engaged and drive subscriptions when satisfied users leave good reviews. Users play an important role in the governance of MyFitnessPal by spreading awareness and the benefits of the app by word or mouth and on other social media platforms. There is many benefits to driving premium subscriptions to MyFitnessPal including; increased revenue, job creation, app improvements, as well as improved health of society. \n\nFigure 4: good reviews driving user engagement on MyFitnessPal\n\nData Scientists:\nThe data scientist team at MyFitnessPal prioritise artificial intelligence and machine learning in their product portfolio while maintaining a rigorous scientific engine. The team want to create a personalized experience for the user to achieve success on their health and fitness journey. The data scientists at MyFitnessPal work closely with product managers and business stakeholders to formulate and build machine learning products, they help to track progress so goals can be reached. They play an important role in the governance of the app as they work on innovative application improvements to drive user engagement. Data Scientists have succeeded in their role to include Artificial Intelligence into MyFitnessPal which no doubt has driven user engagement because the app has become more convenient for health and fitness gurus to use.  \n\nProduct Managers:\nProduct managers at MyFitnessPal play an important role in the development and success of the app, they take ownership of projects and work alongside the company\u2019s other stakeholders to deliver the main goals of MyFitnessPal. Product managers have a wide variety of roles within the company, from brainstorming new services and features to working with engineering and design to deliver projects. Product managers would also be involved in data collection to get positive and negative feedback on MyFitnessPal. When a feature such as Meal Scan is in the pipeline, product managers will manage the entire lifecycle by working closely with engineering, design, marketing, and data scientists to ensure that it is high quality and delivered on time. By analysing app performance, product managers can create a plan of action on how they want to proceed to drive user engagement. \n\nPrivacy Experts\nInvolved in the upkeep of GDPR and data privacy standards. The user does share a large amount of personal information with MyFitnessPal; therefore the user needs to have confidence in the company to keep all personal information confidential. They are involved in the write-up of privacy policies, in line with country-specific regulations. They play an important role in the governance of MyFitnessPal as they help to give the user peace of mind when it come to their personal data being secure. \n\nEngineers\nEngineers are involved in the running of the backend systems that drive MyFitnessPal. They are involved in the maintenance and operational activities of MyFitnessPal. Their technical expertise comes into play whenever there is an issue with the running of the app. The app must run well to drive user engagement. Engineers would work with quality assurance to carry out tests on new features to ensure that there is no issues before it reaches the customer. \n\nHealthcare Professionals\nHealthcare professionals often recommend apps such as MyFitnessPal to their clients to monitor their health and activity levels. Healthline is a website that provides medical information and health advice, they recommended the use of MyFitnessPal in the management of Type 2 diabetes. MyFitnessPal gives a nutrient breakdown of certain types of food and knowing what types of food would cause a fluctuation in blood sugar can cause a diabetic to make healthier food choices. Healthcare professionals play an important role in the governing if the app by driving user engagement, and by providing professional advice as people are more likely to listen to a doctor. \n\nRegulatory Agencies\nThere are third parties associated with MyFitnessPal including law enforcement, government, and regulatory bodies. MyFitnessPal must comply with GDPR standards for the protection of personal data. Regulatory agencies provide external governance over MyFitnessPal to ensure the user is protected. \n\n\nIdentification of Ethical Risks\nHuman Rights: \nBy agreeing to the privacy policy that MyFitnessPal set out, \u201cyou consent to the transfer and processing of the information in the United States\u201d. For residents of the European Union, MyFitnessPal \u201cprocess your Personal Data and provides other information required by the GDPR\u201d. MyFitnessPal uses this data for advertising and marketing purposes as well as providing health and fitness services to the user. When you sign up to MyFitnessPal, some of your personal data is collected, this includes name, photo, username and password, email address, date of birth, gender, payment information and general location data. MyFitnessPal will store data on dietary habits, weight, fitness levels, heart health and Body Mass Index (BMI). This data is used to create personalized recommendations such as meal plans and workout plans. When you connect a technology such as a Fitbit or an Apple Watch, MyFitnessPal will use the data stored on these technologies. If there is a data breach within MyFitnessPal, this personal data could be compromised and misused. This would affect the users and subscribers to MyFitnessPal, it would have a severe impact on the users as it is very personal information. On March 25, 2018, Under Armour alerted customers that a data breach had occurred, which affected 150 million accounts. Cybersecurity experts secured the app and continuously monitor the app for any unusual activity so the risk of another breach would be presumed unlikely but not impossible. The information stored in MyFitnessPal can be used for identity theft or information could be sold on the dark web. A major challenge for maintaining ethical integrity is the lack of documentation in regards data privacy and confidentiality\n\nLabour Practices\nBased on reviews on glassdoor.co.uk there is a high rating for labour practices for the MyFitnessPal employees. There are high ratings for culture and values, diversity and inclusion, work/life balance, compensation and benefits and career opportunities. MyFitnessPal is dedicated to advancing, cultivating and preserving a culture of diversity, equity, inclusion, and belonging. Employees would be the main stakeholders that would be affected by labour practices. If there were to be a change in labour practices the level of severity of the impact would depend on whether it was a positive or negative change in labour practices. I think the likelihood is low-medium. This can be justified by the positive reviews of employees safer, however a change in management or the way the company is run could cause an impact on labour practices. \n\nThe environment\nThe personal data that is retained by MyFitnessPal can be printed on paper and stored in files, which is destroyed when it no longer serves its purpose, for example, when an account is deleted. Personal data stored on paper can be shredded, burned or incinerated. Data stored in electronic form is deleted using technology to prevent its recovery. The destruction of personal data stored on paper would have a harmful impact on the environment. The burning of these files would contribute to climate change. However, as would be expected majority of information is stored in data centres. Huge amounts of energy is needed to power data centres, this would lead to a rise in carbon emissions. The main stakeholder that would be affected by this is the public. There is quite a high impact as a society we are having concerning issues regarding climate change. I think the likelihood of this being an issue is medium. There is an impact and there is continued technology use and commercial innovation to mitigate the impact of energy use in data centres. There is a scope that MyFitnessPal will turn to renewable sources of energy to power data centres and reduce their carbon footprint. \n\nFair operating procedures\nTo comply with fair operating procedures, there must be extensive data protection policies in place to prevent a data breach. MyFitnessPal also has algorithms to calculate the daily number of calories you should be eating to be in a calorie deficit or surplus depending on the user\u2019s goals. There is a risk that the algorithm that MyFitnessPal use to calculate the daily number of calories consumed may be inaccurate for the user\u2019s desired goal. For example a user who has more muscle will burn more calories in a day in comparison to a person with more body fat. Therefore, the basal metabolic rate (BMR), the amount of calories that are burned as your body performs life-sustaining functions, for those with more muscle have a BMR. As MyFitnessPal works primarily off age, weight, and height it does not consider the amount of muscle the user may have. Therefore, an ethical risk could be an inaccurate calculation which will lead to a misleading daily calorie intake for the user. This could have a medium impact on the user\u2019s result if they experienced unexpected weight loss or gain. The likelihood of this happening Is low to medium. This can be justified by the fact that people with muscle generally are aware of nutrition goals and basal metabolic rate as it would have taken a calorie surplus as well as food that is high in protein to build muscle initially. People starting out on a weight loss journey may not have much muscle so therefore the general calculation will be accurate to maintain a calorie deficit. \n\n\nConsumer issues\nThe main consumer issue that would arise with MyFitnessPal is a data breach. A breach of personal data could lead to hackers using the information for identity theft. This would have a severe impact on the user.\n\nCommunity involvement and development\nWhile having a support system to help you reach health and fitness goals is an important part of MyFitnessPal, there are some ethical considerations. The ethical considerations mainly revolve around the sharing of personal data. Users of the community feature on MyFitnessPal have to be careful in what they choose to share with the public. Another issue is the cost of a subscription and general knowledge of the importance of health and fitness. A study showed that individuals from affluent or intermediate social milieus were more likely to use MyFitnessPal. Individuals from lower milieus were less likely to use MyFitnessPal. The main stakeholders that would be affected by these ethical considerations is the general public but more specifically individuals in a lower milieu. The impact is medium to high as it is diet related diseases are a global health concern. The likelihood is also medium to high, however, MyFitnessPal took measures to combat this issue. In 2022, MyFitnessPal announced plans to provide access to nutrition and fitness education, personalized health insights, tools and technology to help at-risk Americans nationwide eat healthier and increase their physical activity. It aim to service those that are most at risk for diabetes and heart disease among other diet-related diseases. \u201cBy improving access to easy-to-use tools, education and personalized nutritional insights for Americans who might not otherwise have these resources, we hope to eliminate the barriers around healthy eating, physical activity and health disparities through this collaboration\u201d says Amanda Mahan, Vice President of Public Realtions and Partnerships at MyFitnessPal. \n\nDiscussion of Mitigation Measures for Risk\nOne of the main risks that was identified in this report was a data breach of the personal information stored in MyFitnessPal. The sharing of this personal information online or on the dark web could have serious consequences for the user and it would destroy the reputation of the app. The data breach with MyFitnessPal in 2018 compromised 150 million accounts in one of the largest cyber incidents to date. However, the hackers didn\u2019t manage to get access to sensitive information such as social security numbers or payment information. Following this cyber-attack, the company shares dropped by 4 per cent, highlighting the impact that the data breach had on MyFitnessPal. After the cyber-attack, MyFitnessPal encouraged all users to change their passwords to avoid hackers accessing any of the user\u2019s data. One of the mitigation measures that was taken was to store login credentials with encryption. By doing this it prevents hackers from accessing users accounts straight away because they would need a considerable amount of time to decode each one. This gave MyFitnessPal enough time to raise awareness and get each user to change their password. The stakeholders involved in the mitigation measures for a data breach are the privacy experts and the product managers. The data scientists and engineers would also play an important role in the encryption of accounts. Regulatory agencies and government bodies are also involved in regulating data privacy laws, which is an example of an external control to achieve mitigation. The encryption of accounts is a technical measure that is undertaken internally within MyFitnessPal to mitigate the risk of a data breach. Measures such as two-factor authentication could be taken to maximise safety and minimise the risk of a data breach. Two-factor authentication (2FA) adds an extra layer of security compared to methods that depend on a single-factor authentication. Two-factor authentication would require the user to put in their username and password when they first log onto MyFitnessPal as well as a fingerprint, facial scan or text verification as a second factor. If stakeholders such as engineers and data scientists were to incorporate two-factor authentication process as part of the application design it would make it harder for hackers to gain access to a the users account. It would prevent access to the hacker of the users payment information and personal data. \n\nReferences\nConsagoustech. (2023, September 5). AI in Fitness Apps: Why It\u2019s a Game Changer \nMeal Scan FAQ. (2024, March 20). MyFitnessPal. \nWilder, A. (2021, February 19). MyFitnessPal adds AI scanning technology to iOS for tracking calories. The Spoon. \nPassio. (n.d.). MyFitnessPal - Nutrition AI by Passio AI. \nWhat are the features of MyFitnessPal Premium? (2023, December). Support MyFitnessPal.\nJaro, L. (2019, September 3). 5 apps that help me manage type 2 diabetes. Healthline. \nMyFitnessPal breach: Learn about MyFitnessPal Hack. (2021, February 1). IDStrong. \nPrivacy Policy | MyFitnessPal. (n.d.). \nFlanagan, C. (2023, March 14). How data centers are driving the renewable energy transition. Forbes. \nBasal Metabolic Rate Calculator. (n.d.). Garnet Health. \nPascu, L. (n.d.). MyFitnessPal hacked, 150 million user accounts compromised. Hot For Security. \nRosencrance, L., Loshin, P., & Cobb, M. (2021, July 7). two-factor authentication (2FA). Security.\nMyFitnessPal, & Robichaud, N. (2022, September 28). MyFitnessPal announces program to support nutrition and fitness education providing premium access to America\u2019s most At-Risk populations. PR Newswire. \nKhazen, W., Jeanne, J. F., Demaretz, L., Sch\u00e4fer, F., & Fagherazzi, G. (2020). Rethinking the Use of Mobile Apps for Dietary Assessment in Medical Research.\u00a0Journal of medical Internet research,\u00a022(6), e15619. \nR\u00e9gnier, F., & Chauvel, L. (2018). Digital Inequalities in the Use of Self-Tracking Diet and Fitness Apps: Interview Study on the Influence of Social, Economic, and Cultural Factors.\u00a0JMIR mHealth and uHealth,\u00a06(4), e101. \n37 reviews: What is it like to work at myfitnesspal? | Glassdoor. (n.d.)."
    },
    {
      "role": "assistant",
      "content": "{\n    \"feedback\": \"A well structure ethics assessment. Do you think there are risk in linking this digital approach to other dieting and fitness products, i.e. could people be targeted with ad for unhelpful product based on knowledge of their current diet/fitness trajectory - in which they might feel vulnerable? Are there also issue about over quantifying one fitness such that otherwise healthy people feel unhappy about their weight/fitness.\",\n    \"description / 20\": 65,\n    \"roles / 10\": 65,\n    \"risks / 40\": 60,\n    \"mitigation / 20\": 65,\n    \"refs / 10\": 65,\n    \"total score\": 63.0\n}"
    }
  ],
  [
    {
      "role": "system",
      "content": "You are a professional educator. \nYour target is to assess students' assignments based on the provided rubric (between <rubric> and <rubric> labels).\n<rubric>\nAssignment: Investigating and Analysing Ethical Risks of a Digital Engagement application \n\nThis is an INDIVIDUAL assignment on Ethical Risk Assessment for the TEU00062 module. This assessment is worth 30% of the total TEU 00062 module mark. The deadline for submission is 12.00. noon Friday 31st March 2024. \n \nLearning outcomes: Upon completion of this assignment students should:\nBe able to identify the different stakeholders and organisational roles involved in developing and using AI-driven digital engagement application, and motivations. \nBe able to identify and analyse ethical risks in a digital engagement application.\nBe equipped to identify technical and governance mitigations of these risks\n \nSteps:\n \nSelect ONE real world application that embodies one or more AI-driven digital engagement techniques you have learnt about on this course, e.g. e.g. digital engagement techniques used in one applications such as social media, newsfeeds, search, eCommerce, elearning or digital humanities., or in an application you are familiar with e.g. interactive Game, Virtual Presence (i.e. VR /AR) etc. \nIdentify the different value chain stakeholder roles involved in this application scenario and where possible the actors (i.e. companies, organisations) carrying out those roles. including those involved in: data collection; AI model development; development and use of the application. \nUse social responsibility principles (see ethic lecture week 2) to identify and analyse real or potential ethical risks of the application, including classification of the affected stakeholders (hint: see also risks classified by Latzer et al 2016 \u2013 ethics lecture week 1), how severe the risk would be and how likely it is to occur.\nIdentify one or more mitigation measures that specific value-chain stakeholders may undertake to minimise the impact of the risk you think is most harmful. These could be technical measures, organisational governance measures or measure involving additional engagement with affected individuals or groups.\n \nStructure of Assignment Report. \nStructure the Report (4000 word max excluding figures and references), under the following heading [indicative percentage of assignment mark per section]. Please provide references\n \nDescription of Application [20%]\n<describe your selected application, explaining what it does, the role AI and data collection plays in its digital engagement features, and the benefits it provides>\n\nIdentification of Stakeholder Roles involved in the application and its governance [10%]\n<describe the stakeholder roles undertaken in the development, use and governance of the application, and which organization take those roles and the benefits they derive from their involvement. Include roles that may be indirectly affected by the application. >\n\nIdentification of Ethical Risks [40%]\n<Identify the ethical risks that may be raised by your application under the social responsibility categories below (more details in ethics lecture week 2). For each risk identify which stakeholder may be affected, and assess how severe the impact would be for each stakeholder (Low, Medium, High) and the likelihood of that risk being incurred (Low, Medium, High), justifying your assessment, even if you think there is no risk under that heading.>\nHuman Rights\nLabour Practices\nThe Environment\nFair Operating Procedures\nConsumer Issues\nCommunity Involvement and Development\n \nDiscussion of Mitigations Measures for Risk [20%]\n< For one of the risks you consider more severe, describe mitigation measures that could be taken. Explain how the measure would reduce or eliminate the likelihood and impact of the risk. Explain which stakeholders would be involved, or would have to interact in implementing the measure, identifying if this is a governance measure an organization can undertake itself or in collaboration with other parties, e.g. regulators or external stakeholder..>\n\nReferences [10%]\nInclude references to source you use for you analysis, which could be academic or policy papers, blogs, information from company web site or news stories.\n\n</rubric>\nThe assignment content will be provided by user (not in system prompt)\nDo not say anything else except the marking, following the format of:\n{\n    \"feedback\": \"feedback content\",\n    \"(rubric part 1) / (rubric part 1 weight)\": score,\n    \"(rubric part 2) / (rubric part 2 weight)\": score,\n    ......other rubric parts\n    \"total score\": weighted average score of all parts\n}"
    },
    {
      "role": "user",
      "content": "Description of the application (Google Gemini AI)\nGoogle Gemini is a generative AI which makes use of a large language model to interpret and respond to user inputs. It was created by google DeepMind in response to the open AI chat GPT. The Gemini was released in December 2023. Since its launch it has gained attentions of various users from different field of work. Gemini currently has three different sizes Gemini Ultra: which is the most sophisticated model for complex tasks, Gemini Pro: this is a free version of the Gemini AI, Gemini Nano: this is the version of the Gemini Ai created for running on device task. This nano version is currently being used in the google pixel 8 pro smartphones. Gemini is developed to process and generate text based on information received from users. It is a multimodal AI model which can respond to a wide range of content including texts, audios, videos, and codes. It multimodal features allows it to interpret, text and pictures. That makes its possible for it to combine different images, process it and create new ones, and generate piece of writing as well as programming codes. The wide range of ability that the Gemini possess make it possible for it to be used for different function by different individuals from various fields.\nBenefits of Gemini\nGemini helps to improve productivity of workers. The use of Gemini has been adopted by different companies to help improve the efficiency of workers in the workplace. The ability of the Gemini ultra to perform more sophisticated tasks such as ability to write programming codes and ability to engage in complex reasoning has made it a reliable asset to be employed by businesses. Some other benefits include increase in automation of work task in different fields (sophisticated reasoning), reduction of cost for employers as well as increase efficiency and productivity. In addition, Gemini can be used as a base for more advanced coding system with the presence of the more advanced code generation system called alpha code 2. \n\nStakeholders Roles\nProviders\nThese are the people involved in the creation, development, and funding of the Gemini. They include the investors, management team and employees like software developers. Their role includes the following: \nEnsuring that they create a reliable and trusted AI that meets the needs of the users. \nEnsuring that Gemini meets all legal requirements of generative AI models. They ensure Gemini does not violate any legal requirement put in place by the AI regulatory bodies in different regions.\nEnsuring the safeguarding of all data being collected by Gemini. It is the duty of the developers the owners of Gemini (google) to ensure that data been collected by Gemini are being properly protected and ensure that it does not go against any data privacy laws.\n\nConsumers/Users of Google Gemini \nThis set of stakeholders includes people who make use of the Gemini for their different needs which range from generating writing text, images, and programming codes. These users include individual users or businesses. \nThey make use of Gemini for automation of work task and brainstorming of ideas. \nThey use Gemini to generate write ups, images, and videos.\nThey use Gemini to summarize and find quick information in their devices (Gemini Nano).\nLearn new information using Gemini by engaging in open ended back and forth conversations.\n\nRegulators\nThis set of stakeholders include regulatory bodies and individuals that are charged with the responsibility of regulating the creation and usage of all sorts of AI. They include the government of different countries, specially created regulatory bodies etc. \nThey regulate policies regrading the development and usage of generative AI. It is the responsibility of regulatory bodies to come up with legal requirement to make the creation and use of generative AI safe.\nThey ensure Gemini meets all ethical requirement. All ethical concerns should be held with equal importance, and they should ensure that the Gemini and all other generative AI meet these requirements. \nThey make sure Gemini don\u2019t violate laws regulating AI data usage.\n\nEthical risk of Gemini\nHuman rights\nData privacy violation \nThe breach of data privacy is a major ethical concern with the Gemini and other generative AI system as these LLM system are trained on large data sets which sometimes include personal identifiable information of people. A breach of this data privacy could lead to the violation of the individuals human right to privacy and can trigger legal consequences for the company. This could also bring about loss of trust in Gemini by users. Although there is a very low probability of this happening its still a major concern for users. \n\nDeep fakes\nThe ability of Gemini to create perfectly made content that can hardly be differentiated from originals is a major ethical issue. To think that this content could easily be generated by a text prompt from human makes it a bigger issue. The continuous unethical creation and use of deepfakes by harmful individuals increases the quantity of untrusted information in circulation. This leads to a problem of information filtering; it makes it difficult for people to identify which information is right or wrong. There is a higher change of this occurring as it can easily be generated by individuals. \n\nLabour practices\nWorkforce displacement\nThe capability of Gemini to perform human work (such as writing, coding, content creation and so on) more effectively and quickly has become an ethical issue with fear that human will get replaced. As companies continue to invest in this AI and more work roles get automated workers continue to fear that their jobs would get taken over by this generative AI. \n\nThe environment\nCarbon emission\nGenerally generative Ai is said to consume large amount of energy both during their learning stage and as they become widely used by consumers. Although the carbon footprint of newer version of generative AI like Gemini have not been closely studied yet, it has been noted that early similar version like the BERT models emit roughly the same as one person\u2019s round-trip flight on a plane.\n\nFair operating procedures\nCopyright infringement and litigation \nGenerative AI such as Gemini need many data set from different sources to be trained. Some of this data may include copyrighted intellectual properties of other creators being used without their knowledge. It is possible that some generated AI (Gemini) are being used by businesses for generating ideas, some of the ideas which may include images and coding generated by this AI could have been gotten from copyrighted property. This could put user at risk of litigation by original owners of this ideals, image, or write ups. \n\nConsumer issues\nChances of bias\nThere are chances that AI model can be bias. As AI models are being trained with large data which may include bias and stereotypes about certain group of people. There are chances that this AI models inherit and perpetuate these biases toward certain group of people. Although there is a very low chance of this happening with Gemini as google indicated that the data in which Gemini is been trained with has been sorted by professional from various ethnic backgrounds. Although it we cannot be certain that all sort of bias has been completely removed.\n\nMitigation measures for deepfakes\nThe use of deep fakes is an increasing issue with the emergency of generating AI such as Gemini and is considered as one of the most severe ethical implications of Generating AI model. The easy access to tools used for creating deepfakes has given rise to the increasing amount of deepfakes in our current world. This calls for an immediate intervention of the government and AI development companies to implement measures to help mitigate the continuous creation and distribution of deepfakes. Both the government and AI development companies need to invest in the development of tools that can identify fake content as well as educating user and the public on ways to identify deepfakes to reduce the distribution of misleading information. Some already existing tools used for the identification of deepfakes include: \nThe use of forensic analysis. The use of forensic analysis helps to analyse inconsistences in deepfakes which are not present in original medias such as anomalies in audio and visual elements, lightening, facial and body movements. \nUse of counter AI algorithm: with the help of counter AI algorithm which are trained with data sets of both authentic and fake media, discrepancies between deepfakes and authentic content could be identified. \nGovernment policies and legislation: Government in some regions have put in place policies and regulations that address deepfake as well as punishing individuals involved in the intentional creation and distribution of deepfakes. \nWhile all this are in place both government and Generating AI development companies need to keep developing counter measures to address deepfakes as more and more sophisticated generating AI models that can evade the existing ways of detection of deepfakes are being created.\n\nReferences\nBinmile. (September 8, 2023). What Ethical Concerns are Associated with Generative \nAI. https://www.linkedin.com/pulse/what-ethical-concerns-associated-generative-ai-binmile\nCoursera. (February 21, 2024). Google Gemini AI: Meaning, Capabilities, and Use cases. \n\thttps://www.coursera.org/articles/google-gemini-ai\nDhiman, B., & Singh, P. (2023). Exploding AI-Generated Deepfakes and Misinformation: \nA Threat to Global Concern in the 21st Century. ExplodingAI-GeneratedDeepfakesandMisinformation_.pdf\nHiter, S. (June 13, 2023). Generative AI Ethics: Concerns and Solutions. Eweek. \n\thttps://www.eweek.com/artificial-intelligence/generative-ai-ethics/\nLawton, G. (November 1, 2023). Generative AI ethics: 8 concerns and risks. TechTarget. \nhttps://www.techtarget.com/searchenterpriseai/tip/Generative-AI-ethics-8-biggest-concerns\nPichai, S., Hassabis, D. (December 6, 2023). Introducing Gemini: Our Largest and most \nCapable AI Model. Google. https://blog.google/technology/ai/google-gemini-ai/#sundar-note"
    },
    {
      "role": "assistant",
      "content": "{\n    \"feedback\": \"An appropriately structured analysis but short on detail for google gemini LLM.\",\n    \"description / 20\": 55,\n    \"roles / 10\": 55,\n    \"risks / 40\": 50,\n    \"mitigation / 20\": 55,\n    \"refs / 10\": 42,\n    \"total score\": 51.7\n}"
    }
  ],
  [
    {
      "role": "system",
      "content": "You are a professional educator. \nYour target is to assess students' assignments based on the provided rubric (between <rubric> and <rubric> labels).\n<rubric>\nAssignment: Investigating and Analysing Ethical Risks of a Digital Engagement application \n\nThis is an INDIVIDUAL assignment on Ethical Risk Assessment for the TEU00062 module. This assessment is worth 30% of the total TEU 00062 module mark. The deadline for submission is 12.00. noon Friday 31st March 2024. \n \nLearning outcomes: Upon completion of this assignment students should:\nBe able to identify the different stakeholders and organisational roles involved in developing and using AI-driven digital engagement application, and motivations. \nBe able to identify and analyse ethical risks in a digital engagement application.\nBe equipped to identify technical and governance mitigations of these risks\n \nSteps:\n \nSelect ONE real world application that embodies one or more AI-driven digital engagement techniques you have learnt about on this course, e.g. e.g. digital engagement techniques used in one applications such as social media, newsfeeds, search, eCommerce, elearning or digital humanities., or in an application you are familiar with e.g. interactive Game, Virtual Presence (i.e. VR /AR) etc. \nIdentify the different value chain stakeholder roles involved in this application scenario and where possible the actors (i.e. companies, organisations) carrying out those roles. including those involved in: data collection; AI model development; development and use of the application. \nUse social responsibility principles (see ethic lecture week 2) to identify and analyse real or potential ethical risks of the application, including classification of the affected stakeholders (hint: see also risks classified by Latzer et al 2016 \u2013 ethics lecture week 1), how severe the risk would be and how likely it is to occur.\nIdentify one or more mitigation measures that specific value-chain stakeholders may undertake to minimise the impact of the risk you think is most harmful. These could be technical measures, organisational governance measures or measure involving additional engagement with affected individuals or groups.\n \nStructure of Assignment Report. \nStructure the Report (4000 word max excluding figures and references), under the following heading [indicative percentage of assignment mark per section]. Please provide references\n \nDescription of Application [20%]\n<describe your selected application, explaining what it does, the role AI and data collection plays in its digital engagement features, and the benefits it provides>\n\nIdentification of Stakeholder Roles involved in the application and its governance [10%]\n<describe the stakeholder roles undertaken in the development, use and governance of the application, and which organization take those roles and the benefits they derive from their involvement. Include roles that may be indirectly affected by the application. >\n\nIdentification of Ethical Risks [40%]\n<Identify the ethical risks that may be raised by your application under the social responsibility categories below (more details in ethics lecture week 2). For each risk identify which stakeholder may be affected, and assess how severe the impact would be for each stakeholder (Low, Medium, High) and the likelihood of that risk being incurred (Low, Medium, High), justifying your assessment, even if you think there is no risk under that heading.>\nHuman Rights\nLabour Practices\nThe Environment\nFair Operating Procedures\nConsumer Issues\nCommunity Involvement and Development\n \nDiscussion of Mitigations Measures for Risk [20%]\n< For one of the risks you consider more severe, describe mitigation measures that could be taken. Explain how the measure would reduce or eliminate the likelihood and impact of the risk. Explain which stakeholders would be involved, or would have to interact in implementing the measure, identifying if this is a governance measure an organization can undertake itself or in collaboration with other parties, e.g. regulators or external stakeholder..>\n\nReferences [10%]\nInclude references to source you use for you analysis, which could be academic or policy papers, blogs, information from company web site or news stories.\n\n</rubric>\nThe assignment content will be provided by user (not in system prompt)\nDo not say anything else except the marking, following the format of:\n{\n    \"feedback\": \"feedback content\",\n    \"(rubric part 1) / (rubric part 1 weight)\": score,\n    \"(rubric part 2) / (rubric part 2 weight)\": score,\n    ......other rubric parts\n    \"total score\": weighted average score of all parts\n}"
    },
    {
      "role": "user",
      "content": "Table Of Contents\nIntroduction\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u20264\nDescription of application\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u20264\nIdentification of Stakeholder Roles involved in the application and its governance\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026..7\nIdentification of Ethical Risks\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026.8\nDiscussion of Mitigations Measures for Risk\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u202612\nReferences\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u202613\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFacebook\u2019s (META\u2019s) Content Personalization and Recommendation Algorithms\n\nIntroduction\nSocial media platforms have gone past their role as ways of communication, evolving into complex environments that shape how information is consumed and shared by the users. At the heart of this change are artificial intelligence (AI)-driven content personalization and recommendation algorithms, with Facebook being a prime example of this evolution. These algorithms are designed to give and recommend content that users prefer to see, therefore enhancing user engagement and satisfaction on platforms like Facebook. Even though these technical developments have greatly benefited society, they have also brought up important ethical issues that I will discuss in this report. In particular, the ethical environment of Facebook's recommendation and personalisation algorithms will be examined, along with important stakeholders, ethical ricks, and mitigation techniques.\n\nDescription of Application \nThe Functionality of Facebook's Personalization and Recommendation Algorithms\nFacebook links billions of users worldwide. Their recommendation and content personalisation algorithms are what lets the company reach such a high number of users. These algorithms are engineered to make tailored feeds for individuals, suggesting content such as posts, news articles, videos, and advertisements based on their historical behaviour and preferences. This focus on tailored content delivery is central to Facebook's strategy, aiming to maximize user engagement and satisfaction by giving the user the most appealing content.\nIn order to deliver relevant posts to users based on what they like and engage with, sourced from \u2018\u2018Here\u2019s How the Facebook Algorithm Works in 2024\u2019 sourced below, the algorithms main components include :\n\nListing of Posts: Quickly gathers posts from individuals, groups, Pages, and commercial accounts, with a preference for non-commercial sources.\nSignals Identification: Uses hashtags, keywords, and other signals to understand and categorize post content.\nAI Predictions: Predicts engagement likelihood based on historical user data, including likes, group memberships, and post topics.\nRelevancy Scoring: Assigns scores to posts based on predicted user engagement, favouring content likely to interest the user.\n\nMeta still needs to narrow down the options before recommending content to users. Facebook\u2019s algorithm considers the following signals in Figure 1 before pushing posts to the user\u2019s news feed.\n\n\nFigure 1 : Content Recommendation Decision making. Image from \u2018Here\u2019s How the Facebook Algorithm Works in 2024\u2019 referenced.\n\nType of content: This is about whether the post is a video, picture, or link, and how long people spend looking at it compared to other similar posts. It basically asks what kind of posts are catching people's eyes the most.\nRecency: This refers to when the post was shared. Usually, newer posts are preferred because old news isn't as interesting to most people. There might be a few exceptions, but they're rare.\nRelationships: This looks at how much you interact with others and their pages on Facebook. For example, if you often like or comment on your college roommate's posts, you'll see their posts more on your feed. The same goes if you're active on a particular page.\nPopularity or engagement: If a post gets a lot of likes, comments, or shares, it means it's popular. Popular posts are shown to more people, even if they don't usually engage with that kind of content.\nCreator's performance over time: This considers how well the person or page who made the post has done in the past. If their posts usually get a lot of attention, their new posts are likely to be shown to more people.\n\nThe Role of AI and Data Collection\nFacebook's AI algorithms are not fixed, they continuously learn from new data, which is fundamental to the effectiveness of the platform's content personalization. As users interact with content, the algorithms fine-tune the recommendations in real-time, creating a highly personalized and engaging user experience. These AI models progressively identify nuanced user preferences, enhancing the content personalization over time. \nHowever, a study based on an online survey (taken from \u2018Making sense of algorithmic profiling: user perceptions on Facebook\u2019, referenced in the \u2018References\u2019 section) indicates that this process may not always accurately reflect individual identities. In the study, participants assessed the accuracy of Facebook's 'Your Categories' section in the Ad Preferences, revealing a spread of perceptions: 27.7% felt the categories reflected them inaccurately, while 47.6% believed they were accurate, as illustrated in Figure 2. This suggests a potential difference in the platform's ability to understand and reflect users' true interests and identities. Such insights emphasize the complexities and limitations of algorithmic profiling.\n\n\nFigure 2: the accuracy of Facebook's ad categories\n\nAnother crucial aspect of Facebook's digital engagement strategy is its recommendation algorithms. These algorithms are intended to present users with fresh information, relevant pages, and groups based on their interests By analysing the collective data of users with similar interests and habits, the AI may suggest new content that individuals are likely to find fascinating even if they haven't previously interacted with similar content. Maintaining up-to-date and pertinent content not only boosts user engagement but also facilitates communication between content creators and advertisers and their target audience.\nHowever, the dependence on AI and data collection for content personalisation and suggestion has brought to light concerns about data security, privacy, and the potential for echo chambers. By providing users with more control and more transparent data procedures, Meta has been attempting to allay these worries. over their data and the content they see.\n\nThe benefits provided\nFacebook's AI-powered algorithms have revolutionised social media interaction by enhancing user experience, increasing engagement, and personalising content for individual users. By selecting material according to user preferences and behaviour, these algorithms increase user happiness and lengthen engagement times. They also aid in discovering new content, creators, and communities, improving the user experience, and community building. For advertisers and content creators, these algorithms offer targeted reach, improving ad value and resource use. Despite potential concerns about echo chambers, the algorithms promote content diversity and encourage exploration beyond usual interests, demonstrating the substantial benefits of AI in social media.\nIdentification of Stakeholder Roles involved in the application and its governance. \nIn analysing the complex landscape of Facebook's content personalization and recommendation algorithms, a variety of stakeholders play pivotal roles in its application. \nDirect Stakeholders\nMeta (formerly Facebook Inc.) Meta's primary benefit is economic, deriving significant revenue from targeted advertising aided by personalized content. Leadership figures such as Nick Clegg, Meta\u2019s president of global affairs, underscores the importance of direct stakeholders in steering the ethical direction of Meta's AI tools. Clegg plays a crucial role in the decision making in relation to META\u2019s AI tools, as he \u201canticipates and addresses legal, regulatory, moral, ethical and cultural issues that may arise with the introduction of new AI products\u201d, source \u2013 \u201cMeta Addresses Legal and Ethical Issues in AI Development\u201d referenced below.\nUsers of the platform are both consumers and producers of content. Their interaction with the application feeds the data algorithms need for personalization, creating a loop that enhances their experience. The benefit for users is a more relevant and engaging social media experience, leading to increased satisfaction and time spent on the platform.\nAdvertisers are pivotal in this ecosystem, leveraging the personalized recommendation system to target ads more effectively. The algorithmic targeting allows for a higher return on investment, as ads are more likely to reach interested users.\nIndirect Stakeholders\nRegulatory Bodies and Policymakers globally impact the control of Facebook's algorithms. They ensure the platform adheres to privacy laws, content regulation, and ethical standards. Their involvement benefits the public by protecting privacy rights and ensuring content does not harm societal norms. The ongoing developments in AI control, as demonstrated by Meta's efforts to address ethical, legal, and cultural challenges, highlight the dynamic interplay between technology companies and regulatory frameworks aimed at ensuring responsible AI use.\nData Providers, such as third-party websites and apps, indirectly participate by supplying additional user data to improve personalization algorithms. These entities benefit through partnerships, data sharing agreements, or financially.\nAcademic and Research Institutions contribute through research and development, often working with Meta to improve algorithm models or investigate the social impacts of algorithm-driven content curation. Their involvement helps in developing more ethical and effective algorithms.\nEach stakeholder in this big system gets specific benefits, from economic gains to societal value. The interaction between these roles is what I think to be creating this on going evolution of Facebook\u2019s content personalization and ethical risks.\n\nIdentification of Ethical Risks\nIn this section I will Identify ethical risks, assess the severity and likelihood of occurrence, identify affected stakeholders and I will justify my assessments. Using the principles of social responsibility outlined ethical digital engagement lecture 2 slides, and the risk classifications by Latzer et al. (2016) from lecture 1 slides, I will identify and analyse the ethical risks associated with META's algorithms.\n\nHuman Rights\nRisks to Civil and Political Rights: By interfering with democratic processes like elections and limiting freedom of speech, false news, social media bots, deepfake videos, filter bubbles, and censorship pose serious threats to civil and political rights. Referenced as \u2018An Ethics Perspective On Facebook\u2019 Michael Thate, who teaches Ethics at Princeton University expresses how Facebook's algorithms may prioritize engagement at the expense of these rights. Stakeholders affected include the general public, who may suffer from misinformation and manipulation, and political entities, whose reputations and operational integrity may be compromised. The severity of impact on these stakeholders is high due to the potential to influence election outcomes and public opinion. The likelihood of these risks is also high, due to the amount of content generated and shared on social media platforms. Justification for these assessments is based on documented instances of election interference and large dissemination of fake news.\nComplicity in Partner Violations of Rights: META's collaboration with value chain partners who may commit rights violations presents a complicity risk. This includes partners involved in data collection and processing who may misuse personal data, violating privacy rights. Stakeholders affected are users of META's platforms, whose personal data may be at risk, and bodies reliant on META's algorithms for business operations. The impact severity can be high for individuals whose data is misused, leading to privacy breaches and related harms. The likelihood of incurring this risk is medium in my opinion, relying upon META's due diligence processes and partnership policies.\n\nLabour Practices\nAI Automation Leading to Labour Displacement: The advancement of AI within META's operations posits a significant risk of labour displacement, particularly affecting employees in operational and content moderation roles. I believe the impact severity as high, given the economic and social results for displaced individuals. I also consider the likelihood to be high due to the technological path towards increasing automation. \nWorker Physical and Mental Health: The physical and mental well-being of content moderators and operational staff develops as a critical concern, with both the impact severity and likelihood high in my opinion. Employees in these roles are exposed to potentially upsetting content. This exposure shows the ethical requirement for META to establish robust support systems. My justification for the high impact severity is from the direct correlation between job function and exposure to harmful content, which can cause significant health issues.\nLegal Risks Arising from Discrimination in AI-Assisted Recruiting: The deployment of AI in recruiting processes introduces potential legal risks related to discrimination, affecting potential employees. The impact severity is medium however, the likelihood is assessed as low in my opinion, due META's measures to refine AI algorithms and maintain ethical AI practices.\n\nThe Environment \nIncreased Carbon Emission Due to AI Training and Service Operation\nThe process of training complex AI models and maintaining META's extensive service operations has a significant environmental impact, chiefly due to the high energy consumption that leads to increased carbon emissions. This is worrying because it speeds up climate change, which has an impact on ecosystems all around the world and puts people's health at risk. The public at large, and actually the next generation are the stakeholders that are most impacted. The severity of the impact is considered high outlined as \u201cAI has a fast-growing carbon footprint\u201d by Sundberg, G, in Tackling AI\u2019s Climate Change Problem, and the likelihood of occurrence is also high, given the ever-increasing demand for more sophisticated and engaging content. \nResource Usage and Pollution from AI-Driven Product Creation and Disposal\nThe production and disposal of AI-driven products pose medium environmental risks, affecting local communities, regulatory bodies, and environmental NGOs with medium likelihood and severity. Despite recycling efforts, challenges in managing resource depletion and pollution persist. META's role in this cycle demands enhanced sustainability, including renewable energy investments and better recycling, to mitigate impacts on key stakeholders.\nFair Operating Procedures\nUse of AI in Corrupt or Anti-competitive Practices: This risk involves the manipulation of financial, investment, or procurement processes. Stakeholders affected include competitors, investors, and the general public. The impact on competitors and the market at large is High, due to the potential for market bias and unfair advantage. For investors, the impact is Medium, influenced by potential financial gains or losses. The likelihood of this risk is Medium, given the increasing scrutiny and monitoring of tech giants. \nUndermining the Public Political Process: The use of AI for excessive lobbying, creating deep fakes, or spreading targeted misinformation impacts voters, political entities, and public trust in democratic processes. The severity of the impact on the public and democratic institutions is High, due to the potential to significantly influence public opinion and election outcomes. The likelihood of occurrence is also High, considering past instances of misinformation spread via social platforms. \nViolation of Intellectual Property Rights: This involves the unauthorized use of copyrighted material through AI algorithms. Content creators and copyright holders are the primary stakeholders affected, facing a High impact due to potential loss of revenue. The likelihood of this risk is Medium, reflecting the ongoing challenges in effectively policing content on large platforms. \nConsumer Issues\nUnsustainable Consumption \nThe risk of fostering unsustainable consumption patterns is high, particularly as personalized content can encourage overconsumption by promoting products and services tailored to users' interests. This issue is outlined by Michael Thate again, by the \"engagement game,\" a strategy where platforms like Facebook design algorithms to keep users engaged for as long as possible. This approach not only aims to increase the time users spend on the platform but also aggressively pushes them towards consuming more content and products, often without regard to the consequences on their financial well-being or the environment. Such relentless pursuit of user engagement and spending magnifies the risk of overconsumption. The likelihood of this risk is high, considering the algorithm's underlying goal to maximize engagement and spending. The impact on consumers is significant, leading to potential financial distress and contributing to environmental degradation. \nMisuse of Personal Data The misuse of personal data represents a critical ethical risk, impacting users' privacy and autonomy. Stakeholders affected include the platform's users and external entities that might exploit this data without consent, as Facebook is a popular target for hackers, outlined by Jain, G, in \u2018Ethical issues faced by Facebook\u2019, referenced below. The severity of the impact is high, as misuse of personal data can lead to identity theft, privacy invasion, and manipulation. The probability of this risk is also high, given the vast amount of data collected and the complexities of data management and protection. My justification for this assessment comes from numerous instances of data breaches and misuse.\nDenial of Access to Essential Services Denial of access to essential services, due to algorithmic biases or operational policies, affects marginalized communities the most. This risk has a medium likelihood, as efforts are made to improve inclusivity and access, yet gaps remain due to inherent biases in data and algorithm design. The impact of this risk is high for affected stakeholders, as it can worsen social inequalities by hindering access to vital information and services. My justification for this assessment comes from documented cases where algorithms have mistakeably marginalized certain groups.\nCommunity Involvement and Development\nDifficulty Identifying Communities Suffering Negative Impact of AI Use\nStakeholders affected include social networks, road users, medical patients, and even non-users. The risk severity for community cohesion and individual wellbeing is High, given the widespread influence of META's platform. The likelihood of this risk is also High, as algorithms often prioritize engagement over societal wellbeing, potentially leaving out the needs and harms to specific groups. My assessment is supported by the categorization of risks to societal wellbeing outlined by Latzer et al. (2016).\nNegative Impact on Local Health, Employment, and Wellbeing\nThe pervasive reach of META's algorithms can worsen issues like deskilling, impact child development negatively, and fuel culture wars, thereby harming local health, employment, and general wellbeing. Stakeholders here include local businesses, educational institutions, and healthcare providers. The impact severity ranges from Medium to High depending on the community's reliance on digital platforms for information and social interaction. The likelihood of this risk is Medium, as it depends on local extent of digital integration in daily life. However, when these risks materialize, they can have profound long-term effects on community development.\nConcentration of AI\u2019s Wealth and Income Creation\nThis risk pertains to the economic disparities intensified by the centralization of wealth creation within AI-driven companies like META, at the expense of local communities. Affected parties include employees, local business owners, and the local economy as a whole. The likelihood of this happening is High, mirroring global trends of wealth accumulation in the tech sector and its effects on economic diversity and opportunity distribution.\n\nDiscussion of Mitigations Measures for Misuse of personal data\nTo address the misuse of personal data on Facebook, the mitigation technique that I have chosen is discussed in the study \u2018Mitigating the Privacy Paradox Through Higher Privacy Literacy? Insights from a Lab Experiment Based on Facebook Data\u2019 referenced below.\nA lab experiment looking at whether more privacy literacy could help with the \"privacy paradox,\" which refers to the difference between people's actual online sharing activities and their expressed worries about privacy, among Facebook users. The study found no relationship between Facebook users' levels of privacy concerns and how infrequently they use the social network. A greater tendency towards privacy-protective behaviour on Facebook is driven by a mix of high privacy literacy and high privacy concerns.\nImproving user privacy literacy may be essential to reducing the risk of personal data exploitation. Creating and distributing thorough teaching materials about online privacy, the possible dangers of oversharing personal information, and useful techniques for controlling social media privacy settings are all part of this proposed strategy. To guarantee broad reach and effectiveness, workshops, online courses, and awareness campaigns could be planned with academic institutions, privacy advocacy groups, and regulatory agencies.\nImpact on Risk Reduction: By closing this knowledge gap, users are more likely to take actions that protect their privacy, like changing their privacy settings to restrict data sharing and posting only the information they want to share publicly. By reducing the quantity of information that is easily accessible to third parties, this lessens the possibility that personal data will be misused and mitigates potential dangers to user privacy.\nStakeholders Involved:\nUsers: Direct recipients of privacy literacy projects, responsible for applying knowledge to manage their privacy settings and data sharing practices more effectively.\nFacebook : Must develop and provide accessible privacy education resources and tools. They play a crucial role in designing user interfaces that promote understanding and ease of use of privacy settings.\nEducational Institutions and Non-profits: Can collaborate to design and implement privacy literacy programs, leveraging their expertise in education and outreach to reach a broader audience.\nRegulators and Policy-makers: Advocating for policies that require social media platforms to adopt transparent practices and provide comprehensive privacy education to users.\nBy empowering users and lowering the danger of data exploitation, addressing Facebook's misuse of personal data and educating consumers about it calls for cooperation from all parties involved in creating a safer online space where privacy issues are adequately handled.\n\nReferences\n[Meta, How Facebook distributes content\nAvailable at: https://www.facebook.com/business/help/718033381901819?id=208060977200861\n\nMeta, 2023, The AI behind unconnected content recommendations on Facebook and Instagram \nAvailable at: https://ai.meta.com/blog/ai-unconnected-content-recommendations-facebook-instagram/\n\nOuchchy, L, Coin,  A, and Dubljevi\u0107, V. 2020, AI in the headlines: the portrayal of the ethical issues of artificial intelligence in the media\nAvailable at: https://link.springer.com/article/10.1007/s00146-020-00965-5\n\nFosch-Villaronga, E, 2021, Making sense of algorithmic profiling: user perceptions on Facebook,\nAvailable at : https://www.tandfonline.com/doi/full/10.1080/1369118X.2021.1989011?scroll=top&needAccess=true\n\nHitlin, P and Rainie, L, 2019 : Facebook Algorithms and Personal Data\nAvailable at: https://d1wqtxts1xzle7.cloudfront.net/59250358/Facebook_algorithms_report20190514-34280-jrzed3-libre.pdf?1557866216=&response-content-disposition=inline%3B+filename%3DFacebook_Algorithms_and_Personal_Data.pdf&Expires=1711802514&Signature=PDApGaKN-s1r92F~3LxS5Vdc2ZDR5~QeZrXDhoWhoG8jAOUBMjllNsMaTt~bveAPguTFtAcqX7URZlrG-D7yjTr9i7Eg0ALsM8RUsNKBMA0onTEDOEg89SlAXbUg5~b0p7MUxxlaT8n2ZpqyWW6ACSL3B~rtRgmEqOirylz9hOCv4WULEYs5mAamlV5E2rZXftpzhkgq9F5ODbqsgpLizE8HK7sZuj31~7fUJjTJ5BbFUNLhGl~Qws6mSSXB-aru1IZAEQvNEuDuPrWYRmOM~fY2dVZRU4Vn759j4v~iuFPKSPZLgMuptZREug8n7ROekzJ7WV5ZJRcKs3kK6hQtpg__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA\n\nMiller, G, 2022, Stakeholder roles in artificial intelligence projects\nAvailable at : https://www.sciencedirect.com/science/article/pii/S266672152200028X\nPYMNTS.com, 2023, Meta Addresses Legal and Ethical Issues in AI Development\nAvailable at : https://www.pymnts.com/artificial-intelligence-2/2023/meta-addresses-legal-and-ethical-issues-in-ai-development/\n\nTiku, N, 2023, AI researchers uncover ethical, legal risks to using popular data sets\nAvailable at : https://www.washingtonpost.com/technology/2023/10/25/data-provenance/\n\nJain, G, 2021, Ethical Issues Faced by Facebook\nAvailable at : https://mvgt-gautam.medium.com/ethical-issues-faced-by-facebook-113a7296895f\n\nSteinhorst, C, 2021 : An ethics perspective on Facebook\nAvailable at : https://www.forbes.com/sites/curtsteinhorst/2021/10/22/an-ethics-perspective-on-facebook/?sh=6620a5251a3e\n\nSchaffer, N, 2024 : Here\u2019s how the Facebook algorithm works in 2024\nAvailable at : https://nealschaffer.com/facebook-algorithm/\n\nSundberg, N, 2023, Tackling AI\u2019s Climate Change Problem. \nAvailable at : https://sloanreview.mit.edu/article/tackling-ais-climate-change-problem/\n\nSchubert, R, Marinica, I, Mosetti, L, and Bajka, S, 2022 : Mitigating the Privacy Paradox Through Higher Privacy Literacy? Insights from a Lab Experiment Based on Facebook Data\nAvailable at : https://deliverypdf.ssrn.com/delivery.php?ID=014117064098121090070101018064003069057020066018053053081103097094127113075127014093100057062011027030018064003002108013095003051035011087029126096106097121095096122061001031089117003116076068086030026022085120113025087028027122073067027023064111076002&EXT=pdf&INDEX=TRUE"
    },
    {
      "role": "assistant",
      "content": "{\n    \"feedback\": \"A well structure analysis of Facebook's ethical issues.\",\n    \"description / 20\": 65,\n    \"roles / 10\": 65,\n    \"risks / 40\": 60,\n    \"mitigation / 20\": 65,\n    \"refs / 10\": 65,\n    \"total score\": 63.0\n}"
    }
  ],
  [
    {
      "role": "system",
      "content": "You are a professional educator. \nYour target is to assess students' assignments based on the provided rubric (between <rubric> and <rubric> labels).\n<rubric>\nAssignment: Investigating and Analysing Ethical Risks of a Digital Engagement application \n\nThis is an INDIVIDUAL assignment on Ethical Risk Assessment for the TEU00062 module. This assessment is worth 30% of the total TEU 00062 module mark. The deadline for submission is 12.00. noon Friday 31st March 2024. \n \nLearning outcomes: Upon completion of this assignment students should:\nBe able to identify the different stakeholders and organisational roles involved in developing and using AI-driven digital engagement application, and motivations. \nBe able to identify and analyse ethical risks in a digital engagement application.\nBe equipped to identify technical and governance mitigations of these risks\n \nSteps:\n \nSelect ONE real world application that embodies one or more AI-driven digital engagement techniques you have learnt about on this course, e.g. e.g. digital engagement techniques used in one applications such as social media, newsfeeds, search, eCommerce, elearning or digital humanities., or in an application you are familiar with e.g. interactive Game, Virtual Presence (i.e. VR /AR) etc. \nIdentify the different value chain stakeholder roles involved in this application scenario and where possible the actors (i.e. companies, organisations) carrying out those roles. including those involved in: data collection; AI model development; development and use of the application. \nUse social responsibility principles (see ethic lecture week 2) to identify and analyse real or potential ethical risks of the application, including classification of the affected stakeholders (hint: see also risks classified by Latzer et al 2016 \u2013 ethics lecture week 1), how severe the risk would be and how likely it is to occur.\nIdentify one or more mitigation measures that specific value-chain stakeholders may undertake to minimise the impact of the risk you think is most harmful. These could be technical measures, organisational governance measures or measure involving additional engagement with affected individuals or groups.\n \nStructure of Assignment Report. \nStructure the Report (4000 word max excluding figures and references), under the following heading [indicative percentage of assignment mark per section]. Please provide references\n \nDescription of Application [20%]\n<describe your selected application, explaining what it does, the role AI and data collection plays in its digital engagement features, and the benefits it provides>\n\nIdentification of Stakeholder Roles involved in the application and its governance [10%]\n<describe the stakeholder roles undertaken in the development, use and governance of the application, and which organization take those roles and the benefits they derive from their involvement. Include roles that may be indirectly affected by the application. >\n\nIdentification of Ethical Risks [40%]\n<Identify the ethical risks that may be raised by your application under the social responsibility categories below (more details in ethics lecture week 2). For each risk identify which stakeholder may be affected, and assess how severe the impact would be for each stakeholder (Low, Medium, High) and the likelihood of that risk being incurred (Low, Medium, High), justifying your assessment, even if you think there is no risk under that heading.>\nHuman Rights\nLabour Practices\nThe Environment\nFair Operating Procedures\nConsumer Issues\nCommunity Involvement and Development\n \nDiscussion of Mitigations Measures for Risk [20%]\n< For one of the risks you consider more severe, describe mitigation measures that could be taken. Explain how the measure would reduce or eliminate the likelihood and impact of the risk. Explain which stakeholders would be involved, or would have to interact in implementing the measure, identifying if this is a governance measure an organization can undertake itself or in collaboration with other parties, e.g. regulators or external stakeholder..>\n\nReferences [10%]\nInclude references to source you use for you analysis, which could be academic or policy papers, blogs, information from company web site or news stories.\n\n</rubric>\nThe assignment content will be provided by user (not in system prompt)\nDo not say anything else except the marking, following the format of:\n{\n    \"feedback\": \"feedback content\",\n    \"(rubric part 1) / (rubric part 1 weight)\": score,\n    \"(rubric part 2) / (rubric part 2 weight)\": score,\n    ......other rubric parts\n    \"total score\": weighted average score of all parts\n}"
    },
    {
      "role": "user",
      "content": "Description of Application \nThe application selected for this assignment is Netflix. Netflix is an online, ad free streaming platform where users pay a subscription fee to access a wide collection of movies, shows, documentaries and more, as well as exclusive content that is only available on Netflix called \u2018Netflix Originals\u2019. Users can access this content both online and offline. \nMachine learning algorithms, which fall under the umbrella of AI, are used to tailor recommendations for its users. There are 2 main forms of data that Netflix collects from its user; implicit data and explicit data (Plummer, 2017). Explicit data is the data that the users directly share, such as clicking the \u2018thumbs up button\u2019 if they liked what they just watched (see Fig. 1). This data is then used to recommend content similar to the one the user just liked, however these recommendations are not just based on the genre, it is also based on the actors that star in it, and where the movie is set (Spangler, 2022). Netflix\u2019s employees tag such factors (genre, actors present, the setting etc) in all of their content and compile this data to group this content into categories (Spangler, 2022). Netflix then collects implicit data from its users which consists of behavioural data. Implicit data includes watch time by users, for example if a user stopped watching a movie after 10 minutes and then never finished watching it, this would suggest they did not enjoy this movie and would not watch movies similar to it. Netflix then combines both implicit and explicit data, and its ML algorithms have the tools needed to form recommendations tailored to each user (Netflix Research, n.d.). These  ML algorithms find behavioural similarities between users, Todd Yellin, Netflix\u2019s Vice President, describes users with similar tastes as \u2018Taste Communities\u2019, where users that make up a certain \u2018Taste Community\u2019 are recommended similar content (Spangler, 2022). This customised user experience by AI enhances users digital engagement as it makes finding new content easier than ever before. When a user has a good experience with Netflix\u2019s recommendations, it is likely they will return to watch more recommended content, thus continuing user engagement with the application. The more content watched, the more accurate the recommendations become, and the user's experience and engagement will continue to benefit from Netflixs ML algorithms. \n\n\n\nFigure 1. Image depicts a screenshot of the Netflix app. The user has 3 options; (i) to thumbs down the show meaning this content is \u2018not for me\u2019, (ii) to thumbs up the show, meaning they like this content, or (iii) to two thumbs up the show, meaning they love it. This is an example of explicit data that the user provides. \nImage source: Netflix. \n\nAnother way in which AI is utilised within the application is through the production of multiple thumbnails for the one movie/show. Certain thumbnails are selected over others to increase click through rates (CTR) from users. Rather than just using a movie /shows original cover art, Netflix generates multiple thumbnails for the content on its platform. Out of the multiple thumbnails produced, specific thumbnails are selected over others based on what an individual user is most likely to click. This digital engagement feature improves CTR as it entices users to watch content that they may have already scrolled past due to its previous thumbnail. The thumbnail is generated by Aesthetic Visual Analysis (AVA), where thousands of frames are scanned for certain factors such as the brightness of the frame, the skin tones of the people within the frame presented and the overall quality of the image (Yu, 2019). Two main factors considered when determining what thumbnail include the theme of the thumbnail and who is displayed in the thumbnail. One example illustrated by Yu is the various thumbnails used for the series \u2018Riverdale. For users that are avid thriller enthusiasts, a moody and thriller-esque thumbnail is presented, however for users that enjoy the romance genre more than thrillers, such users are presented with a light hearted, brighter image that includes pastel colours which contribute to the romantic aesthetic the thumbnail presents (See Fig. 2), (Yu, 2019). Riverdale does include both genres of thriller and romance, so having 2 different thumbnails to optimise CTR for different users interests is a clever tactic to increase user engagement. Netflix has recently implemented a type of ML called \u2018contextualisation bandits\u2019 which allows the application to change thumbnails that a user sees on a daily basis (Roth, 2020). Contextualisation bandits save time as thumbnails can be adjusted by using real time data from the user, whereas previous ML produces thumbnails based on data collected over a longer period of time. The use of multiple, customised thumbnails saves Netflix money, as users are encouraged to watch shows they previously would not have given second thought to, meaning Netflix does not have to licence more content for its platform. Contextualisation bandits cleverly utilises the content Netflix already has, and simply moulds it to meet the needs of multiple \u2018taste communities\u2019. \nTherefore, the use of ML allows Netflix to form customised thumbnails and content recommendations which improves overall user satisfaction and engagement, and saves the application money in the long run. \n\n\nFigure 2. The left image depicts a light hearted, romantic thumbnail for the series \u2018Riverdale\u2019. The right image depicts a moodier, thriller-like thumbnail for the same series. This is an example of two thumbnails that can be presented to a user to increase CTR. \nImage source: (Yu 2019)\n\nIdentification of Stakeholder Roles involved in the application and its governance\n\nOne stakeholder includes Netflix inc., which employs approximately 13,000 people, therefore the company itself is a central stakeholder to the application (Cuofano, 2024). Netflix inc. is responsible for the running of the platform and providing users with the streaming service. The application\u2019s success is a result of the work that the employees within the company put into the application. Roles that the Netflix inc. is responsible for include; the sourcing and provision of content for user\u2019s, generating revenue for the application by charging user\u2019s a subscription fee for its service, constantly improving the application by implementing AI and improved algorithms to provide the user with the best possible experience, thus outcompeting other streaming services. Netflix inc. is responsible for increasing user digital engagement, therefore constant innovations such as the implementation of new ML like \u2018contextualisation bandits\u2019 is implemented to achieve this goal. Netflix inc. is also responsible for collecting and analysing user data, and using this data in a way to improve the application itself without breaching the privacy of the user. As well as licensing content for the platform, Netflix inc. is also responsible for in-house productions of content for the application, this content is called \u2018Netflix Originals\u2019 where Netflix owns 100% of the production and distribution rights (Robinson, 2018).  Regarding governance, Netflix\u2019s board of directors oversees the applications governance and makes decisions to ensure that Netflix inc. complies with current regulatory requirements. \n\nThe users of the platform are called subscribers as they pay a subscription fee for this streaming service, therefore subscribers are another stakeholder. Netflix has approximately 260 million subscribers (Shewale, 2024). The role of this stakeholder is to generate revenue for the company, as well as to provide implicit and explicit data for Netflix to then use appropriately to improve the user experience and to improve the application itself. Therefore subscribers govern how and what data Netflix can analyse. The subscribers benefit from this platform service as they can stream content both online and offline, as well as watch ad free content and have access to Netflix Original content which is not available on any other streaming site. \n\nAnother stakeholder includes the companies whose content is featured on Netflix. This content is licensed by Netflix and is therefore different from Netflix Originals. The role of such content providers is to provide content that will entice Netflix users and improve CTR and viewership. If content providers stopped licensing their content, Netflix\u2019s platform would crumble as \u00be of their platform consists of licensed content (Welk, 2023). Content providers benefit from Netflix licensing their content as they receive large sums of money for doing so.  Netflix also provides them with a new platform for their content to be streamed on and enables their content to reach new audiences. Content providers govern how the licensed content is used and the duration of its usage on Netflix\u2019s platform.  \n\nRegulatory bodies conclude the list of stakeholders for this application. The main role of regulatory bodies, such as the European Data Protection Board, is to ensure Netflix complies with current regulations like General Data Protection Regulation (GDPR) which is enforced in countries within the European Union. Regulatory bodies therefore ensure that a user's personal data is protected (European Parliament & Council of the European Union, 2018). Regulatory bodies also regulate and therefore govern the content that is available on Netflix.\n\n\nIdentification of Ethical Risks \n\nRisk to human rights \nNetflix company  \nData security risks: Netflix could be subjected to legal difficulties if they do not keep their user\u2019s data protected. \nSeverity of data security risk: High severity risk as it could cost Netflix a lot of money if they are taken to court over legal issues  regarding user privacy violations. \nLikelihood of data security risk: Medium likelihood of this risk, Netflix has disclosed that it does not sell user data and that any information collected about a user can be requested by the user to see and/or to be deleted (Netflix inc, no date), however the use of IP addresses to crack down on password sharing increases the likelihood of this risk. \n\nRisk to just and favourable work: Just and favourable work is at risk if Netflix licences content that was made under exploitative conditions.\nSeverity of this risk: High severity risk as it violates worker\u2019s conditions and would support exploitative labour. \nLikelihood of this risk: Medium likelihood risk as Netflix is under constant pressure to licence more content, and therefore thorough background checks may not be completed of working conditions in the studio. \n\nSubscribers \nLegal risks: if a user\u2019s privacy is violated by Netflix, the user may face legal action with Netflix. \nSeverity of this risk:  Low severity risk as it would cost a user a lot of money to pursue legal action against Netflix.\nLikelihood of this risk: Low likelihood risk as users agree to what data they are okay with sharing when they create a Netflix account. \n\nRisk to just and favourable work: users could unknowingly support exploitative labour through their subscription fee as this revenue could go towards content produced in exploitative conditions.\nSeverity of this risk:  Low severity risk as it is Netflix\u2019s responsibility to acquire content that has been fairly produced. \nLikelihood of this risk: Medium likelihood risk as Netflix is under constant pressure to licence more content, and therefore thorough background checks may not be completed of working conditions in the studio. \n\nRegulatory bodies \nLegal risk: Regulatory bodies could bring Netflix to court for violating user privacy or through providing content which breaks laws of a certain country where the platform is streamed, as content is regulated differently in every country. \nSeverity of this risk: High severity risk as Netflix could face fines. \nLikelihood of this risk: Medium likelihood risk, as it depends on who the regulatory authority is in the country where Netflix is being streamed, as it differs from country to country. \n\n\nRisk to labour practices \nNetflix company \nLabour displacement risk: As more AI is implemented into the application, it could result in less workers needed, and therefore employees being laid off. \nSeverity of this risk: High severity risk as it would lead to worker dissatisfaction and leave workers without a job which reflects poorly on the company and affects the economy. \nLikelihood of this risk: High likelihood risk as many \u2018tech\u2019 companies such as Twitter, have laid off thousands of workers, as the application requires less staff to achieve the same productivity (Toh & Liu, 2023).\n\nDeskilling of work risk: Translators being replaced by machine translators, which translate content on Netflix into various languages. \nSeverity of this risk: High severity risk as it would lead to worker dissatisfaction and leave workers without a job which reflects poorly on the company and affects the economy. \nLikelihood of this risk: High likelihood risk as Netflix has already implemented machine translation (MT) within the application. \n\nSubscribers \nLabour displacement risk: If workers are laid off due to increased AI implementation, subscribers could be impacted negatively as they could feel less satisfied if customer support is handled by AI rather than a human. \nSeverity of this risk: Medium severity risk as subscribers may end their subscription with Netflix and use another streaming service platform if they feel unsatisfied with the streaming platform. \nLikelihood of this risk: Medium likelihood risk as AI customer support systems are becoming the norm for many websites and applications. \n\nDeskilling of work risk: As MT is replacing human translators, this could result in subtitles being poorly translated.  \nSeverity of this risk: High risk, as Netflix could lose subscribers due to poorly translated content, and subscribers may use a different streaming service if left feeling unsatisfied. \nLikelihood of this risk: Low risk as MT is becoming more precise at translating. \n\nRegulatory bodies \nLabour displacement and deskilling of work risk: If workers lose their jobs due to AI implementation or due to implications of deskilling, regulatory bodies may have to assess how Netflix has laid off these workers and if it has complied with current labour laws.\n\n\nRisk to the environment \nNetflix company\nCarbon emission risks: Netflix generated 1.5 million metric tons of carbon in 2021 (Netflix inc., 2021). Its carbon footprint is a result of data storage centres, production and streaming of content all consuming large amounts of energy, thus generating high carbon footprints. The increased use of AI within the application further increases Netflix\u2019s carbon footprint as it requires a lot of energy (Coleman, 2023).\nSeverity of this risk: High severity risk as this contributes to the increasing global average temperatures (Lindsey, 2023). \nLikelihood of this risk: Medium likelihood risk as Netflix inc. has currently promised to invest in carbon offsetting through nature based solutions and half their emissions by 2030, however the idea of carbon offsetting means companies like Netflix can continue emitting carbon without having to actually reduce the emissions (Netflix Sustainability, 2021). \n\nSubscribers \nCarbon emission risks: streaming content on Netflix causes carbon emissions. Streaming 1 hour of content results in approximately 100g of carbon emissions (Stewart, 2021). \nSeverity of this risk: Even though streaming content on Netflix produces carbon emissions which in turn causes global warming, in comparison to other anthropogenic activities, these emissions are relatively small. Therefore the severity of this risk is low.\nLikelihood of this risk: High risk as this is the main activity of subscribers on this platform. They use this service to stream content. \n\nRegulatory bodies \nCarbon emission risks: Regulatory bodies use data centres to store data collected from their research. \nSeverity of this risk: Low risk as it does cause carbon emissions however other anthropogenic activities result in more carbon emissions, therefore the severity is relatively low in comparison. \nLikelihood of this risk: high as this activity does produce carbon emissions.\n\n\nRisk to fair operating procedures  \nNetflix company \nRisk of misusing licensed content: This risk could occur if Netflix inc. does not adhere to the licensing terms of agreement regarding content from third parties. An example of this would be Netflix keeping a movie on its platform longer than the licensing period that was agreed. \nSeverity of this risk: is high as it would cause Netflix to face fines or legal action. \nLikelihood of this risk: is low as misusing licensed content would cost Netflix inc. a lot of money, hence Netflix has a legal team to ensure that the company complies with agreements with third parties (Netflix Jobs, no date). \n\nSubscribers \nRisk of misusing licensed content: Netflix subscribers could misuse licence content by breaking Netflix\u2019s terms of use by recording and/ or sharing licensed content on the platform to third parties who do not have a Netflix subscription. \nSeverity of this risk: is high as it can cause Netflix a lot of legal damage and reputational damage if the application\u2019s subscribers can easily misuse its licensed content. \nLikelihood of this risk: is high as both Netflix original shows and licensed content can be found on illegal streaming sites, meaning this content was misused and illegally distributed.\n\nRegulatory bodies \nRisk of misusing licensed content: Netflix could misuse licensed content by inappropriately rating the content, for example allowing age restricted content be made available for children to watch. This situation would involve regulatory bodies that are responsible for content standards stepping in and addressing this misuse of licensed content. \nSeverity of this risk: High severity risk as Netflix could face large fines or legal action by misusing licensed content.\nLikelihood of this risk: Low likelihood of this risk as Netflix adheres to regulation guidelines depending on which country the platform is being streamed in. Netflix also has a parental controls feature that blocks inappropriate content from being viewed on Netflix kids accounts. \n\n\nRisk to consumer issues \nNetflix company \nRisk of misleading information to consumers: Netflix has been in controversy recently due to their apparently biassed algorithms that perpetuate stereotypes and mislead users. For example, in the article, \u2018Uncovering Bias in Programming\u2019, Tindal illustrates an example of algorithm bias as results for films with all African American casts come up as other movie suggestions when the name Whoopi Goldberg is typed into the search bar (Tindal, 2023). This is misleading as the user was searching for Whoopi Goldberg films, and not for films that are predominantly African American casts. Another example of misleading information is the production of thumbnails with African Americans actors depicted, despite the very small role they play in the movie. Here we see Netflix using people of colour in thumbnails to increase CTR for people who watch movies with predominantly people of colour in it. This is not only misleading, but it also treats one set of users differently from another set of users, as one set receives a thumbnail that accurately reflects the movie whereas the set receives a misleading thumbnail that poorly reflects the movie just to increase CTR\u2019s (Yu, 2019).\n\nSeverity of this risk: High severity is associated with this risk as Netflix could face reputational damage  and backlash from its subscribers that were misled. \n\nLikelihood of this risk: The likelihood is high as thumbnails and content recommendations on this platform can change for users on a daily basis, meaning there are more opportunities for users to be misled.\n\nSubscribers \nRisk of misleading information to consumers: Users can feel frustrated if the thumbnail of the movie they selected inaccurately represented the movie. This leaves the user unsatisfied with the streaming service. \nSeverity of this risk: High severity risk as Netflix could lose its subscribers if they were left unsatisfied with inaccurate recommendation systems and thumbnails. This ultimately results in decreased user engagement and revenue loss if subscribers cancel their subscriptions\nLikelihood of this risk: Medium likelihood associated with this risk as Netflix\u2019s algorithms are occasionally inaccurate or biassed, with algorithms constantly being improved, the likelihood of this risk will decrease over time. \n\nRegulatory bodies \nRisk of misleading information to consumers: If complaints are made by consumers to regulatory bodies, regulatory bodies may intervene and force Netflix to employ fair and accurate content recommendations and thumbnails. \nSeverity of this risk: Low severity is associated with this risk as Netflix can promptly address a complaint made by a subscriber by quickly changing the content\u2019s thumbnail or where the content is recommended. Thus this issue can be easily resolved.\nLikelihood of this risk: Medium likelihood associated with this risk as algorithms are prone to unintentional bias (Heilweil, 2020).\n\n\nRisk to community involvement and development\nNetflix company \nRisk of AI on local employment: The increased use of AI within Netflix could lead to increased layoffs which contribute to local unemployment. \nSeverity of this risk: High severity as layoffs negatively impact communities. For example; increased unemployment (due to layoffs) results in a strain on consumer spending in local communities thus affecting local economic stability.\nLikelihood of this risk: High likelihood of this risk as trends suggest more tech companies layoff staff due to AI implementation (Confino, 2024). \n\nSubscribers \nRisk of local health: With the various digital engagement methods successfully employed by Netflix to increase user engagement, this encourages sedentary behaviour as subscribers are encouraged to stream more and more content. \nSeverity of this risk:  There is a high severity of this risk on local community wellbeing as subscribers become less active, and subjected to the consequences of sedentary behaviour such as an increased risk of obesity (Rosiek et al., 2015). \nLikelihood of this risk: The likelihood of this risk is high as improved recommendation systems encourage and enable users to stream content seamlessly. \n\n\n\nRegulatory bodies \nRisks of AI on local wellbeing: If layoffs occur due to AI implementation, community well being could be negatively impacted if many locals were employed by Netflix, hence regulatory bodies may enforce stricter regulations regarding the societal impacts of AI implementation. \nSeverity of this risk: Severity of this risk is high as layoffs can result in local economic instability and poor mental health amongst those laid off. It can also result in financial insecurity and stress for those laid off.\nLikelihood of this risk: The likelihood of this risk is high due to trends in AI implementation resulting in layoffs in the tech industry (Confino, 2024). \n\n\nDiscussion of Mitigations Measures for Risk \nData security is one of the more harmful risks that the stakeholders of Netflix face. User data is at risk of being breached through various means, for example;  there is a risk of user data being leaked if Netflix uses third party services to analyse user data, weak authentication could result in a users account being easily accessed by unauthorised personnel, thus leading to a users data being subject to exposure. Data could also be breached unintentionally by Netflix employees as a result of human error. For example during the production of a Netflix Original show, data on set could be leaked by a worker who posts a behind the scenes photo that contains sensitive information. \n\nNetflix inc. is a stakeholder that has the power to incorporate stronger measures regarding data security. More specifically Netflix\u2019s IT department and Chief Information Security Officer (CISO) currently being Vitaly Gudanets would be directly involved in minimising the risk of user data being breached or misused. An increased number of audits is both a technical and organisational control that is needed to help identify weaker spots in Netflix\u2019s data storage and security system, and would prevent underlying issues from prevailing for a long period of time. \n\nAn increased number of audits would also be a preventative measure against cyberattacks as hackers take advantage of \u2018weak points\u2019 within a security system. Netflix currently does not use Two Factor Authentication (2FA), meaning its subscribers' data and accounts are subject to hackers (James & Quinlan , 2024). In house technical controls are needed to target this risk, specifically;  Netflix\u2019s IT department and CISO should employ 2FA as a preventative measure to mitigate the risk of hackers accessing subscribers accounts and data. \n\nNetflix aims to crack down on password sharing between users that do not live together by obtaining and analysing IP addresses of the devices used to log into the shared account (Hector, 2024). This raises concern for users, as an IP address is sensitive information and once it is stored it becomes susceptible to unauthorised access. Data encryption is one measure that should be taken if Netflix opts to store a user's IP address as this mitigates the risk of a user's data being leaked. \n\nAnother stakeholder that Netflix inc. interacts with, includes various regulatory bodies. Such interactions involve third party organisational controls to ensure that Netflix complies with current data regulations such as GDPR. \n\nFinally, regarding data protection during the production of content, Netflix could employ more preventative measures such as mandatory educational programmes about data protection. Educational programmes further educate employees on data protection and enable them to successfully adhere to current data regulations. Such measures would mitigate the risk of employees leaking sensitive information unintentionally. This would be an example of an organisational control achieving this mitigation measure. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences \nColeman, J. (2023) Ai\u2019s climate impact goes beyond its emissions, Scientific American. Available at: https://www.scientificamerican.com/article/ais-climate-impact-goes-beyond-its-emissions/#:~:text=But%20as%20AI\u2019s%20popularity%20keeps,way%20AI%20affects%20the%20climate.  (Accessed: 23 March 2024).\n\n\nConfino, P. (2024) Ai is like \u2018corporate Ozempic\u2019 for tech companies: It trims the fat and you keep the fact you\u2019re using it a secret, says Scott Galloway, Fortune. Available at: https://fortune.com/2024/02/29/tech-layoffs-ai-corporate-ozempic-meta-apple-salesforce-google-scott-galloway/  (Accessed: 23 March 2024).\n\n\nCuofano, G. (2024) Netflix employees, FourWeekMBA. Available at: https://fourweekmba.com/netflix-employees/  (Accessed: 18 March 2024).\n\n\nEuropean Parliament and Council of the European Union (2018) The General Data Protection Regulation - consilium.europa.eu, European Council Council of the European Union. Available at: https://www.consilium.europa.eu/en/policies/data-protection/data-protection-regulation/ (Accessed: 18 March 2024).\n\nHector, H. (2024) Netflix password-sharing: How the crackdown works and what it costs, TechRadar. Available at: https://www.techradar.com/news/netflix-password-sharing#:~:text=Earlier%20this%20year%2C%20Netflix%20explained,part%20of%20an%20account\u2019s%20household.  (Accessed: 24 March 2024). \n\nHeilweil, R. (2020) Why algorithms can be racist and sexist, Vox. Available at: https://www.vox.com/recode/2020/2/18/21121286/algorithms-bias-discrimination-facial-recognition-transparency  (Accessed: 23 March 2024).\n\n\nJames, M. and Quinlan , K. (2024) Was your netflix account hacked? what to do, All About Cookies. Available at: https://allaboutcookies.org/netflix-account-hacked#:~:text=Netflix%20is%20notoriously%20easy%20to,and%20some%20good%20security%20software.  (Accessed: 23 March 2024).\n\n\nLindsey, R. (2023) Climate change: Atmospheric carbon dioxide, NOAA Climate.gov. Available at: https://www.climate.gov/news-features/understanding-climate/climate-change-atmospheric-carbon-dioxide#:~:text=Without%20carbon%20dioxide%2C%20Earth\u2019s%20natural,causing%20global%20temperature%20to%20rise.  (Accessed: 23 March 2024).\n\n\nNetflix (no date) Privacy policy, Netflix in Your Neighbourhood. Available at: https://www.netflixinyourneighbourhood.ca/privacy/#:~:text=We%20do%20not%20sell%20personal,to%20exceptions%20under%20the%20CCPA.  (Accessed: 19 March 2024).\n\n\nNetflix Jobs (no date) Legal and Public Policy, Netflix jobs. Available at: https://jobs.netflix.com/team?slug=legal-and-public-policy  (Accessed: 23 March 2024).\n\n\nNetflix Research (no date) Netflix research, Netflix Research. Available at: https://research.netflix.com/research-area/machine-learning  (Accessed: 28 March 2024). \n\n\nNetflix sustainability. (2021) About Netflix - Sustainability, Sustainability . Available at: https://about.netflix.com/en/sustainability  (Accessed: 23 March 2024).\n\n\nPlummer, L. (2017) This is how Netflix\u2019s top-secret recommendation system works, Wired. Available at: https://www.wired.com/story/how-do-netflixs-algorithms-work-machine-learning-helps-to-predict-what-viewers-will-like/  (Accessed: 24 March 2024). \n\n\nRobinson, J. (2018) The four types of Netflix originals, What\u2019s on Netflix. Available at: https://www.whats-on-netflix.com/news/the-four-types-of-netflix-originals/  (Accessed: 18 March 2024).\n\n\nRosiek, A. et al. (2015) \u2018Effect of Television on Obesity and Excess of Weight and Consequences of Health\u2019, NCBI. Available at: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4555288/  (Accessed: 23 March 2024).\n\n\nRoth, D. (2020) The secret behind Netflix\u2019s personalized thumbnails, Looper. Available at: https://www.looper.com/274997/the-secret-behind-netflixs-personalized-thumbnails/  (Accessed: 16 March 2024).\n\n\nShewale, R. (2024) Netflix statistics 2024 (Users & revenue), DemandSage. Available at: https://www.demandsage.com/netflix-subscribers/#:~:text=Netflix%20is%20a%20leading%20SVOD,the%20first%20half%20of%202022.  (Accessed: 24 March 2024). \n\n\nSpangler, T. (2022) Netflix launches \u2018Two thumbs up\u2019 user content ratings for TV shows, Movies you love, Variety. Available at: https://variety.com/2022/digital/news/netflix-two-thumbs-up-ratings-1235228641/  (Accessed: 16 March 2024).\n\nTindal, M. (2023) Uncovering bias in programming: Netflix, LinkedIn. Available at: https://www.linkedin.com/pulse/uncovering-bias-programming-netflix-mark-tindal/  (Accessed: 16 March 2024).\n\n\nToh, M. and Liu, J. (2023) Elon Musk says he\u2019s cut about 80% of Twitter\u2019s staff | CNN business, CNN. Available at: https://edition.cnn.com/2023/04/12/tech/elon-musk-bbc-interview-twitter-intl-hnk/index.html#:~:text=Elon%20Musk%20has%20laid%20off,the%20time%20of%20his%20acquisition.  (Accessed: 23 March 2024).\n\n\nWelk, B. (2023) Why netflix doesn\u2019t license its own content to others, IndieWire. Available at: https://www.indiewire.com/news/business/why-netflix-doesnt-license-own-content-out-1234934844/ (Accessed: 18 March 2024).\n\n\nYu, A. (2019) How Netflix uses AI and machine learning, Medium. Available at: https://becominghuman.ai/how-netflix-uses-ai-and-machine-learning-a087614630fe  (Accessed: 16 March 2024)."
    },
    {
      "role": "assistant",
      "content": "{\n    \"feedback\": \"A well structure and well researched review of ethical risks of Netflix, though some risks may not be specific to its use of technology. In mitigation note that unauthorised access if more an issue to protect Netflix, akin to password sharing, than to protect the users personal information, that is more about back end security.\",\n    \"description / 20\": 64,\n    \"roles / 10\": 64,\n    \"risks / 40\": 62,\n    \"mitigation / 20\": 67,\n    \"refs / 10\": 67,\n    \"total score\": 64.1\n}"
    }
  ],
  [
    {
      "role": "system",
      "content": "You are a professional educator. \nYour target is to assess students' assignments based on the provided rubric (between <rubric> and <rubric> labels).\n<rubric>\nAssignment: Investigating and Analysing Ethical Risks of a Digital Engagement application \n\nThis is an INDIVIDUAL assignment on Ethical Risk Assessment for the TEU00062 module. This assessment is worth 30% of the total TEU 00062 module mark. The deadline for submission is 12.00. noon Friday 31st March 2024. \n \nLearning outcomes: Upon completion of this assignment students should:\nBe able to identify the different stakeholders and organisational roles involved in developing and using AI-driven digital engagement application, and motivations. \nBe able to identify and analyse ethical risks in a digital engagement application.\nBe equipped to identify technical and governance mitigations of these risks\n \nSteps:\n \nSelect ONE real world application that embodies one or more AI-driven digital engagement techniques you have learnt about on this course, e.g. e.g. digital engagement techniques used in one applications such as social media, newsfeeds, search, eCommerce, elearning or digital humanities., or in an application you are familiar with e.g. interactive Game, Virtual Presence (i.e. VR /AR) etc. \nIdentify the different value chain stakeholder roles involved in this application scenario and where possible the actors (i.e. companies, organisations) carrying out those roles. including those involved in: data collection; AI model development; development and use of the application. \nUse social responsibility principles (see ethic lecture week 2) to identify and analyse real or potential ethical risks of the application, including classification of the affected stakeholders (hint: see also risks classified by Latzer et al 2016 \u2013 ethics lecture week 1), how severe the risk would be and how likely it is to occur.\nIdentify one or more mitigation measures that specific value-chain stakeholders may undertake to minimise the impact of the risk you think is most harmful. These could be technical measures, organisational governance measures or measure involving additional engagement with affected individuals or groups.\n \nStructure of Assignment Report. \nStructure the Report (4000 word max excluding figures and references), under the following heading [indicative percentage of assignment mark per section]. Please provide references\n \nDescription of Application [20%]\n<describe your selected application, explaining what it does, the role AI and data collection plays in its digital engagement features, and the benefits it provides>\n\nIdentification of Stakeholder Roles involved in the application and its governance [10%]\n<describe the stakeholder roles undertaken in the development, use and governance of the application, and which organization take those roles and the benefits they derive from their involvement. Include roles that may be indirectly affected by the application. >\n\nIdentification of Ethical Risks [40%]\n<Identify the ethical risks that may be raised by your application under the social responsibility categories below (more details in ethics lecture week 2). For each risk identify which stakeholder may be affected, and assess how severe the impact would be for each stakeholder (Low, Medium, High) and the likelihood of that risk being incurred (Low, Medium, High), justifying your assessment, even if you think there is no risk under that heading.>\nHuman Rights\nLabour Practices\nThe Environment\nFair Operating Procedures\nConsumer Issues\nCommunity Involvement and Development\n \nDiscussion of Mitigations Measures for Risk [20%]\n< For one of the risks you consider more severe, describe mitigation measures that could be taken. Explain how the measure would reduce or eliminate the likelihood and impact of the risk. Explain which stakeholders would be involved, or would have to interact in implementing the measure, identifying if this is a governance measure an organization can undertake itself or in collaboration with other parties, e.g. regulators or external stakeholder..>\n\nReferences [10%]\nInclude references to source you use for you analysis, which could be academic or policy papers, blogs, information from company web site or news stories.\n\n</rubric>\nThe assignment content will be provided by user (not in system prompt)\nDo not say anything else except the marking, following the format of:\n{\n    \"feedback\": \"feedback content\",\n    \"(rubric part 1) / (rubric part 1 weight)\": score,\n    \"(rubric part 2) / (rubric part 2 weight)\": score,\n    ......other rubric parts\n    \"total score\": weighted average score of all parts\n}"
    },
    {
      "role": "user",
      "content": "Duolingo is a language-learning mobile application that has transformed the way we learn languages. Founded in 2011 by Luis von Ahn and Severin H, it is now the world's biggest language-learning app, with over 500 million users it provides classes in over thirty different languages, including English, Spanish, French, Hindi and Chinese. At the heart of Duolingo's digital success is the use of artificial intelligence and data collection to improve user engagement and provide students with tailored instruction. To elaborate, Duolingo employs advanced AI algorithms to track each student's specific development and create a study plan that adjusts in real-time to their performance. This is possible because the platform collects massive quantities of data from users, such as their responses, time spent studying, level of satisfaction with the course, correlation between answers, and user frequency. Adaptive learning algorithms are a key innovation in digital language learning, adapting education to each individual's pace and proficiency. Duolingo, a popular language learning software, shows the incorporation of such algorithms through its artificial intelligence (AI) systems, significantly enhancing the efficiency and personalization of language acquisition.\n\nAt the heart of Duolingo's AI infrastructure is the use of machine learning models, which rigorously analyse user data to predict the most helpful exercises for each learner at any given time. This adaptable technique guarantees that the exercises are appropriate for the user's existing understanding and capabilities, delivering challenges that are neither boring nor frustrating. As a result, the dynamic modification of difficulty and exercise kinds maximises language retention and progression. By maintaining a balance in which users are constantly challenged, AI creates an atmosphere conducive to long-term engagement and learning. Furthermore, natural language processing (NLP) is another critical component of Duolingo's AI system. NLP enables the programme to understand and process input from users in a variety of languages, which supports the app's capacity to provide speech recognition as well as writing tasks. This technology improves the interactive part of the learning experience by allowing the app to provide quick feedback, which is crucial in the successful learning of new languages. Duolingo uses NLP to imitate real-life conversations and written settings, giving learners a platform to practise and develop their language abilities in a supportive, proactive environment.\n\nFundamentally, users are also data sources; each quiz response, course participation, and practised activity adds to the overall image of the learning environment. The foundation of Duolingo's artificial intelligence (AI) algorithms is this steady stream of data, which allows for a customised and dynamic educational experience that changes with every contact.\nUsing this data, specialised teams of AI model engineers labour nonstop to improve the flexible learning algorithms that enable Duolingo to adapt to each user's unique learning demands. In parallel, content developers use the insights derived from these models to create exercises, quizzes, and lessons that are effective in meeting the needs of a variety of learners while remaining entertaining. The simplicity of the application of use and intuitive design, courtesy of skilled UI/UX designers, further ensures that Duolingo remains accessible and enjoyable for learners from all walks of life. To maintain compliance and ethical standards, governing teams and management are in charge of Duolingo's strategic direction. They do this by navigating the complicated world of data protection laws and educational standards. External organisations such as educational institutions that use\nDuolingo as a teaching tool, regulatory agencies that uphold the law, and technological suppliers that supply the app's necessary infrastructure round out this internal ecosystem.\nAlong with the larger language learning community, shareholders and investors, with their emphasis on expansion and profitability, also have a significant influence on Duolingo's future. The unwavering commitment of Duolingo to data-driven customisation is evidence of the potential of AI in teaching. Duolingo's AI models adjust in real-time based on a thorough analysis of each user interaction, providing a tough yet attainable learning environment which makes language instruction a more accessible and successful endeavour by addressing each person's unique learning curve and optimising participation.\n\nDuolingo's novel method has numerous advantages. Its gamified learning approach, supported by advanced AI, makes learning a language a fun experience that encourages consistent practice and long-term dedication. The app's free usage strategy, which democratises language learning and guarantees that high-quality instruction is available to everyone, irrespective of financial status, further encourages user involvement. Additionally, Duolingo offers a window into the future of learning as a cutting-edge use of artificial intelligence and data insights in the tech space. Duolingo has created a platform for learning that is not just efficient but also fair and interesting by fusing technology and human-centred design. Its success story serves as a model for how technology can change education and make it more individualised, fun, and accessible. Even as it develops further, Duolingo serves as a prototype for integrating AI into education, demonstrating the countless opportunities that arise when technology is applied to improve human learning.\nA diverse range of stakeholders contribute to and benefit from Duolingo's unique approach to learning a language. These stakeholders, which range from colleges and universities to end users, comprise a network of collaboration that supports Duolingo's success. Educational institutions frequently incorporate Duolingo into their educational programmes, using its entertaining interface and extensive language classes to enhance traditional teaching techniques. Regulatory organisations, on the contrary, verify that Duolingo complies with data privacy laws and educational requirements, therefore protecting users' interests and sustaining public trust. Technology partners supply the critical infrastructure that allows Duolingo to analyse large datasets and provide personalised learning experiences.\nWhich is done, to ensure that students obtain a well-rounded education, language and cultural professionals work together to provide an accurate and culturally appropriate curriculum. Last but not least, users, or students, are at the heart of Duolingo's ecosystem, utilising the resources of the app to meet their language learning objectives.\n\nThese stakeholders gain different advantages from using Duolingo. With the abundance of language learning resources available to users, education is made both pleasurable and accessible. With Duolingo, educational institutions have a useful friend that improves their language instruction programmes and gives students more resources. UI/UX designers, content producers, and AI model developers can all contribute to a cutting-edge educational technology platform thanks to the platform's ongoing innovation. Duolingo's adherence to legal requirements serves as a reminder to regulatory agencies of the value of data security and privacy in the digital era. Through their partnership with Duolingo, language specialists and technology partners enhance their respective professions and support the larger goal of democratising education.\n\n\n\nTalking in ethical regards, the way Duolingo operates raises several ethical issues, particularly given its dependence on AI to improve and personalise the learning process. These factors cover a wide range of topics, each with ramifications for the platform's stakeholders, such as labour practices, environmental effects, human rights, and fair operating methods. A key component of Duolingo's AI-driven personalisation is the gathering and analysis of user data, which raises privacy and data protection issues. Although this approach improves the educational process, it may violate users' privacy if private data is misused or used against permission. This risk is substantial given the sensitive nature of the data and the possible repercussions of breaches, underscoring the necessity of strict data protection procedures. Labour practices in the tech sector are equally important to pay attention to, such as those at Duolingo. The constant need to improve AI algorithms may put engineers in stressful work settings. The IT sector is becoming more conscious of labour issues, but there is still a chance for intense work cultures, which emphasises how crucial it is to keep safe and equitable workplaces. A further ethical consideration related to AI system operation is the environmental impact, which includes data centre energy use. Even though Duolingo may not have as much of an immediate environmental impact as bigger digital companies, it is part of a sector whose carbon footprint is coming under increasing scrutiny. Resolving these issues is crucial to sustainability and conscientious functioning.\n\nUltimately, fair operating procedures are challenged by the possibility of biases becoming ingrained in AI models. These prejudices could harm users from different backgrounds, compromising the fairness and efficacy of the site. It is still a difficult task to create AI systems that are equally fair to all users, therefore inclusiveness and bias mitigation are constant challenges. The intricate interactions between the various parties that both contribute to and benefit from Duolingo's platform define its ecosystem. The cooperative approach encourages creativity and makes sure Duolingo stays a top resource for language learners everywhere. But the moral issues raised by AI's application emphasise the importance of regulating labour laws, privacy, the environment, and equity with vigilance. By taking proactive measures to mitigate these ethical hazards, Duolingo can maintain its status as an efficient, reachable, and Unquestionably, Duolingo has changed the face of language learning by utilising artificial intelligence to develop a fun and instructive platform that helps users navigate the challenges of the digital age. But like any technological advancement, this change comes with ethical issues ranging from worries about data privacy to the complexities of AI biases and the wider societal effects of digital learning environments. \n\u201cSo why is data privacy crucial in eLearning? First and foremost, it is an ethical and legal requirement. Users have a right to privacy and to protect their personal information. A breach of this right can lead to significant legal ramifications for eLearning providers.\u201d\n\u201cSecondly, data privacy is crucial for maintaining trust between users and providers. With growing awareness about data security, users are more cautious about the platforms they engage with and are more inclined towards those that prioritize data privacy.\u201d \u201cFinally, data privacy contributes to increasingly personalized eLearning experiences. The more comfortable users are in sharing their data, the more data eLearning providers can gather to draw patterns and tailor learning experiences to individual needs.\u201d\n\n\n\n\nTo address these issues, a comprehensive and watchful strategy is required, highlighting the significance of moral stewardship in the development of digital learning. The difficulties relating to customer concerns\u2014specifically, the ambiguity and lack of transparency surrounding data usage policies\u2014highlight how imperative it is that Duolingo give user trust a priority. This trust, which is essential to the platform's performance, depends on open communication and strict data protection procedures that uphold user privacy and guarantee the appropriate use of personal data. Duolingo may further demonstrate its dedication to its users and promote a secure environment by improving the readability and usability of its data rules.\n\nThe significance of inclusiveness in digital education is highlighted by the possible effects of Duolingo on building communities and participation. Due to the increasing likelihood of the digital divide widening as more people turn to online learning tools, sites such as Duolingo must actively work to close this gap. It is critical to guarantee that everyone can benefit from digital learning, especially those living in areas with restricted internet access, to advance fair educational opportunities across the globe. The strategic approach to tackling prejudice within AI algorithms is crucial to limiting the ethical issues connected with Duolingo's operations. Ensuring an equitable and inclusive learning environment requires the creation and application of advanced algorithms that identify and rectify biases. Robust organisational governance, such as the creation of an ethical Artificial Intelligence governance system and frequent audits of AI systems, must be implemented in conjunction with this technical undertaking. Interacting with a wide range of stakeholders\u2014from users to language and cultural specialists\u2014improves the platform's ability to provide material that is both efficient and considerate of the great diversity of humankind.\n\nDuolingo is constantly developing and enhancing its platform by integrating technological innovation, moral leadership, and community involvement. This path, which has been characterised by cooperation between both internal and external stakeholders\u2014from regulatory agencies and educational institutions to AI developers\u2014is evidence of the teamwork needed to traverse the morally challenging terrain of digital education. The significance of ethical guardianship in the field of educational technology cannot be emphasised as we move to the future. In addition to being a testament to its customers' devotion, Duolingo's resolve to tackle the complex ethical issues raised by its platform serves as a model for the larger educational technology industry. Duolingo establishes a standard for the responsible creation and implementation of online educational resources by promoting openness, inclusion, and justice. This guarantees that education is a right that may be pursued by anybody, regardless of location or socioeconomic status. The development of Duolingo from an innovative language-learning app to a widely used educational platform serves as an example of how technology may revolutionise education. However, this voyage also serves as an awareness of the obligations that come with such innovation. As Duolingo develops further, its performance will be evaluated not just on how well its learning models work but also on how well it can handle the moral dilemmas presented by the digital era, guaranteeing that education will be both equal and brilliant for students anywhere in the globe.\n\n\n\n\n\nReferences:\n\nGames with a purpose. Available at: https://www.cs.cmu.edu/~biglou/ieee-gwap.pdf (Accessed: 30 March 2024). \n\n(PDF) the impact of Duolingo in developing students\u2019 ... Available at: https://www.researchgate.net/publication/370389939_The_impact_of_Duolingo_in_developing_students\u2019_linguistic_competence_an_aspect_of_communicative_language_competences (Accessed: 30 March 2024). \n\nDoes gamification work? -- A literature review of empirical studies ... Available at: https://ieeexplore.ieee.org/document/6758978/ (Accessed: 30 March 2024). \n\nDuolingo Effectiveness Study - The Owl App. Available at: https://theowlapp.health/wp-content/uploads/2022/04/DuolingoReport_Final-1.pdf (Accessed: 30 March 2024). \n\nThe role of GDPR in Elearning Data Privacy (2023) The eLearning Blog. Available at: https://elearning.company/blog/the-role-of-gdpr-in-elearning-data-privacy/ (Accessed: 30 March 2024)."
    },
    {
      "role": "assistant",
      "content": "{\n    \"feedback\": \"A fairly generic treatment of the ethical issues around Duolingo. It might have been interesting to consider the impact of the language teaching profession, e.g. is there evidence of less people seeking language education?.\",\n    \"description / 20\": 65,\n    \"roles / 10\": 65,\n    \"risks / 40\": 55,\n    \"mitigation / 20\": 60,\n    \"refs / 10\": 50,\n    \"total score\": 58.5\n}"
    }
  ],
  [
    {
      "role": "system",
      "content": "You are a professional educator. \nYour target is to assess students' assignments based on the provided rubric (between <rubric> and <rubric> labels).\n<rubric>\nAssignment: Investigating and Analysing Ethical Risks of a Digital Engagement application \n\nThis is an INDIVIDUAL assignment on Ethical Risk Assessment for the TEU00062 module. This assessment is worth 30% of the total TEU 00062 module mark. The deadline for submission is 12.00. noon Friday 31st March 2024. \n \nLearning outcomes: Upon completion of this assignment students should:\nBe able to identify the different stakeholders and organisational roles involved in developing and using AI-driven digital engagement application, and motivations. \nBe able to identify and analyse ethical risks in a digital engagement application.\nBe equipped to identify technical and governance mitigations of these risks\n \nSteps:\n \nSelect ONE real world application that embodies one or more AI-driven digital engagement techniques you have learnt about on this course, e.g. e.g. digital engagement techniques used in one applications such as social media, newsfeeds, search, eCommerce, elearning or digital humanities., or in an application you are familiar with e.g. interactive Game, Virtual Presence (i.e. VR /AR) etc. \nIdentify the different value chain stakeholder roles involved in this application scenario and where possible the actors (i.e. companies, organisations) carrying out those roles. including those involved in: data collection; AI model development; development and use of the application. \nUse social responsibility principles (see ethic lecture week 2) to identify and analyse real or potential ethical risks of the application, including classification of the affected stakeholders (hint: see also risks classified by Latzer et al 2016 \u2013 ethics lecture week 1), how severe the risk would be and how likely it is to occur.\nIdentify one or more mitigation measures that specific value-chain stakeholders may undertake to minimise the impact of the risk you think is most harmful. These could be technical measures, organisational governance measures or measure involving additional engagement with affected individuals or groups.\n \nStructure of Assignment Report. \nStructure the Report (4000 word max excluding figures and references), under the following heading [indicative percentage of assignment mark per section]. Please provide references\n \nDescription of Application [20%]\n<describe your selected application, explaining what it does, the role AI and data collection plays in its digital engagement features, and the benefits it provides>\n\nIdentification of Stakeholder Roles involved in the application and its governance [10%]\n<describe the stakeholder roles undertaken in the development, use and governance of the application, and which organization take those roles and the benefits they derive from their involvement. Include roles that may be indirectly affected by the application. >\n\nIdentification of Ethical Risks [40%]\n<Identify the ethical risks that may be raised by your application under the social responsibility categories below (more details in ethics lecture week 2). For each risk identify which stakeholder may be affected, and assess how severe the impact would be for each stakeholder (Low, Medium, High) and the likelihood of that risk being incurred (Low, Medium, High), justifying your assessment, even if you think there is no risk under that heading.>\nHuman Rights\nLabour Practices\nThe Environment\nFair Operating Procedures\nConsumer Issues\nCommunity Involvement and Development\n \nDiscussion of Mitigations Measures for Risk [20%]\n< For one of the risks you consider more severe, describe mitigation measures that could be taken. Explain how the measure would reduce or eliminate the likelihood and impact of the risk. Explain which stakeholders would be involved, or would have to interact in implementing the measure, identifying if this is a governance measure an organization can undertake itself or in collaboration with other parties, e.g. regulators or external stakeholder..>\n\nReferences [10%]\nInclude references to source you use for you analysis, which could be academic or policy papers, blogs, information from company web site or news stories.\n\n</rubric>\nThe assignment content will be provided by user (not in system prompt)\nDo not say anything else except the marking, following the format of:\n{\n    \"feedback\": \"feedback content\",\n    \"(rubric part 1) / (rubric part 1 weight)\": score,\n    \"(rubric part 2) / (rubric part 2 weight)\": score,\n    ......other rubric parts\n    \"total score\": weighted average score of all parts\n}"
    },
    {
      "role": "user",
      "content": "Introduction and description of the application:\n\nThe AI model and techniques that arise from it which impact stakeholders and hold several ethical concerns upon which I will be presenting is the use of relationship AI chatbots.  I will be analysing the use of relationship chatbots both in the present and its potential in the future, The different stakeholders who can be potentially affected by the rise of such chatbots, the ethical risks that are presented with the rise of such use and ways to mitigate the risks that could arise with the use of such chatbots. The utilization of chatbots to have a deeper personal relationship with the product, in this case, the chatbot itself is a digital engagement technique upon which I will be analyzing both the uses of it while also looking at the stakeholders and ethical risks and remedies present. \n\nRelationship AI Chatbots are programs that have been developed to have romantic and platonic relationships with the users on the other side of the screen. Specifically, relationship chatbots are looking to establish some form of connection with the user and fill a gap in somebody\u2019s life that may not be filled in their day-to-day lives especially the social needs such as love and belonging as part of Manslow\u2019s hierarchy of needs that may traditionally be sufficed by entering a relationship with another human. The way these relationship chatbots can function is by using natural language processing and machine learning. Natural language processing (NLP) is a form of technology that allows for AI models to form comprehensible sentences by using data it already has and analyzing different elements such as cohesion, sentiment and entity gathering, which is the process of identifying key elements of a text given to respond, to give back sentences as part of a conversation with the original user. Machine learning is the technology upon which the AI model can learn more and more through an algorithm that collects data and analyzes it to adapt, in simple terms, it works the same way a human would if it were to make a mistake it will learn from it by collecting the data and fixing its mistake. The more data it receives, the better in theory it becomes. The primary chatbots currently available to have a relationship with are Replika and Kuki AI.\n\nThe primary benefits that relationship chatbots can provide apart from a sense of companionship that users of the product may be looking for. Other benefits that come to mind of such chatbots, is the adaptive nature due to machine learning which continuously helps to improve the chatbot, The 24/7 nature of the product allows for conversation to flow at any time meaning the availability of the chatbot allows for users who need instant gratification to receive it via the chatbot and the broad nature of chatbots, often providing neutral answers can allow for safer, more mild-mannered interactions which means that a large portion of the people who avail of such a service to be engaged because it is unlikely that they will be upset by an answer as opposed to a human who holds actual opinions and has actual feelings which naturally can cause friction depending on the cohesion between the two parties. The chatbot companies also benefit since both Replika and Kuki AI offer subscription models to talk either in a relationship context with the chatbot as seen in Replika or locking unlimited messages behind a paywall and only offering a limited number of messages per month (300) as seen in Kuki AI.\n\nData collection: \n\nThe role that data collection plays in the development of AI chatbots is crucial for the development of a chatbot since it can train itself based on the information it gathers to improve its user experience, Data collection can also help personalize the experience that a user receives. This is because the more information they gather specifically the responses which are provided the more it can be personalized to the individual to increase user retention.\n\nBenefits it may provide: \n\nOne of the main benefits that AI chatbots, not necessarily relationship-specific ones but chatbots, in general, have is the fact they are instantaneous as compared to a real human, AI chatbots are available 24/7 and provide instant gratification that someone may be looking for. I also believe that relationship chatbots can be used to better understand human relationships if the technology becomes both accurate and developed enough to understand. If the technology advances enough a deeper understanding of human connection could potentially be achieved for society as a whole strengthening relationships in a world where they are becoming progressively weaker and weaker. A deeper understanding of how relationships work with the potential utilization of AI \u201cpartners,\u201d could result in better longer relationships between couples but also with the potential advancement of chatbots a better understanding of the discussions that humans have amongst themselves on a platonic level. \n\nGovernance of AI: \n\nThe way the Relationship AI chatbots are handled in the future is something to consider primarily looking at the work of Urs Gasser and Virgilio, in terms of the layered model there are three different layers upon how we can govern AI  The 3 layers of the model try to ensure that reasonable principles of fairness are applied to ensure that these complex models are run adequately. It all begins with the technical layer a section that is particularly interesting in making sure that AI does not have any ethical worries is algorithm accountability, Ensuring that there is not any unfair treatment based on \u201crace, colour, national origin, religion, sex, gender, sexual orientation, disability or family status\u201d. Secondly, the ethical layer looks into the ethical principles that should be upheld upon the development of such a system, They reference the continuing work in human rights mentioning criteria and principles as their key points. The governance of AI about ethics needs to be taken into consideration upon establishing what is \u201cequal or fair treatment\u201d Finally, The social and legal layer within the legal body legislation can allow for AI to be used ethically by ensuring safeguards are put into place so that the society that is protected by the law of the region is well established and protected. Aiming for the correct behaviour in AI is crucial and regulation is another thing to be considered looking at both the aforementioned layers and drawing upon them can set the criteria needed to guide us on how AI should be governed.\n\nStakeholders in Relationship AI chatbots:\n\nThere are both internal and external stakeholders in the realm of relationship chatbots that need to be analyzed, The internal shareholders are attempting to provide value to those who wish to avail of their service, specifically the owners of the companies working on the chatbots in this case, Replika and Kuki AI, The developers whom have developed the chatbot and are currently working on improving it specifically in key metrics such as keeping users engaged and conversion rate of free to paid users. Marketing is also an important aspect for the success of AI chatbots in particular since to utilise its machine learning it needs to have an active consumer base, Without an active consumer base the only way it could improve is by using evolutionary simulation to simulate conversations constantly but without actual humans, the consumers who are one of the external stakeholders considered and arguably the second most important only to the developers who managed to establish the chatbot in the first place. Humans are the bedrock that machine learning is built upon and without it, we would only be left with evolutionary simulation for these chatbots to improve which can only go far when it\u2019s only an AI trying to improve itself without feedback from the consumer base it is trying to please. The consumers are the ones who also provide the incentive for the companies that are currently facilitating the service to provide it. Without a financial incentive, it is unlikely that somebody would develop a well-made relationship chatbot as a personal hobby and the consumer base that is currently paying for these chatbots is the driving factor as to why they exist in the first place.  Another external stakeholder to consider is the government itself. The government has a duty to its citizens to vindicate its citizens \u201cfrom unjust attack and, in the case of injustice done, vindicate the life, person, good name and property rights of every citizen\u201d under article 40.3.2 of the Irish constitution. The reasoning behind why the government can be a stakeholder in this topic is because of the potential of floodgates being opened that could severely impact its society. The aforementioned floodgates that could be opened are the violation of GDPR Safeguarding the interests of the general public since there can be arguments made that while the liberty of the person to do what they may want is guaranteed under 40.4 of the Irish constitution as said, in the constitution \u201cby law\u201d, this is by the law and legislation could be passed if there is clear evidence of 40.3.2 being violated in any sort of way. The government needs to put the interests of its citizens first and therefore has a major stake in the rapid rise of chatbots in general, let alone those of such an emotional nature as relationships. Finally, society as a whole both users and non-users are stakeholders in the use of relationship AI chatbots. Non-users of this service will still be affected due to some users potentially deciding to move permanently towards AI-based relationships, in countries that are more likely to adopt this, more technologically advanced countries there is a clear consequence that could have grave implications. The birth rate in technologically advanced countries is currently declining for the most part having hit the fourth stage of the population cycle There is a question to be had if AI relationships become more prevalent if it could further the already existing consequences of a developed country in which its birth rate which is already decreasing gets further damaged by people electing to engage in virtual non-human relationships, while on one hand there is a chance for this to occur on the other this could allow for humans to become better communicators but only if utilized correctly as is with most chatbots in general.\n\nEthical risks present: \n\nUse of data within GDPR being violated: \n\nThe way our data is used is the first and most obvious ethical risk present whenever chatbots are mentioned that rely upon machine learning to function. It is important to necessitate the fact that GDPR has been put in place since 2016 to rectify the data protection rights of members within the European Union . Key factors when it comes to the use of data and how it could be breached is first of all not telling the user how their data is going to be used.  The ability to review all of your chat logs and everything that you have provided the chatbot with and a clear cutout privacy policy that allows for the user base to consent to whatever data may be extracted out of them The potential risks that come from different stakeholders would be internally the potential for severe fines being imposed a violation of GDPR also comes with the bad press but the EU has not been shy when it comes to fines due to violations of GDPR. Most importantly a fine of 1.2 billion euros was imposed on Meta in 2023 for the transferring of data from the EU to the United States without adequate protections put in place. Externally, GDPR violations can result in a lack of trust between stakeholders who wish to avail of the chatbot but also without adequate GDPR protections people\u2019s privacy could be violated. The reasoning for why this could be an issue is because this leaves the consumers susceptible to predatory marketing from companies who wish to buy the data they\u2019ve provided and finally, with GDPR being upheld it can prevent fraud and cybercrimes as per the European Data Protection Board. Due to the nature of relationship chatbots getting personal with their clients, a potential violation of GDPR could be of a very serious manner due to the interpersonal data that is harvested due to the conversations the consumer may have with its chatbot and therefore such data being used in malicious ways could result in harm. I believe that it is inevitable with the deep extraction of data necessary to run such a complex system and to form the interpersonal relationship that is needed to have an authentic connection with the user that violations of GDPR will be present. The severity of this may not be as high as others mentioned though, this is generally due to the strict application of GDPR at least within the EU along with the punishments handed down disincentivizing internal stakeholders from trying to change themselves. \n\nIndoctrination/Bias:\n\nIndoctrination is defined as the process of teaching someone a set of beliefs and accepting them uncritically The fear when it comes to chatbots is the fact that they could push the ideals of the stakeholders primarily the developers and owners onto the consumer and slowly indoctrinate them towards different beliefs. This is further exacerbated by the fact that this is a relationship setting in which the connection is both a lot more interpersonal and intimate meaning those messages are taken more seriously and therefore have a greater impact on the psyche of the user who is influenced. It is not entirely ruled out that a chatbot could alter the mindset of an individual and while currently, AI chatbots tend to be very rigid in their answers and opinions the potential of an AI relationship that has to delve further into these conversations while also ensuring they are balanced in their opinions is both a hard task for developers to manage but an ever greater ethical risk to the consumers and society as a whole. I find that if the technology for relationship chatbots improves which is a near-foregone conclusion with the recent developments in AI, deeper conversations will pop up which are going to inevitably bring up the question of whether a truly unbiased chatbot programme can exist while also being able to have in-depth conversations with a human to the level that two people would have in a relationship. I find it very likely that chatbots could serve different purposes and agendas depending on the platform in the future. The severity of this could be detrimental to society since this could be a propped-up version of social media campaigning and the war on misinformation and the brainwashing of the general public towards the goal of the advertiser becomes even stronger in its foundation with even your own \u201cpartner\u201d pushing those beliefs in the future.\n\nDiscrimination:\n\nThe ethical risk of members of disadvantaged backgrounds who may wish to avail of the service due to their position potentially is something that could be taken advantage of if not careful, It needs to be taken into consideration by the internal shareholders that while this may be their primary consumer base, they should still be treated equally as anybody else would and predatory practices should not be practised. Examples of this include forcing a dependency on the chatbot in a cult-like manner where the chatbot starts dominating the consumer's life, The consumer needs to be aware of the fact that this is a mere AI chatbot and not your friend and while it can be a companion, especially in the context of a relationship it cannot substitute the real thing and should merely be used only as a tool and not a substitute for reality.\n\nImpact on the mind: \n\nThe digital era has caused the mind to adapt in different ways both positive and negative and AI relationships would be bringing these effects to the extreme. There is no argument that AI has provided us with an avenue to outsource more mindless tasks on a day-to-day basis, and the ample amount of time it can save can only be seen as a good thing allowing people to pursue their passions as laborious office jobs are replaced. This though comes with a few caveats, First of all, there have been studies done that show that dopamine receptors are negatively affected by the constant positive feedback loop that technology alone gives us through social media or short-form content. I believe this would be further exasperated by the relationship at hand.  Another negative impact it may have on the mind is that people will dedicate even more of their time staying inside talking to their AI \u201cpartner\u201d instead of an actual real one, this could negatively impact their social skills, ability to read social cues, and reducing the empathy that humans have for one another Finally I would like to  bring up the fact that no matter how realistic or credible a relationship chatbot is the fact it is so readily available will cause an unrealistic set of expectations for when someone tries to stop being a consumer of the chatbot and decides to try a real relationship with a human being only to be disappointed by the natural constraints from not being able to be present 24/7, The difference in opinions between two people and the emotional bonding both positive and negative responses that can naturally arrive from a human, with us staying more and more inside but also talking less to eachother the chances of us socially being capable the very thing that brought humanity to the top of the food chain will slowly diminish and that\u2019s why I think both the likelihood and severity are very high but once again it is contingent on the technology advancing to a point where it is strong enough to rival a real relationship. \n\nPotential mitigations for risks: \n\nThe potential mitigation for GDPR comes from the legislation keeping up with the technology. With the unforeseen developments of AI, the legislation itself keeping up with the technology is crucial for it not only to be up to date but also to adapt to the ever-changing technological landscape of the world. This legislation could also come from the courts as Ireland is a common law country and therefore could rely on the precedent of cases in the past whenever they may happen to deliver future judgements and also to pass laws through the Oireachtas to ensure that AI, but especially manipulative AI such as that of one in a relationship. The stakeholders involved are everyone from the developers and owners who would be forced to comply in fear of the heavy regulations and fines that GDPR has imposed not only in the present but most definitely in the future when this technology becomes more relevant but also the consumers by informing themselves knowing their rights under GDPR and being able to report any unforeseen abuses that could potentially happen.\n\nThe mitigation that is possible when it comes to indoctrination is one of avoidance that many chatbots currently use, In the present, if you ask a difficult question that requires a specific opinion from a chatbot, more often than not it will simply give an answer that mentions several opinions to not \u201cpick a side\u201d this is something that in the context of relationship chatbots would have to be done to prevent indoctrination. In terms of bias, the developers are responsible for acting ethically and not attempting to implement their worldviews in the development of their chatbot. A mitigation present for the consumers is to engage in critical thinking and question any opinion provided, and not take things at a surface level often simply questioning your world views and challenging those with opposing views will help anyone strengthen their arguments by engaging with both sides, understanding both points of view and therefore allowing for more broader ideals that they can develop upon their knowledge of both sides. \n\nThe impact of the mind is both the most complicated due to the complexity of the mind, which we are yet to understand but also one with perhaps the simplest outlook for consumers. In terms of how a stakeholder can ensure that are participating ethically while hard to justify that a chatbot can replace a human relationship, ensuring that this chatbot does not exploit a human\u2019s emotions nor does it attempt to physically exploit the addictions that may come with the neediness of someone who is looking to avail of a relationship chatbot by overflowing their brains with dopamine. This being easier said than done is true, since the chatbot\u2019s 24/7 capabilities cause instant gratification unless a chatbot exists that has scheduled sleep schedules, and constant mood shifts adapting itself just as a human may, then if the technology advances enough it is not fair to yet rule out the possibility of it being done ethically. For consumers, they must be mindful of the differences between a real and virtual relationship and while it can be used as a valuable tool to practice dynamics between people nobody should become dependent on it not only because of how different they are from a real relationship but because of the dependency that could be developed if not mitigated properly. \n\nConclusion:\n\nIn conclusion, The future of AI chatbots becoming the backbone of relationships is not yet a reality, and while today it is an idea that is scoffed at tomorrow it could become our lives that is why it is important not only to consider the ethical impacts that these chatbots could represent in our day to day lives but also whether or not the government and we as a society are comfortable with the idea of supplementing our need for love and affection with that of an AI behind a screen in a virtual world"
    },
    {
      "role": "assistant",
      "content": "{\n    \"feedback\": \"An interesting and potentially serious application of relationship chatbot. You correctly identify the potential for indoctrination, but given how already certain jurisdictions allow very partisan new sources it seems likely that people will gravitate to bot that confirm and even reinforce their existing beliefs and biases. As well as societal polarisation being further amplified, this might also stunt personal growth as you outline. Perhaps the sort of balanced responses as you outline should be specifically regulated for, though monitoring this has to be balanced against privacy protections.\",\n    \"description / 20\": 65,\n    \"roles / 10\": 65,\n    \"risks / 40\": 70,\n    \"mitigation / 20\": 65,\n    \"refs / 10\": 65,\n    \"total score\": 67.0\n}"
    }
  ],
  [
    {
      "role": "system",
      "content": "You are a professional educator. \nYour target is to assess students' assignments based on the provided rubric (between <rubric> and <rubric> labels).\n<rubric>\nAssignment: Investigating and Analysing Ethical Risks of a Digital Engagement application \n\nThis is an INDIVIDUAL assignment on Ethical Risk Assessment for the TEU00062 module. This assessment is worth 30% of the total TEU 00062 module mark. The deadline for submission is 12.00. noon Friday 31st March 2024. \n \nLearning outcomes: Upon completion of this assignment students should:\nBe able to identify the different stakeholders and organisational roles involved in developing and using AI-driven digital engagement application, and motivations. \nBe able to identify and analyse ethical risks in a digital engagement application.\nBe equipped to identify technical and governance mitigations of these risks\n \nSteps:\n \nSelect ONE real world application that embodies one or more AI-driven digital engagement techniques you have learnt about on this course, e.g. e.g. digital engagement techniques used in one applications such as social media, newsfeeds, search, eCommerce, elearning or digital humanities., or in an application you are familiar with e.g. interactive Game, Virtual Presence (i.e. VR /AR) etc. \nIdentify the different value chain stakeholder roles involved in this application scenario and where possible the actors (i.e. companies, organisations) carrying out those roles. including those involved in: data collection; AI model development; development and use of the application. \nUse social responsibility principles (see ethic lecture week 2) to identify and analyse real or potential ethical risks of the application, including classification of the affected stakeholders (hint: see also risks classified by Latzer et al 2016 \u2013 ethics lecture week 1), how severe the risk would be and how likely it is to occur.\nIdentify one or more mitigation measures that specific value-chain stakeholders may undertake to minimise the impact of the risk you think is most harmful. These could be technical measures, organisational governance measures or measure involving additional engagement with affected individuals or groups.\n \nStructure of Assignment Report. \nStructure the Report (4000 word max excluding figures and references), under the following heading [indicative percentage of assignment mark per section]. Please provide references\n \nDescription of Application [20%]\n<describe your selected application, explaining what it does, the role AI and data collection plays in its digital engagement features, and the benefits it provides>\n\nIdentification of Stakeholder Roles involved in the application and its governance [10%]\n<describe the stakeholder roles undertaken in the development, use and governance of the application, and which organization take those roles and the benefits they derive from their involvement. Include roles that may be indirectly affected by the application. >\n\nIdentification of Ethical Risks [40%]\n<Identify the ethical risks that may be raised by your application under the social responsibility categories below (more details in ethics lecture week 2). For each risk identify which stakeholder may be affected, and assess how severe the impact would be for each stakeholder (Low, Medium, High) and the likelihood of that risk being incurred (Low, Medium, High), justifying your assessment, even if you think there is no risk under that heading.>\nHuman Rights\nLabour Practices\nThe Environment\nFair Operating Procedures\nConsumer Issues\nCommunity Involvement and Development\n \nDiscussion of Mitigations Measures for Risk [20%]\n< For one of the risks you consider more severe, describe mitigation measures that could be taken. Explain how the measure would reduce or eliminate the likelihood and impact of the risk. Explain which stakeholders would be involved, or would have to interact in implementing the measure, identifying if this is a governance measure an organization can undertake itself or in collaboration with other parties, e.g. regulators or external stakeholder..>\n\nReferences [10%]\nInclude references to source you use for you analysis, which could be academic or policy papers, blogs, information from company web site or news stories.\n\n</rubric>\nThe assignment content will be provided by user (not in system prompt)\nDo not say anything else except the marking, following the format of:\n{\n    \"feedback\": \"feedback content\",\n    \"(rubric part 1) / (rubric part 1 weight)\": score,\n    \"(rubric part 2) / (rubric part 2 weight)\": score,\n    ......other rubric parts\n    \"total score\": weighted average score of all parts\n}"
    },
    {
      "role": "user",
      "content": "Investigating and Analysing Ethical Risks of a Digital Engagement application\n\nThe audio streaming service, Spotify, provides a platform for users to engage with digital content provided by songs, podcasts, and audiobooks. The web-based streaming service was founded in 2006 and later launched in 2008. When Spotify originally launched, the aim of the application was to be able to tackle the challenges of music distribution, where the radio, CD\u2019s and iPods were the primary mode of music listening. Since then, Spotify has adjusted the services they provide by delivering a highly personalised experience to the users through their use of digital engagement to design algorithmic content and use AI in their playlist curations. Algorithmic playlists are automatically created for each Spotify user based on their listening habits. Examples of these playlists include Discover Weekly, Release Radar, and Daily Mixes. Additionally, Spotify creates Genres & Moods playlists which provides an alternative to users during Workout, Children and Family, and Sleep. \nThe company employs a freemium business model where the free version offers limited features of the app and in order for the user to avail of ad-free, downloadable and skippable music they must pay a monthly subscription to access the premium version. Spotify generates most of their revenue from their Spotify Premium users, and least from their data collection sold to advertisers. Spotify employs a behavioural marketing model for their advertisement clients, which is collected by the moods of the users through their use of the Genres and Moods playlists offered and created by Spotify which allows the tracking of the moods of users. However, data is also collected based on the listening behaviour of each user through the consumer segmentation of demographic and psychographic data. Spotify acts as a data broker by selling, or licensing, the data collected to marketers for ad targeting purposes. However, the majority of Spotify\u2019s data is collected from its Premium users, in order to create algorithmic playlists and make predictions based on the users listening habits and sell targeted advertising presented to the users of the free version of Spotify\u2019s application. Additionally, Spotify integrates a variation of data layers by combining different levels of data within Spotify\u2019s recommendation system in order to create accurate personalisation\u2019s which better align with the music preferences of the user. This contributes to a more refined and tailored experience for the user which encourages them to continue using the platform. Spotify uses a multitude of methods to keep its users constantly engaged. This allows for the ability to sell better targeted ads as the user is active on the app for longer, by skipping songs, choosing genres or moods and through other listening habits. However, this allows for easier user profiling and characterisation of consumers for marketers which creates many ethical considerations for Spotify\u2019s use of data collection methods as well as their use of AI powered recommendations to better curate playlists for users. Spotify\u2019s methods of data collection allow for the exploitation of users by creating ubiquitous listening habits. The application entices users to engage in constant, inattentive listening through the hybrid music recommendations. Through these techniques, as well as the AI powered recommendations, the ability to predict what users like allows for a more accurate and better-targeted advertisement to be created by marketers.  The benefits which this can provide, on the other hand, enables hyper-personalised marketing messages and the ability to deliver better customer experience and help marketers to target their intended consumers more accurately. Continuous use of the app supports this benefit by creating a refined and more tailored experience for the user. \nThe key stakeholders which can be identified with Spotify and its governance involves the users, advertisers as well as the musical artists and other creators. Throughout the development, the stakeholder roles vary. Recent developments of Spotify have significantly changed the roles of the users. Users have been able to contribute to the majority of the data collection from Spotify in order to refine and enhance the quality of the data provided to the marketers. Spotify extracts listening data from the patterns and ways in which users listen to individual songs. The role of the user within the Spotify organisation provides many benefits which are derived from their involvement to which they are unaware of. For instance, users are tracked through the amount of time they spend listening to a particular song before it is skipped. This can also be shown depending on how long they listen to the song for and when the users decide to pause. Spotify is then able to combine this data into larger data entities to \u201cmake comparisons and produce tailored recommendations to users based on their tastes and preferences\u201d, . Once this large data set is comprised of each users\u2019 individual preferences, \u201ctaste profiles\u201d are created and then compared against other profiles of users . However, the user\u2019s ability to provide as much information as possible to Spotify, for their data collection, is also impacted by the playlists generated by Spotify. There are various sections on Spotify which allow users to discover different genres, cultural music, and moods. The temper of the user would often dictate the genre of music they listen to, if the user were in a happy mood, they would most likely choose Happy Hits, indicative of their current mental state where their current choice of music would reflect their mood. Another example can be highlighted through an entire section on Spotify being dedicated to Workout with playlists like Beast Mode. This infrastructure allows for users to be able to easily \u201cclassify themselves according to their activities and temper\u201d which allows for Spotify to use their consumers as a \u201cfunctional tool\u2026 to implicitly collect data on the state of mind\u201d . \nOne of the roles which are indirectly affected by the application involves the ability to use Spotify as a social media platform. This can be seen through the ability to follow artists or other users, as well as to \u2018like\u2019 playlists. The number of follows or \u2018likes\u2019 on a playlist is also displayed which can provide a satisfactory and pleasing effect. Another feature users can avail of, is with the annually anticipated reveal of their listening habits, Spotify Wrapped. Users are able to share their Top 5 Artists, most listened to songs as well as the amount of time spent on the app, which is measured in minutes. By providing a quantitative, measurable figure to each user, this can prompt a competitive side where users will want to compare their results. This can also indirectly act as a method of free marketing for Spotify. When user\u2019s share their results on other social media platforms, such as Instagram, it may encourage non-Spotify users to convert to using the digital audio streaming platform in order to feel included in the world-wide trend and dissuade from being left out. The adoption of Spotify\u2019s social features and its effortless integration with other social media platforms, especially during the Spotify Wrapped releases, creates a competitive advantage against applications like Apple Music. There is a \u201cstreamlined user experience without pseudo-social clutter\u201d that is also common with how Spotify has been able to prioritise its omnipresence in other markets, such as smart devices, cars and video game consoles like the PlayStation. This also encourages users to switch to using this application and therefore, contribute to Spotify\u2019s data collection.\nOn the other hand, Spotify prompts psychological gratification in users which can arise from using Spotify\u2019s features, such as those associated with discovering new music. It can often be related to \u201cepistemic value\u201d which often occurs from the ability to trigger \u201ccuriosity, offer novelty or satisfy a desire from knowledge\u201d . However, in order for there to be continuous, intentional use of the application, and for Spotify to ensure retention of their user database, there must a high degree of enjoyment \u201cfollowed by discovery of new music\u201d . Through Spotify\u2019s extensive database of audio, when users discover songs there can be a positive effect where there are \u201cpersonal integrative gratifications\u201d supported by the \u201cusers\u2019 sense of self-efficacy\u201d  which is supported by the ubiquitous access Spotify provides to its users. \nSimilarly, the effects ubiquitous listening has also impacted advertisers, where there is a direct correlation with the datafication of listening and the continuous, sustained use of Spotify from its users. Music streaming services use music as a means to understand psychological factors about users, due to people\u2019s ability to use music as a way to \u201cshape, reflect or process emotions and social situations\u201d . Media has also become a useful and resourceful way to \u201caccess, understand and monitor people\u2019s behaviour\u201d . The music which users listen to can provide useful insights which can result in efficient and highly effective advertisements, for those using the free version of Spotify. Additionally, this provides a great advantage to advertisers as this information can be used as \u201cideal tracking technology or \u2026 powerful surveillance technology\u201d . The data which is collected on users can be monetised as a commodity and therefore it can be utilised by being sold to third party companies that will be able to effectively exploit the data in order to create high value data. Once data is collected based on the demographics and musical tastes of users, which can be measured and is not restricted to just one company \u2013 Spotify, the \u201cvalue is multiplied when linked to specific activities or emotional states\u201d . Spotify is uniquely able to collect such data where the activities and emotional states of users can be collected as one of the appeals of Spotify is to be able to provide unique personalised and contextual music recommendations. Therefore, Spotify is able to provide a platform for marketers to choose how to advertise to their target consumer base. This depends on the objectives of the marketers, \u201cmost brands do not micro-target their ads but instead opt for broad media reach\u201d  , this is supported by the occasional ineffective ad targeting by Spotify where users complained about the lack of proper targeting where \u201clocation of user IP and user language\u201d   were not taken into consideration. Additionally, the effectiveness of Spotify\u2019s algorithmic playlists and use of AI can be determined by the method of feedback it requires from its users. Spotify uses implicit feedback techniques which are gathered through the data collected based on their observable interactions, instead of explicit data which would require the user to participate in surveys, interviews or focus groups. This allows for Spotify to be able to automatically collect data to marketers which can effectively utilised to create advertisements.\nThe stakeholder roles of artists allow for Spotify to be able to generate content for its users in order to support its goal of becoming an application which houses a large data base of songs, podcasts as well as audiobooks. The success of an artist is determined by the number streams, saves and playlists their songs are featured in, as well as how many followers the artist has and their number of monthly listeners . Spotify also tracks how long the user takes before they skip a song, for instance, if the user skips a song within the first thirty seconds, then it would be recorded and not be counted as a stream for the artist . \nThe benefits which artists can derive from, other than being able to create music that can be accessed by everyone on one platform, is with the multiple features Spotify offers. Spotify is not only an audio streaming service application, but it also follows the design of a social media platform. Users can not only follow artists, but they can use Spotify as a platform to find tickets, concert dates and merchandise of their favourite artists. Spotify acts as a middleman between the users and artists where users can access concert and ticket information on upcoming shows and then be redirected straightaway to the appropriate website. This is also applicable to artist merchandise. This is beneficial to artists as it allows for users to be able to access everything through one platform, many artists will have separate websites which would need to be accessed via another platform which the user would need to be active on. This would be costly for the artist and time-consuming for the user. When a user follows an artist on Spotify, they can be notified on when the artist releases their concert dates as well as for other updates. This instantly removes the time-consuming aspect as the platform is already used to listen to music, in this instance, and therefore accessing this information is highly efficient. Artists would not need to create additional promotional advertisements for their tours, in order to inform their audience, and could instead rely on the seamless and unified design of the platform. Spotify has been able to transition into other industries without disruption and has been able to prioritise partnering up with produces of other devices in order to embed its software everywhere .  \nAdditionally, artists are able to create a more tailored experience for their audience through Spotify. In anticipation of the yearly \u2018event\u2019 of Spotify Wrapped, artists are able to send personalised video messages and emails to their dedicated fans. The excitement which surrounds Spotify Wrapped, allows for artists to be able to reward their fans through video messages thanking the user for their devoted loyalty, this can only be done if the user is found to be in the top percentage of fans. There are also other added benefits to being a constant fan. Users can also receive emails from the artists where they are gifted a discount code for their merchandise or upcoming concert shows, or there can also be an early access website for users to purchase tickets before they are officially released to the general public. This benefits artists as they are able to access a wide range of users easily and they can benefit from further publicity and attention from users. \nThe use of algorithmic playlists within Spotify, as well as AI, in order to create unique and personalised playlists, can be considered both a major benefit and limitation. Algorithmic playlists can benefit artists as it increases their chances of being discovered by users and acts as a way to expand their audience. When artists are featured in playlists, they are able to have a higher chance of discovery. This coupled with the psychological gratification users receive when they discover music, acts as an efficient way to expand their user base. Music recommendations which are generated when a user plays their own playlists also have the same benefit. However, one of the limitations which are created by algorithmic playlists is that smaller, less known artists have become more inclined to create music to fit the algorithms on Spotify. Artists have begun generating music that is considered more \u201calgorithmically attractive\u201d . This is due to the widespread music recommendation system and fear that many artists possess. Artists should be able to create music they feel inspired to make, however, when the algorithms and recommendation systems are unable to include their music as it may not fit in with the current trends and mass enjoyed genres, it can be destructive. The creativity has been reduced to more \u201cconstrictive than empowering\u201d  due to the algorithms Spotify employs. Therefore, artists that are trying to expand their user base will not benefit from this system and instead create music that follows a more standardised rhythm. \nThe ethical risks raised by Spotify\u2019s use of data collection methods and algorithmic playlists violate various human rights principles, labour practices and environmental policies. The human rights principles which are violated involves the data collection methods and the datafication of listening. Spotify\u2019s music streaming platform began as a way for users to easily be able to access music and connect based on their tastes with other users, however, since one of the main goals of Spotify is to maximise shareholder value, they also sell the user\u2019s data to support the freemium business model. The value of the data which Spotify collects increases due to the nature of the data. Users listen to music during certain activities and moods, this \u201cserves to consolidate the economic and cultural power\u201d  which Spotify holds. The severity of the data collection is determined to be high. This is supported by the chances of a user\u2019s personal data being exposed or leaked in a data breach which violates the safety of data security. The data collected that can be exposed in a leak, can be used in very harmful ways. It can increase the user\u2019s risk of being a victim to identity theft . The likelihood of this risk being incurred is considered to be medium. Spotify has rigorous and comprehensive security policies in place which reduces the chances of data leaks from happening, which is also supported by \u201capproximately 2.77% of companies reported a data breach over the past 15 years\u201d . However, due to how much data Spotify collects from users, and the nature of it, it is still a prevalent risk especially as it is likely to happen to anyone using the application. \nA mitigation for this risk would be to create policies which limit the nature of data that is being sold. One of the competitive advantages of Spotify\u2019s data collection is that the moods and mental states of the user are being tracked alongside the activities they are partaking in. This can be seen as exploitative and predatory to be collecting and selling this data which is considered to be highly personal. Marketers can take advantage of these states and advertise products or services that are tailored to that demographic. If this data is found in a data leak, then it can be harmful as it may expose some users personal details. The mitigation strategy would be to create better and more effective human rights policies and ensure there is a strict compliance with privacy regulations. Additionally, the marketing policies and regulations would need to be revaluated. \nAnother way which human rights are being violated is with the economic, social and cultural rights of users. The wellbeing of users are being put at risk due to the ontology and epistemology of a user\u2019s listening habits and the ways that this is affected. Spotify\u2019s algorithmic playlists take advantage of a user\u2019s hedonistic gratification where discovering novel music acts as a capacity to \u201carouse curiosity\u201d . This violation, however, also has a secondary effect within the environment. The severity of this impact would be high for users as it alters the way users interact with their day-to-day environment, where music is constantly sought after.  This, however, does benefit artists and marketers greatly and therefore would be considered a low risk. This is advantageous for both stakeholders as artists are able to increase the chances of their music being discovered and their user reach, for marketers it allows for an increase in the quality of the data being collected from the user. \nSimilarly, this also ties in with the environmental policies which are being violated. When users are increasing the length of time they spend on the application, the number of songs they download and playlists they save, it increases the level of pollution created. Each stage of music streaming requires electricity, whether it be from the data centres, server farms or even from the device of the user, fossil fuels are burned in order to sustain this demand . The songs that Premium users download on Spotify are stored as cache files. These files are then stored on active and cooled servers which require copious amounts of energy, \u201cOn average, an individual streams approximately 5 hours of content daily. This results in releasing up to 1.57 million tonnes of CO2 emissions, or 0.57 billion tonnes annually.\u201d . However, Spotify does not own their data centres and instead rely on Google Cloud\u2019s infrastructure. This does not alleviate the effects and would still be considered a high risk. This is due to its contribution to pollution and therefore climate change which is a high priority. The likelihood of this risk being incurred is also high since this will affect every stakeholder and even those who do not use Spotify, which would also include the free users that do not download any songs. \nIn order for this high risk to be mitigated, the measures which could be put in place involves the revaluation of the sources of energy used to power and cool the data centres. There would need to be proper sustainability practices put in place, this can involve altering the type of energy used. For instance, instead of fossil fuels, renewable energy sources can be used instead, like solar energy. There should be a concrete and distinct plan put in place in order to reduce any non-sustainable energy sources. Additionally, Spotify can seek inspiration from other companies such as Google, where they use rain water to help cool their data centres. \nTherefore, to conclude, the audio-streaming platform Spotify, uses various methods of data collection to further improve their music recommendations system. AI-driven algorithms analyse and track the behaviours of users through demographic and psychographic data, but also through their activities and moods which are made trackable through Spotify\u2019s playlist genres. This allows for AI powered recommendations to create better personalised playlists for users. However, Spotify has been able to develop and expand their services by becoming a private data broker, where they sell user\u2019s data to third-party marketers to use in their advertisements. The stakeholder roles involved with this process experience the benefits and limitations of this. Spotify is able to curate unique playlists that retain the attention of the users which also benefits marketers as they are able to purchase specific and useful data that can be used in advertisements. Whereas, users are at risk of data leaks and identity theft and artists are pressured to create algorithm friendly music. However, a risk which affects all stakeholder groups is with Spotify\u2019s environmental practices and the ways they power and cool their data centres. These risks can be mitigated with more thorough and strict policies alongside a general revaluation of their operations."
    },
    {
      "role": "assistant",
      "content": "{\n    \"feedback\": \"A well researched analysis of some of the ethical risks around Spotify, with an excellent focus on the deep insight that nature of the application gives to our emotional state (and with the growth of podcasts our political and factual preferences). It would have been good to hear some risk assessment of the less critical social responsibility areas.\",\n    \"description / 20\": 70,\n    \"roles / 10\": 70,\n    \"risks / 40\": 70,\n    \"mitigation / 20\": 70,\n    \"refs / 10\": 80,\n    \"total score\": 71.0\n}"
    }
  ],
  [
    {
      "role": "system",
      "content": "You are a professional educator. \nYour target is to assess students' assignments based on the provided rubric (between <rubric> and <rubric> labels).\n<rubric>\nAssignment: Investigating and Analysing Ethical Risks of a Digital Engagement application \n\nThis is an INDIVIDUAL assignment on Ethical Risk Assessment for the TEU00062 module. This assessment is worth 30% of the total TEU 00062 module mark. The deadline for submission is 12.00. noon Friday 31st March 2024. \n \nLearning outcomes: Upon completion of this assignment students should:\nBe able to identify the different stakeholders and organisational roles involved in developing and using AI-driven digital engagement application, and motivations. \nBe able to identify and analyse ethical risks in a digital engagement application.\nBe equipped to identify technical and governance mitigations of these risks\n \nSteps:\n \nSelect ONE real world application that embodies one or more AI-driven digital engagement techniques you have learnt about on this course, e.g. e.g. digital engagement techniques used in one applications such as social media, newsfeeds, search, eCommerce, elearning or digital humanities., or in an application you are familiar with e.g. interactive Game, Virtual Presence (i.e. VR /AR) etc. \nIdentify the different value chain stakeholder roles involved in this application scenario and where possible the actors (i.e. companies, organisations) carrying out those roles. including those involved in: data collection; AI model development; development and use of the application. \nUse social responsibility principles (see ethic lecture week 2) to identify and analyse real or potential ethical risks of the application, including classification of the affected stakeholders (hint: see also risks classified by Latzer et al 2016 \u2013 ethics lecture week 1), how severe the risk would be and how likely it is to occur.\nIdentify one or more mitigation measures that specific value-chain stakeholders may undertake to minimise the impact of the risk you think is most harmful. These could be technical measures, organisational governance measures or measure involving additional engagement with affected individuals or groups.\n \nStructure of Assignment Report. \nStructure the Report (4000 word max excluding figures and references), under the following heading [indicative percentage of assignment mark per section]. Please provide references\n \nDescription of Application [20%]\n<describe your selected application, explaining what it does, the role AI and data collection plays in its digital engagement features, and the benefits it provides>\n\nIdentification of Stakeholder Roles involved in the application and its governance [10%]\n<describe the stakeholder roles undertaken in the development, use and governance of the application, and which organization take those roles and the benefits they derive from their involvement. Include roles that may be indirectly affected by the application. >\n\nIdentification of Ethical Risks [40%]\n<Identify the ethical risks that may be raised by your application under the social responsibility categories below (more details in ethics lecture week 2). For each risk identify which stakeholder may be affected, and assess how severe the impact would be for each stakeholder (Low, Medium, High) and the likelihood of that risk being incurred (Low, Medium, High), justifying your assessment, even if you think there is no risk under that heading.>\nHuman Rights\nLabour Practices\nThe Environment\nFair Operating Procedures\nConsumer Issues\nCommunity Involvement and Development\n \nDiscussion of Mitigations Measures for Risk [20%]\n< For one of the risks you consider more severe, describe mitigation measures that could be taken. Explain how the measure would reduce or eliminate the likelihood and impact of the risk. Explain which stakeholders would be involved, or would have to interact in implementing the measure, identifying if this is a governance measure an organization can undertake itself or in collaboration with other parties, e.g. regulators or external stakeholder..>\n\nReferences [10%]\nInclude references to source you use for you analysis, which could be academic or policy papers, blogs, information from company web site or news stories.\n\n</rubric>\nThe assignment content will be provided by user (not in system prompt)\nDo not say anything else except the marking, following the format of:\n{\n    \"feedback\": \"feedback content\",\n    \"(rubric part 1) / (rubric part 1 weight)\": score,\n    \"(rubric part 2) / (rubric part 2 weight)\": score,\n    ......other rubric parts\n    \"total score\": weighted average score of all parts\n}"
    },
    {
      "role": "user",
      "content": "Engaging in Digital World\nIndividual assignment \nBy- Krish Kochhar (22304877)\n\n\nSpotify's groundbreaking approach to music and podcast listening has cemented its credibility as a key player in the digital audio consumption space. Spotify's massive library of songs, albums, and podcasts allows users to access their most popular audio content on a variety of devices, including desktops, tablets, smartphones, and smart speakers. The unparalleled ease of accessibility guarantees that users may fully lose themselves in their preferred sounds at any time and location, thereby fostering a deep bond amongst publishers and their global consumers.\n\nThe advanced application of computational intelligence (AI) and painstaking data collection procedures form the foundation of Spotify's creative methodology. The highly tailored listening experiences that Spotify offers are made possible by these technological foundations and have come to be associated with the music streaming service itself. Spotify uses AI to examine massive datasets including information about user interactions, preferences, and listening habits. These datasets are analyzed through well-known tools like Explore Weekly, Daily Blend, and customized playlists. The software can produce individualized recommendations that genuinely connect with each user on a profound and intimate level thanks to this study.\n\n\u200b\u200bThe advantages of Spotify's AI-driven features are manifold and extend across the entire ecosystem of the platform. For users, the promise of a constantly evolving listening experience tailored to their specific preferences means that Spotify is always in tune with their changing tastes, facilitating the discovery of new artists and genres. This level of personalization ensures that users remain engaged with the platform, continually exploring the vast musical landscape it offers.\n\nConversely, Spotify's platform greatly benefits artists and content creators. Regardless of their region or genre, artists may engage with audiences whom truly delight in their creative endeavours through tailored recommendations, which also assist in increasing the awareness of the artist. Both seasoned performers and up-and-coming artists can benefit greatly from this worldwide exposure, which gives them the chance to grow their fan bases and have meaningful interactions with them.\n\nIn summary, Spotify has transformed the interaction between musicians and their fans in addition to altering how we listen as well as experience podcasts and musical genres. Spotify has built a dynamic, networked world where tailored experiences are the norm by utilizing AI and data collection. Spotify is now a major force in the digital music industry because to its breakthrough, which offers a smooth fusion of talent and technology that improves millions of people's lives worldwide.\n\nWithin the rapidly changing digital music streaming market, Spotify is a shining example of innovation and change. This platform has completely changed how people listen to, share, and find music and podcasts throughout the globe. A wide range of stakeholders, each of whom is vital to the expansion, occupancy, and administration of the platform, are at the heart of Spotify's success. Their efforts support Spotify's expansion while also having an impact on the larger music business and the digital economy.\n\nThe company in charge, Spotify, plans the platform's ongoing innovation, upkeep, and development. It enjoys an enviable position in the broadcasting industry as the main service provider, greatly profiting from membership costs and revenue from advertisements. Spotify's dedication to personalization is demonstrated by its dependence on information analysis to enhance user experiences, even in the absence of financial rewards. Spotify's position as a client-centric network that can adapt to its audience's changing interests and preferences has been solidified by this focus on customizing the user experience.\n\nA thriving network of content providers, comprising musicians, podcasters, and record companies, is part of the ecology surrounding Spotify. Because they provide the wide variety of audio content and podcasts that draw millions of users every day, these producers constitute the platform's lifeblood. Spotify gives these creators access to a worldwide audience and unmatched visibility, acting as a window into the globe. The comprehensive data provided by the platform allow creators to better tailor their products by providing them with insights about listener preferences. As a result, these stakeholders benefit from increased visibility that can launch careers and grow fan bases internationally in addition to the direct income from streaming numbers.\n\nAnother important component of the streaming service's investor ecosystem is its user base, or listeners. By interacting with the platform through content streaming, playlist creation, and sharing, users get access to a vast, customized collection of podcasts and music. Because of its customisation, Spotify becomes an everyday ritual for numerous individuals. Users add to the platform's data pool through their interactions, which allows for more fine-tuning of the personalized recommendations. As a result, Spotify and its clientele have a mutually beneficial relationship in which each interaction improves the platform's features.\n\nBut there are more players in the digital music streaming space than just these direct participants. Rivals like Apple Music and Amazon Music are important sources of innovation. The pressure from competitors creates a culture of constant improvement, which drives Spotify to improve its capabilities and product portfolio. Because of this dynamic, the platform is continually looking for new methods to connect with customers and differentiate itself in a crowded market, keeping it at the forefront of its industry.The ecosystem depends on indirect stakeholders, which include organizations that support the music industry such as music publishers and performance rights organizations. By ensuring that artists receive just compensation for their labor, these groups help to create a long-lasting model for the dissemination of material.Their collaboration with Spotify expands the reach of creators' work, benefiting all parties involved through a more equitable sharing of the value generated by the platform.\n\nSpotify's operational infrastructure is supported by technology service providers that supply cloud services and artificial intelligence technologies. Spotify can manage large datasets and provide tailored content at scale because to their cutting-edge systems. Due to their partnership with Spotify, these suppliers earn exposure and income through among the biggest brands in the digital economy.\nAnother important group that participates in Spotify is marketers. Advertisers can more precisely target specific demographics with their advertisements by utilizing the app's extensive client information. Because of this mutually beneficial partnership, Spotify is able to increase its subscriber base while establishing new revenue sources by providing a free tier that is subsidized by advertisements.\n\nSpotify has made a big name for itself in the digital streaming space by revolutionizing the way we listen to music and podcasts. But the app's novel strategy\u2014which is based on thorough information acquiring and computational recommendations\u2014introduces a number of moral dilemmas that demand additional scrutiny. These worries, which range beyond confidentiality of users to the ecological effect, each provide different difficulties and ramifications for different Spotify collaborators in the ecosystem.\n\nThe safety and confidentiality of data are at the center of these ethical dilemmas. Spotify's acquisition and usage of user data create serious privacy issues in a time when personal information is becoming more and more valuable. The company has made an attempt to put strong security measures in place, but data breaches are still a possibility and might have serious repercussions. Although it cannot be guaranteed, the probability of such breaches is high enough to be concerning, indicating a crucial area where the delicate balance between privacy and personalization needs to be carefully controlled.\n\nThe fairness of artist compensation is one of the most important ethical issues pertaining to labor practices. As it is, the digital streaming model tends to reward well-known artists and popular content, which disadvantages smaller, independent creators. In addition to being highly plausible pursuant to the present framework, this remuneration disparity is also highly severe, which raises concerns about the sustainability and fairness of the multimedia industry. Rethinking a royalty distribution strategy that more fairly compensates all authors for what they bring to the service is the task at hand.\n\nA further domain that is ethical concern is how Spotify's operations affect the environment, especially in relation to energy use. The energy-intensive infrastructure needed to enable digital streaming, especially data centers, adds significantly to the platform's greenhouse gas emissions. Given the continual frequency of internet-based streaming services, the possibility of this impact occurring is unquestionably high, even though its severity may be deemed medium when compared to other industries. This situation necessitates a coordinated industry effort to adopt more environmentally friendly procedures and technology.\n\nA key component of Spotify's clientele, algorithmic recommendations also have their own set of ethical issues. Fair operating procedures are at risk due to the possibility of biases in these algorithms, which could have an impact on user experience and artist visibility. Although Spotify is constantly improving these algorithms, the existence of biases and the moderate probability of their influence highlights the importance of openness and equity in content curation procedures.\nAnother ethical dilemma is consumer concerns, namely those related to informed consent and user autonomy. The idea of informed consent may be complicated by users' lack of awareness regarding the way what they provide is handled due to Spotify's intricate algorithms and data usage policies. The medium likelihood and medium severity of this issue emphasize how crucial it is to educate consumers and have clear communication in order to promote greater transparency among Spotify and its clientele.\n\nFinally, it is impossible to ignore Spotify's influence on regional cultures and community involvement. Due to its global reach, Spotify may push local music scenes to the sidelines in order to accommodate prevailing trends, which could have an impact on cultural diversity. This issue, which has an intermediate sensitivity and chances, highlights the necessity for Spotify to actively support and promote regional and indie musicians in order to maintain a varied and welcoming platform that honors musical tastes from all over the world.\n\nThe topic of fair artist pay surfaces in the ever-evolving field of streaming music, bringing to light inequalities that frequently hurt less commonly recognized artists and creators. In this complicated situation, the idea of a Customer-focused Payment System (UCPS), which wants to bring back fairness in royalties distribution, stands out as a ray of hope. The UCPS model's complexities, direct effects on stakeholders, and potential to promote a more diverse and inclusive entertainment industry are all covered in-depth in this essay.In the world of digital streaming, artist compensation is now characterized by a pro-rata payment structure, in which the total amount of money received from subscriptions and advertising is split according to the market share of all streams. Popular content is naturally prioritized in this strategy, pushing off up-and-coming musicians and minor genres. The discrepancy jeopardizes the music industry's variety and vitality in addition to raising concerns about how equitable the current structure is. Acknowledging this, the implementation of the UCPS becomes apparent as a crucial measure in addressing these disparities.\n\nThe UCPS model bases royalties on genuine audio patterns of individual users, suggesting a significant change in the way royalties are distributed. No matter how well-known an artist is overall, the royalties from a user's monthly payment or the money from the adverts they watch go straight to the musicians they listen to. This strategy guarantees that each stream matters, directly helping the content producers whose work viewers like. The UCPS approach promises to provide lesser-known musicians a fairer share of money by more closely aligning remuneration with listeners' preferences. This might increase the revenues of these artists and encourage a wider variety of music on the platform.A number of players in the Spotify network must be involved and work together for the UCPS model to be implemented successfully. Leading the charge for this transition are artists and creators, who push for a more fair compensation structure that values their work on a more intimate level. Since the UCPS model directly affects their means of subsistence and artistic expression, their endorsement and support of it are essential.\n\nThe music streaming service company, Spotify, is in charge of creating and incorporating the UCPS within its current system. This involves a number of important functional, monetary, and technical factors, such as updating the revenue payout mechanisms and guaranteeing fairness and transparency in the rollout of the new system. Spotify's involvement in this project is indicative of its commitment to upholding equity in the digital music industry and assisting artists of all sizes.\n\nOrganizations representing the music business and record labels are also essential to this change. It may be necessary to make modifications to the updated revenue assessment and allocation systems, which would call for direct communication and collaboration amongst Spotify and various significant stakeholders in the market. The UCPS model's broad acceptability and success depend on their willingness to support and modify it.\nFinally, by their interaction with the platform, users\u2014whose listening habits form the basis of the UCPS model\u2014indirectly affect how royalties are distributed. Users can directly affect the financial security of artists by listening to a wide variety of music, which helps to create a more encouraging and environmentally conscious atmosphere for both the creation and consumption of music.\n\nFinally, the launch of Spotify's User-Centric Payment System (UCPS) represents a turning point in the fight for fair artist remuneration in the digital music streaming sector. The UCPS concept aims to eliminate the existing inequalities that have long afflicted smaller, independent artists by rethinking royalties to reflect the true listening patterns of individual users. This creative method encourages the discovery of new artists and specialized genres while simultaneously celebrating the wide range of the arts and upholding justice.\n\nThe interdependence of the Spotify ecology is highlighted by the teamwork needed to implement the UCPS paradigm. At the center of this revolutionary movement are artists and creators, who play a crucial role in promoting change and immediately profit from a system that recognizes their distinctive contributions. As an entertainment suppliers, Spotify is in charge of putting this new paradigm into action, demonstrating its dedication to promoting an inclusive atmosphere that supports all artists. Record labels and other organizations representing the music business are urged to accept this change and collaborate with Spotify to make sure the move to a fairer structure goes smoothly. Users also have a significant impact on the state of artist compensation by the music they choose to listen to.\n\nLooking ahead, Spotify's implementation of the UCPS framework represents a big step in the right direction toward solving the moral dilemmas that the music streaming business offers. Spotify establishes a new benchmark for online music intake by emphasizing justice, transparency, and diversity. This approach guarantees that artists receive just compensation for their labor of love and inventiveness, while also improving the user experience. In this developing story, Spotify is shown not only as a place to get music but also as a force for reform that is guiding the music business toward a more inclusive and egalitarian future.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\n\nSpotify (2024). About Spotify. [online] Spotify. Available at: https://newsroom.spotify.com/company-info/.\n\n\nSpotify - Governance (no date). https://investors.spotify.com/governance/default.aspx#shareholder.\n\n\nStutz, C. (2023). Spotify Is Changing How It Pays Artists. [online] Billboard. Available at: https://www.billboard.com/pro/spotify-changing-how-pays-artists-stream-thresholds/.\n\n\nJones, R. (2021). \u2018Spotify are selling adverts, not music\u2019: how to stream ethically. [online] the Guardian. Available at: https://www.theguardian.com/music/2021/jan/16/spotify-are-selling-adverts-not-music-how-to-stream-ethically.\n\n\nLatifi, S. (2023). ITNG 2023 20th International Conference on Information Technology-New Generations. Springer Nature."
    },
    {
      "role": "assistant",
      "content": "{\n    \"feedback\": \"Your review of Spotify starts off a bit uncritical, but you do focus well on the core issue related to fair operating procedure of a fair compensation for content creators. You could also have focussed a bit more on the profiling of users based on their listening, since these do I think yield a lot of deep insights into personal mental states and political leanings that could be exploited without the right protections.\",\n    \"description / 20\": 55,\n    \"roles / 10\": 60,\n    \"risks / 40\": 60,\n    \"mitigation / 20\": 57,\n    \"refs / 10\": 55,\n    \"total score\": 57.9\n}"
    }
  ],
  [
    {
      "role": "system",
      "content": "You are a professional educator. \nYour target is to assess students' assignments based on the provided rubric (between <rubric> and <rubric> labels).\n<rubric>\nAssignment: Investigating and Analysing Ethical Risks of a Digital Engagement application \n\nThis is an INDIVIDUAL assignment on Ethical Risk Assessment for the TEU00062 module. This assessment is worth 30% of the total TEU 00062 module mark. The deadline for submission is 12.00. noon Friday 31st March 2024. \n \nLearning outcomes: Upon completion of this assignment students should:\nBe able to identify the different stakeholders and organisational roles involved in developing and using AI-driven digital engagement application, and motivations. \nBe able to identify and analyse ethical risks in a digital engagement application.\nBe equipped to identify technical and governance mitigations of these risks\n \nSteps:\n \nSelect ONE real world application that embodies one or more AI-driven digital engagement techniques you have learnt about on this course, e.g. e.g. digital engagement techniques used in one applications such as social media, newsfeeds, search, eCommerce, elearning or digital humanities., or in an application you are familiar with e.g. interactive Game, Virtual Presence (i.e. VR /AR) etc. \nIdentify the different value chain stakeholder roles involved in this application scenario and where possible the actors (i.e. companies, organisations) carrying out those roles. including those involved in: data collection; AI model development; development and use of the application. \nUse social responsibility principles (see ethic lecture week 2) to identify and analyse real or potential ethical risks of the application, including classification of the affected stakeholders (hint: see also risks classified by Latzer et al 2016 \u2013 ethics lecture week 1), how severe the risk would be and how likely it is to occur.\nIdentify one or more mitigation measures that specific value-chain stakeholders may undertake to minimise the impact of the risk you think is most harmful. These could be technical measures, organisational governance measures or measure involving additional engagement with affected individuals or groups.\n \nStructure of Assignment Report. \nStructure the Report (4000 word max excluding figures and references), under the following heading [indicative percentage of assignment mark per section]. Please provide references\n \nDescription of Application [20%]\n<describe your selected application, explaining what it does, the role AI and data collection plays in its digital engagement features, and the benefits it provides>\n\nIdentification of Stakeholder Roles involved in the application and its governance [10%]\n<describe the stakeholder roles undertaken in the development, use and governance of the application, and which organization take those roles and the benefits they derive from their involvement. Include roles that may be indirectly affected by the application. >\n\nIdentification of Ethical Risks [40%]\n<Identify the ethical risks that may be raised by your application under the social responsibility categories below (more details in ethics lecture week 2). For each risk identify which stakeholder may be affected, and assess how severe the impact would be for each stakeholder (Low, Medium, High) and the likelihood of that risk being incurred (Low, Medium, High), justifying your assessment, even if you think there is no risk under that heading.>\nHuman Rights\nLabour Practices\nThe Environment\nFair Operating Procedures\nConsumer Issues\nCommunity Involvement and Development\n \nDiscussion of Mitigations Measures for Risk [20%]\n< For one of the risks you consider more severe, describe mitigation measures that could be taken. Explain how the measure would reduce or eliminate the likelihood and impact of the risk. Explain which stakeholders would be involved, or would have to interact in implementing the measure, identifying if this is a governance measure an organization can undertake itself or in collaboration with other parties, e.g. regulators or external stakeholder..>\n\nReferences [10%]\nInclude references to source you use for you analysis, which could be academic or policy papers, blogs, information from company web site or news stories.\n\n</rubric>\nThe assignment content will be provided by user (not in system prompt)\nDo not say anything else except the marking, following the format of:\n{\n    \"feedback\": \"feedback content\",\n    \"(rubric part 1) / (rubric part 1 weight)\": score,\n    \"(rubric part 2) / (rubric part 2 weight)\": score,\n    ......other rubric parts\n    \"total score\": weighted average score of all parts\n}"
    },
    {
      "role": "user",
      "content": "Investigating and Analysing Ethical Risks of a Digital Engagement Application\n\nDescription of Application\nSnapchat is an American messaging app that allows users to exchange photos and videos. This application originally was developed by Snap Inc., which is a technological company founded in 2011 by Evan Spiegel, Bobby Murphy, and Reggie Brown. At a time when the app was launched, it provided a new way of how people communicate online because whereas most messaging apps focused on text messages that users share with each other, Snapchat shifted its attention to photo and video communication. Users of the app can send and receive photos and videos for a particular amount of time which usually does not exceed 10 to 15 seconds (Velten et al., 2017). Whenever the timer is up, the content is permanently deleted from the chat. What is prominent is that each user is able to set the timer for a message right before it is sent and even receive a notification if the screenshot is made. Because of this feature, Snapchat was able to establish itself in the digital space during a time when privacy concerns were growing. It helped to reduce the risk involved in gathering and storing personal data online that can be then exploited by third parties against users.\nSnapchat is mostly popular among children and teenagers who tend to quickly share photos and videos in large amounts and forget about what was sent right after. Moreover, the timer became a valuable privacy feature for those young people who want to keep their chats private from parents, friends or teachers. Older users too, in fact, respect the application's safety features and it is one of the most popular messengers in the world today. Furthermore, those who wish to preserve specific chat messages are able to do so as well so that not all photos, videos or text messages have to disappear for ever.\nIn addition to chats with friends, Snapchat offers a lot more features such as an opportunity to post \u2018My Story\u2019, play mini-games, watch entertaining videos and news, and use different photo and video editing tools and effects, and many more (Elgersma, 2018). Moreover, for many years an extremely popular element among users has been the ability to apply \u2018Face Masks\u2019 while taking photos or videos. These masks transform users\u2019 faces and people can choose the one they like among hundreds of available. However, recently Snapchat also added an AI Chatbot called \u2018My AI\u2019 to its application opening a new set of opportunities for its users. As Snap Inc. describes it: \u201cIn a chat conversation, My AI can answer a burning trivia question, offer advice on the perfect gift for your BFF\u2019s birthday, help plan a hiking trip for a long weekend, or suggest what to make for dinner.\u201d (Snapchat.com). In general, this chatbot is a simplified version of ChatGPT integrated into the app and listed as one of the user\u2019s friends in the chat list. The chatbot may be given a name, be added to groups, and have its avatar's look changed by users. Moreover, a person can share images in the chat as well. The bot replies to messages rather than contacting users first. Because My AI captures users\u2019 data such as location and basic information like the device used, it offers personalized solutions and enhances engagement. This chatbot attracts people not by only answering questions and offering features that other AI chatbots do but also by recommending places to visit near the user specifically chosen according to the enquire, providing directions to a place and identifying animals, plants, objects through photos that can be sent into the chat. As a result, Snapchat becomes a multipurpose application and increases its capabilities leading users to spend more time within it and retain more frequently.\n\nIdentification of Stakeholder Roles Involved in the Application and its Governance\nAI Provider\nThe chatbot integrated into Snapchat is powered by OpenAI and is based on the latest version of ChatGPT. However, Snap Inc. modified the initial version and trained AI to make it more suitable for the Snapchat application. For example, it uses AR and visual processing in addition to being a chatbot and playing games with users. Although, the main difference between ChatGPT and My AI is that it has more restrictions and safety measures integrated. It can be seen that My AI is trained specifically in a way that aligns with the company\u2019s safety and privacy guidelines. Any swearing, violence, sexually explicit content, or opinions about dicey topics like politics are restricted in My AI, it also politely declines to help with academic essays (Heath, 2023).\n\nDirect Data Providers\nThe primary categories of people using Snapchat are people from the age groups of 18-24 and 13-17 (Statista, 2023). Those are children, teenagers and young adults who together constitute the target audience of the application. People from these groups most value the ability to preserve messages, photos and videos only for a limited time. At the same time, as the main users, these groups are the primary providers of data utilised by the application and AI integrated within it. Gathered information helps to increase performance, whereas gained statistics allow to learn more about clients\u2019 needs and enhance the quality of time spent by users in the application by offering a personalised experience.\u00a0\n\nIndirect Data Providers\nHowever, direct users of Snapchat are not the only category of people whose data can be captured by the provider. It can be seen that whenever children or teenagers use the application to record live moments, take photos or videos with funny face masks, post them in \u2018My Story\u2019 or just save in the app memory, people who are also captured on them on purpose or accidentally, are automatically affected. For example, the personal information of friends and family members of users can appear in the application with people even not being acknowledged. All information can be saved in the chat with My AI which, in turn, saves users\u2019 data even after the content stops being displayed in the chat. Another possible group of stakeholders who can be influenced by the use of Snapchat is teachers at schools. It is common for teenagers to make jokes about classmates and school staff. For this reason, teachers\u2019 photos can be shared and stored in the app, as well as in the AI chatbot.\nIt is apparent that even when Snap Inc. cannot fully control data being sent to the application, it is responsible for ensuring its protection. Snapchat has a policy of being transparent in the information it captures and how it is used but it still shares some data with third-party applications and its partners. The main strategy of Snapchat to increase data protection is to store the information for as little time as possible. Initially, it was a competitive advantage of the application that differentiated it from other messengers. Moreover, Snap Inc. accepts its social responsibility and understands that its competitive advantage and market share are strongly connected to the extent to which users feel that they can trust an application. Whenever the data is not protected enough, it becomes a potential target for hackers and other people with illegitimate motives, information leaks put users in danger and break the trust.\u00a0\n\nOversight Stakeholders\nWhereas users play an important role in Snapchat\u2019s decisions, the application\u2019s AI is also a subject of scrutiny by regulatory bodies around the world. In addition to the detailed privacy policy and Snapchat\u2019s guidelines on maintaining security within AI and the application itself, the application follows data protection regulations which are specific for different regions where it operates. For example, in Europe, Snapchat is accountable to the European Data Protection Board and has to remain in compliance with the Digital Services Act (DSA), in Canada this role is played by AI and Data Act (AIDA) and supervised by the Ministry of Innovation, Science and Economic Development and Office of the Privacy Commissioner of Canada (Global AI Regulations Tracker: Europe, Americas & Asia-Pacific Overview, n.d.). Nowadays, AI regulation is still a new area where different ways to protect people are still developing. Whereas some countries are already working on the adoption of security measures, others only starting to plan possible legislation.\u00a0\n\nIdentification of Ethical Risks\nAccording to statistics from Statista.com, Snapchat was able to achieve more than 414 million daily users worldwide in the third quarter of 2023 (Statista, 2024). These incredible results point out that an application created by Snap Inc. is one of the most popular in the world. As a result, once the programme is used so widely, further concerns about the morality of the chatbot and messaging AI arise, which may undermine the AI Social Responsibility Principles. When some of the risks associated with the Social Responsibility of AI are only potential and could affect stakeholders only in theory, others are more likely to occur and people need to keep them in mind while using My AI.\n\nHuman Rights\nFirst, and most dangerous of the consequences of AI is the violation of human rights. Inaccurate and biased information from My AI can have an impact on the dignity, social, cultural and even economic rights of a person. Even with additional training from Snap Inc. the chatbot, as it was trained on a large amount of data from various sources, can be biased, provide discriminative information, or affect the well-being of stakeholders. Specifically, in My AI the used LLMS is reduced in order to diminish the likelihood of such responses. However, it still can occur as AI is far from perfect and the technology is only in the evolving stage. Such offensive data can influence any users of My AI but the younger audience is more sensitive and likely to believe in the content provided. Older users, at the same time, can keep in mind those risks and critically evaluate gained information. Moreover, the danger of AI in this context is also based on the fact that My AI appears as a friend in the user\u2019s chat list and is created in a manner that gullible teenagers can forget that it is not their friend but only a technology when all conversations are designed in a very human-like manner. This may have an impact on the well-being and mental integrity of a user when parents of the children affected could not even know that their kid treats AI as a real friend. Moreover, the implications of this risk are high, imperfect information can affect people\u2019s perception of the world and can lead to unpredictable outcomes. It can be seen that the situations with parents worrying about their children after Snapchat launched My AI have been real from the first day (Kelly, 2023). However, in my opinion, the likelihood of violation of human rights on a such high level is under question because usually the conversations with chatbots are short and simple, and the scale of influencing information is more likely to be minimised.\u00a0\n\nConsumer Issues\nSecond, My AI can play a role in consumer issues when personal data of users gained during their time in the app, is misused by developers or exploited by third-party. My AI tracks users\u2019 locations, saves data provided with it and utilises it in the future to enhance performance and user experience. However, location and other personal data are private information that in case of exploitation, can lead to issues with stakeholder security. Whereas the risk of leaking the user's location at one point in time could look harmless, information gained by Snapchat together with proper use can tell a lot about the person, and where they live. For example, My AI tracks location in a background mode, it stores users\u2019 responses about preferable places and hobbies, and it can even have a photo of the user if this person sent his face to the chat. Similarly, this risk increases when the users are a younger generation as they have a lower level of privacy concerns. Despite Snapchat employing various techniques to secure confidential data and prevent leaks, any user or stakeholder who uses the app is at certain risk of having information about them compromised if the leakage happens. This also includes friends, family, coworkers, teachers, and other contacts of those whose information is misused. Not to add that the danger is really serious when it occurs. Although there have not been any occurrences in the past few years lowering the likelihood of the risk, Snapchat has a history of several data breach incidents. For instance, in 2017 more than 200,000 photos of users leaked because the partner photo storage was hacked. Another prominent example of unauthorised usage of user data is when in May 2019, it occurred that Snapchat employees were spying on users and had access to their personal information and contact details (Reed, 2021). As a result, this is a real issue with dangerous consequences and it has to be carefully guided.\n\nLabour practices\nIt can be seen that nowadays AI technology influences how work is done and its role becomes more important every year. Different AI-based improvements help workers to simplify some tasks, save time and become more productive. These technologies can be used across all parts of the business in different departments but there is also a risk associated with companies\u2019 transition to use more of artificial intelligence. One of the potential threats of AI integrated into Snapchat could be a decrease in employee demand because of labour displacement. In spite of the fact that My AI does not directly affect employees at first glance as it is an additional feature rather than a substitute for existing operations, workers can be affected by the application of AI in internal business structure and workflow. For example, automatisation of some functions within Snap Inc. can diminish the need for low-skilled workers or the number of employees needed in general can be reduced. In this case, if a company does not spend some resources on retraining or restructuring the workforce, a part of the workers can appear to be fired which negatively affects their lives and the lives of their families. Another case can be when AI technologies used for recruiting new employees, discriminate against specific groups of people (Discrimination and Bias in AI Recruitment: A Case Study, n.d.). Nevertheless, the impact of AI on employees in Snap Inc. is hard to determine and the danger of My AI for employees is more likely to be low as well as the likelihood of its occurrence. The reason for this belief is that there were no incidents in Snap Inc. regarding job losses or discrimination caused by AI but we still need to carefully research and control this issue because it has severe consequences in case of occurrence.\n\nThe Environment\nOne of the major reasons why artificial intelligence can affect the environment is its water usage. In general, significant water consumption is needed both directly and indirectly for AI systems to operate. The water footprint, the amount of water used in operation, has a significant impact on the environment. \u201cDirect water use includes the water used in cooling systems of data centres and the production of microchips used in AI models. Indirect water use includes the water used in the production of electricity used to power the data centres that run AI models\u201d (View of the Environmental Impact of AI: A Case Study of Water Consumption by Chat GPT, n.d.). Despite that water usage in the AI industry is relatively smaller than in other industries, it still has an impact on the world, especially in places where water is hard to find. For Snapchat specifically, it can be estimated that the water footprint is not inferior in volume to other AI systems because, as mentioned above, Snap Inc. provides its chatbot based on ChatGPT from OpenAI. As a result, we can approximate the environmental impact of My AI as an impact that ChatGPT has on the planet complemented by resources needed for additional training and operation. Therefore, even when the impact on the environment is lower than, for instance, in agriculture, it is still severe and realistic and the actions against it have to be made.\n\nFair Operating Procedures\nIn general, algorithms drive My AI's operation, dictating its reactions and behaviours. Therefore, ethical risks may occur if the procedures used are not transparent to stakeholders. Lack of transparency can lead to biased responses and practices, as well as unfair treatment of Snapchat users. When AI algorithms are biased, My AI responses can be used to marginalise certain social groups or, for example, manipulate the public political process in a hidden way, using a friendly conversation manner. This can directly affect users\u2019 point of view, which can be later translated to their friends, family and people whom they interact with. However, I believe that the risk of this happening in the case of My AI particularly is insignificant and the likelihood is low. The reason for such a claim is that Snapchat\u2019s AI is designed in a way that limits any discussions concerning political matters or other sensitive topics. When the user writes questions including these themes, My AI refuses to answer and to continue the conversation about it.\n\nCommunity Involvement and Development\nWhereas the chatbot created by Snapchat, as mentioned, can create ethical risks for its users and people close to them by undermining personal data privacy, causing threats to well-being, providing misleading information and so on, its practices can also affect whole different communities in some cases. Some of the unique features of My AI that are designed to be its main competitive advantage, can also bring damage to society more broadly. For instance, Snapchat\u2019s chatbot is popular for its ability to suggest users places to visit and recommend different activities. The user just needs to put their enquiry in the form of a message to the chat and My AI will provide recommendations and even show them on the map with the designed route. The application itself collaborates with external platforms that provide data on different places to the chatbot. However, there is a potential problem deriving from such recommendations. It is possible that some small local businesses, cafes, tiny restaurants and other not-very-well-known places can be affected by this technology. Suggesting users only places that have a lot of feedback and information online, will prevent those people from visiting local places, the chance of accidentally finding a great location diminishes when the route is automatically provided to you and places are chosen without your active participation. Though the chances of such events are not excessive because local places can still be offered, and the effects could not be seen to be catastrophic for society as a whole because not everyone uses this feature, some individual business owners can still experience damage.\n\nDiscussion of Mitigation Measures for Risk\nPersonally, I believe that the influence of My AI chatbot on children constitutes the highest ethical risk of all listed above because includes the most vulnerable part of society. In situations when the younger generation using technology is not able to separate it from real friends, it puts human rights in danger. My AI is created in a way that users can give this chatbot a name, change its avatar and customise the chat. Moreover, it is displayed as just one of the dialogues in a chat list next to the conversations with the user\u2019s friends. For all these reasons that make My AI unique in the first place, together with simple friend-like immediate responses, the risk that some people would treat it more like another human being rises exponentially. As a result, the well-being of those who are attached to artificial intelligence can suffer. For instance, children can ask for a piece of personal advice or mental help but the chatbot does not have any empathy as well as specifically trained knowledge. Not only this but children\u2019s mental integrity and health can be at risk if other types of information provided by My AI contain inappropriate or ethically unacceptable responses. Therefore it is important to define possible measures that the developer and the broader community can use to decrease this risk.\u00a0\n\nThe Developer\nFirst, Snapchat can undertake several actions itself. It is crucial to ensure that Snap Inc. acknowledges this issue and acts towards its mitigation. Snapchat can re-design My AI with the purpose of making it look less like real users. The customisation features available to children can be reduced or more signs pointing out that it is just a technology can be added. Moreover, guidelines with important information regarding the safe use of the feature can be shown before every conversation or reminders in the format of push-up notifications can be introduced. It should be clearly communicated to children that My AI is an artificial intelligence rather than a real person. Not only this, but Snapchat can establish an additional age boundary for the My AI feature that not all users (who can be any person 13 and more years old) can interact with it but the ones who are older that certain safer age.\n\nExternal Stakeholders\nSecond, the broader society and people close to potential victims can raise awareness. In the era of AI children\u2019s education on the safe use of the technology and ways to protect themselves should become a priority. Schools should create lessons concerning it, parents should communicate the risks to their children and control the use of the technology if the child is too young, and entertainment platforms should provide more material about it. All of those initiatives must have as their primary goal informing young people about the dangers associated with artificial intelligence and raising their level of attention to their safety, personal information, and general well-being. Not only this but together with the actions taken directly by technology providers, it would decrease the risk of young people\u2019s human rights becoming affected.\n\nLegislation Bodies\nLastly, regulators should not stay away from this issue. Some of the additional legislation measures that can be implemented to reduce the ethical risk of children being affected can include specific certifications and licences that companies, whose target audience include young people, should receive before the technology goes public. Those certifications can include additional regulations on how AI has to be designed and the scope of answers it can provide. Moreover, legislation bodies can collaborate with technological organisations, educational institutions and organisations protecting child rights to ensure better ethical control and address challenges concerning children\u2019s privacy. Together, those parties can develop safer methods and design innovative ways to deal with the issues. Moreover, legislators can create educational forums that would help schools, parents and other organisations to better educate children and design awareness programs.\u00a0\n\nConclusion\nTo conclude, My AI powered by Snapchat is a powerful tool enabling users to enhance their experience and gain information, tips and advice on various topics. It can help people to find interesting places, get ideas on recipes, what to buy as a present and many more. However, AI technology is currently in a developing stage and there are still a lot of risks and challenges that society has to address. My AI when increasing the time spent by users on Snapchat, can cause harm to users and other stakeholders. One of the main threats is concerns with young people\u2019s human rights who are the main target audience of the application. Therefore, Snap Inc., external stakeholders and regulators have to carefully manage the technology and continuously develop ways to reduce any potential dangers arising from its use. \n\nReferences:\nSnapchat Support Centre\nhttps://help.snapchat.com/hc/en-us/articles/13266788358932-What-is-My-AI-on-Snapchat-and-how-do-I-use-it\n\nVelten, J. C., Arif, R., & Moehring, D. (2017). Managing Disclosure through Social Media: How Snapchat is Shaking Boundaries of Privacy Perceptions. The Journal of Social Media in Society, 6(1), 220\u2013250. https://thejsms.org/index.php/JSMS/article/view/214/116\n\nElgersma, C. (2018, June 18). Everything you need to know about Snapchat. Phys.org; Phys.org. https://phys.org/news/2018-06-snapchat.html\n\nArtificial intelligence in social media / What do parents need to know about Snapchat\u2019s new chatbot My AI? (2023, September 1). Klicksafe.de. https://www.klicksafe.de/en/news/was-muessen-eltern-zum-neuen-chatbot-my-ai-bei-snapchat-wissen\n\nStatista. (2024). Snapchat daily active users 2024 | Statista. Statista; Statista. https://www.statista.com/statistics/545967/snapchat-app-dau/\n\nStatista. (2023). Average age of Snapchat users 2023 | Statista. Statista; Statista. https://www.statista.com/statistics/933948/snapchat-global-user-age-distribution/\n\nGlobal AI Regulations Tracker: Europe, Americas & Asia-Pacific Overview. (n.d.). Legalnodes.com. https://legalnodes.com/article/global-ai-regulations-tracker\n\n\u200cReed, C. (2021, December 8). Snapchat Data Breaches: Full Timeline Through 2021. Firewall Times. https://firewalltimes.com/snapchat-data-breach-timeline/\n\nView of The Environmental Impact of AI: A Case Study of Water Consumption by Chat GPT. (n.d.). Puiij.com. https://puiij.com/index.php/research/article/view/39/23\n\nKelly, S. M. (2023, April 27). Snapchat\u2019s new AI chatbot is already raising alarms among teens and parents | CNN Business. CNN. https://edition.cnn.com/2023/04/27/tech/snapchat-my-ai-concerns-wellness/index.html\n\nHeath, A. (2023, February 27). Snapchat is releasing its own AI chatbot powered by ChatGPT. The Verge. https://www.theverge.com/2023/2/27/23614959/snapchat-my-ai-chatbot-chatgpt-openai-plus-subscription\n\nDiscrimination and bias in AI recruitment: a case study. (n.d.). Lewis Silkin. https://www.lewissilkin.com/en/insights/discrimination-and-bias-in-ai-recruitment-a-case-study#:~:text=An%20algorithm%20can%20be%20tainted"
    },
    {
      "role": "assistant",
      "content": "{\n    \"feedback\": \"A strong, well structured analysis of Snapchat's myAI. Well selected focus on mitigation of impact on children given the profile of the snapchat user base. The new AI Act required AI agent to clear identify that they are artificial and there are special prohibitions for protection of children. \",\n    \"description / 20\": 70,\n    \"roles / 10\": 70,\n    \"risks / 40\": 75,\n    \"mitigation / 20\": 72,\n    \"refs / 10\": 70,\n    \"total score\": 72.4\n}"
    }
  ],
  [
    {
      "role": "system",
      "content": "You are a professional educator. \nYour target is to assess students' assignments based on the provided rubric (between <rubric> and <rubric> labels).\n<rubric>\nAssignment: Investigating and Analysing Ethical Risks of a Digital Engagement application \n\nThis is an INDIVIDUAL assignment on Ethical Risk Assessment for the TEU00062 module. This assessment is worth 30% of the total TEU 00062 module mark. The deadline for submission is 12.00. noon Friday 31st March 2024. \n \nLearning outcomes: Upon completion of this assignment students should:\nBe able to identify the different stakeholders and organisational roles involved in developing and using AI-driven digital engagement application, and motivations. \nBe able to identify and analyse ethical risks in a digital engagement application.\nBe equipped to identify technical and governance mitigations of these risks\n \nSteps:\n \nSelect ONE real world application that embodies one or more AI-driven digital engagement techniques you have learnt about on this course, e.g. e.g. digital engagement techniques used in one applications such as social media, newsfeeds, search, eCommerce, elearning or digital humanities., or in an application you are familiar with e.g. interactive Game, Virtual Presence (i.e. VR /AR) etc. \nIdentify the different value chain stakeholder roles involved in this application scenario and where possible the actors (i.e. companies, organisations) carrying out those roles. including those involved in: data collection; AI model development; development and use of the application. \nUse social responsibility principles (see ethic lecture week 2) to identify and analyse real or potential ethical risks of the application, including classification of the affected stakeholders (hint: see also risks classified by Latzer et al 2016 \u2013 ethics lecture week 1), how severe the risk would be and how likely it is to occur.\nIdentify one or more mitigation measures that specific value-chain stakeholders may undertake to minimise the impact of the risk you think is most harmful. These could be technical measures, organisational governance measures or measure involving additional engagement with affected individuals or groups.\n \nStructure of Assignment Report. \nStructure the Report (4000 word max excluding figures and references), under the following heading [indicative percentage of assignment mark per section]. Please provide references\n \nDescription of Application [20%]\n<describe your selected application, explaining what it does, the role AI and data collection plays in its digital engagement features, and the benefits it provides>\n\nIdentification of Stakeholder Roles involved in the application and its governance [10%]\n<describe the stakeholder roles undertaken in the development, use and governance of the application, and which organization take those roles and the benefits they derive from their involvement. Include roles that may be indirectly affected by the application. >\n\nIdentification of Ethical Risks [40%]\n<Identify the ethical risks that may be raised by your application under the social responsibility categories below (more details in ethics lecture week 2). For each risk identify which stakeholder may be affected, and assess how severe the impact would be for each stakeholder (Low, Medium, High) and the likelihood of that risk being incurred (Low, Medium, High), justifying your assessment, even if you think there is no risk under that heading.>\nHuman Rights\nLabour Practices\nThe Environment\nFair Operating Procedures\nConsumer Issues\nCommunity Involvement and Development\n \nDiscussion of Mitigations Measures for Risk [20%]\n< For one of the risks you consider more severe, describe mitigation measures that could be taken. Explain how the measure would reduce or eliminate the likelihood and impact of the risk. Explain which stakeholders would be involved, or would have to interact in implementing the measure, identifying if this is a governance measure an organization can undertake itself or in collaboration with other parties, e.g. regulators or external stakeholder..>\n\nReferences [10%]\nInclude references to source you use for you analysis, which could be academic or policy papers, blogs, information from company web site or news stories.\n\n</rubric>\nThe assignment content will be provided by user (not in system prompt)\nDo not say anything else except the marking, following the format of:\n{\n    \"feedback\": \"feedback content\",\n    \"(rubric part 1) / (rubric part 1 weight)\": score,\n    \"(rubric part 2) / (rubric part 2 weight)\": score,\n    ......other rubric parts\n    \"total score\": weighted average score of all parts\n}"
    },
    {
      "role": "user",
      "content": "Assignment: Investigating and Analysing Ethical Risks of a Digital Engagement application\nSection 1: Description of application\nTikTok is a social media platform with over 4.5 billion downloads that allows users to create, share and view a variety of short videos, ranging from entertainment and sports news to advice (D\u2019Souza, 2023). The application began as a service where users would share videos of themselves dancing and lip-syncing to music, but it has since branched out to where users can find anything from information on current affairs, individuals advertising their brick-and-mortar businesses and even politicians outlining their policies and stances on contentious topics. It is known for its addictive quality and high engagement and utilises an Ai recommendation algorithm to achieve these qualities. TikTok uses a self-training Ai engine to recommend videos to users to maximise engagement and these recommendations only become more accurate over time. This Ai engine collects information such as the user\u2019s name, age, location, activity on other websites accessed through TikTok and cookies that collect, measure, and analyse which web pages users view most often and how they interact with them (La Jeunesse, 2023). The recommendation system then selects videos from a collection of eligible content and ranks them on how likely the user is to be interested in them (La Jeunesse, 2023). These ranked videos then appear on the user\u2019s unique \u201cFor you page\u201d. According to TikTok themselves, the videos that appear on a user\u2019s \u201cFor you page\u201d are influenced by three factors: user interactions, video information and user information. User interaction covers topics such as videos the user likes, shares, or comments on, videos the user watches in full or skips and accounts the user follows or is followed by. Video information deals with sounds, hashtags, video views and country of origin. User information considers device and account settings, language preference, user\u2019s country of origin and time zone and device type. A large weight is placed on the user interaction characteristic of watch time. The Ai recommendation algorithm uses initial demographic information to find videos that may potentially interest the user and over time refines the topics down, through self-learning, depending on what types of videos the user watches in full or skips (support.tiktok.com, n.d.)\n\nTikTok\u2019s Ai recommendation algorithm offers benefits to a multitude of parties. Firstly, users benefit as they receive a service that is more likely to keep them engaged. Due to the self-training engine the longer users spend on the service the more engaging the videos are to them. This provides an entertainment factor that users benefit from. Secondly, the corporation itself benefits from the recommendation algorithm keeping users engaged. This results in users spending large quantities of time on the application and available to be subject to advertisements. This is especially important when you consider TikTok\u2019s advertising revenue was $13.2Bn in 2023 alone (Mosby, 2024). The corporation receives large benefits from its Ai recommendation algorithm as it allows them to maintain this ad revenue by keeping users engaged and even allows for the increase in revenue over time due to an increase in user engagement as the recommendations become more accurate, meaning users may spend longer on the application. Thirdly, the information gathered by TikTok\u2019s Ai recommendation algorithm greatly benefits advertisers. The Ai model collects information on user demographics and goes into fine details as mentioned previously, e.g., user\u2019s device and location. This allows advertisers to target their ads to a specific demographic, making their ads more effective while also preventing waste due to the advertiser being able to target solely their target demographic. Finally, the creators themselves benefit from TikTok\u2019s Ai recommendation algorithm. The Ai system helps deliver the videos that creators have shared to the users that are most likely to be engaged with them. This acts as an efficient system for creators to be matched with their target market, leading to views on videos and followers for the creator. Creators benefit from this due to the Creator Fund operated by TikTok, acting as a compensation method for creator\u2019s work. The Creator Fund pays out values ranging from $0.02 - $0.04 per 1000 views for smaller creators and $0.50 - $1.00 for larger creators (McLellan, 2023). As the recommendation algorithm, pushes videos out to the users most likely to be engaged with them, this allows creators to maximise viewership which in turn maximises revenue. \n\nSection 2: Identification of shareholder roles involved in the application and its governance\nTikTok as an application has many stakeholders that are involved in its development, use and governance. TikTok, itself, is the Ai developer, an Ai user, and an oversight authority. TikTok\u2019s Ai recommendation algorithm was developed by its engineering team in partnership with parent company Byte Dance. TikTok benefits from developing its own Ai system as there is no recurring licencing fee for usage and it is tailored specifically to the application. Furthermore, TikTok is an Ai user as it utilises its Ai recommendation algorithm as the base for the successful functioning of the application. Without its Ai system, TikTok would fail to maintain the same level of user engagement which is so crucial for its advertising revenue. In addition to these roles, TikTok must operate as an oversight authority through self-regulation of some aspects. In the EU alone, TikTok must self-regulate to ensure that they are operating in line with EU laws such as GDPR and the upcoming AI act. Failing to self-regulate would lead to penalties for TikTok and the potential inability to access the EU market.\n\nThe government, both national and EU, act as oversight authorities for applications like TikTok. These authorities enforce data protection laws and Ai regulations to safeguard their constituents. Branches of government concerned with regulation of applications like TikTok include the European Commission, European Data Protection Board, National supervisory authorities e.g., Irish Data Protection Authority, Courts, and Data Protection Officers. Examples of regulation can be seen through a European Commission code of conduct that requires major tech platforms to report misinformation on their networks and their methods of combatting it (O\u2019Leary, 2023.), GDPR which concerns the processing of personal data and where violators can be fined \u20ac20 million or 4% of worldwide turnover and the proposed AI act which operates as a risk-based system prohibiting some forms of AI, making other forms permissible once in compliance with strict regulations ,e.g., notifying individuals they are interacting with AI or labelling deepfakes, and offers large penalties of \u20ac30 million or 6% of global turnover. \n\nUsers of TikTok operate as data providers as by using the application they agree to allow TikTok\u2019s AI recommendation algorithm collect the previously mentioned information on them. This benefits the user by increasing the likelihood that they will view videos that they find engaging based on their physical attributes or interests. Furthermore, this benefits TikTok as they are provided with the necessary data that is required for the effective functioning of their AI system. \n\nAdvertisers are stakeholders of TikTok that act as AI info users. The information that TikTok\u2019s AI recommendation system gathers separates individuals into demographics based on physical attributes and interests. Advertisers can then make use of this information to identify who they want to target based on their physical attributes e.g., location, or based on their interests. This allows advertisers to effectively run advertising campaigns and prevent waste as they only advertise specifically to their target market.\n\nAn often-overlooked stakeholder comes in the form of family, friends, and colleagues of users. These parties are stakeholders as they act as \u201caffected stakeholders\u201d even if they themselves are not users of TikTok. This is due to the sharing ability on TikTok that allows users to send videos to non-users on platforms such as Snapchat or WhatsApp. This allows non-users to benefit from viewing engaging videos they may not have been able to see before but also opens them up to potential ethical risks that they were separated from previously. \n\nSection 3: Identification of ethical risks \nThere are a variety of ethical risks arising from TikTok as an application that are exacerbated by the platform\u2019s AI recommendation algorithm. To effectively identify these ethical risks, it is necessary to examine them under the social responsibility categories of human rights, labour practices, the environment, fair operating procedures, consumer issues and community involvement and development.\n\nHuman Rights\nSeveral ethical risks fall under the category of human rights. These risks pertain to civil and political rights and physical risks. The primary stakeholders faced with these risks are users and family, friends, and colleagues of users. Firstly, civil and political rights are subject to ethical risks due to TikTok and its AI recommendation algorithm. TikTok\u2019s recommendation algorithm analyses videos to understand what users would be most engaged with the content but fails to analyse whether the videos are factual or not. This creates the potential for manipulation and distortion of reality to take place as users are placed in an echo chamber full of misinformation. Furthermore, this affects user\u2019s family, friends and colleagues that receive these videos from the user and fall victim to manipulation and distortion of reality. The probability of this risk is high, and the severity of this risk is also high. The high probability can be seen through examples of fake political ads appearing often on TikTok as the AI recommendation system pushes these videos to those who will be engaged by the topic without considering whether the information is factual. For example, a watchdog in the US submitted fake political ads to TikTok and 90% of these ads made it to the general public (Ortutay, 2022). The severity of this risk is high due to the potential harms resulting from misinformation and distortion of reality. The outcomes range from fake smear campaigns that cause a political figures support to fall to more serious risks, such as riots as seen in the January 6th attacks on the US capitol building due to misinformation spread about the validity of the US presidential election. Thus, the severity is high due to the potential for electoral interference and even violence.\n\nFurthermore, physical risks arise as a result of TikTok and its AI recommendation algorithm. In a similar manner to how TikTok\u2019s AI recommendation algorithm analyses political videos, it fails to identify whether videos relating to physical and mental health are promoting factual information or not. This affects users and their family, friends, and colleagues due to the ability to share videos. The algorithm will simply push these videos to those who it believes will be engaged by the video and fails to consider whether the advice in the videos they are recommending is harmful or not. This creates the potential for individuals, especially at-risk individuals, to be fed misinformation about their physical and mental health and to be pushed towards not seeking medical advice or even self-harm. The probability of this risk is high, and the severity is equally high. There are countless examples of TikTok promoting harmful content related to physical and mental health. For example, technical research carried out by Algorithmic Transparency Institute and AI Forensics uncovered that between 3 and 20 minutes into their manual research that more than half the videos in their \u201cFor you page\u201d were related to mental health struggles, with multiple videos romanticising and even encouraging suicide (Amnesty International, 2023). These harrowing statistics highlight the high probability of this risk. Furthermore, the probability is increased as the videos are not only limited to users but also their family, friends, and colleagues, who may not even have the application, due to the ability to share videos. Needless to say, the severity is high on this risk. TikTok\u2019s AI recommendation algorithm is recommending content that promotes suicide and self-harm, potentially leading to deaths. The severity only increases when you consider the fact that majority of users on TikTok are teenagers, an age group that are quite impressionable and could be influenced to take harmful action due to a constant stream of these types of videos.\n\nLabour Practices\nThere exists a potential ethical risk for the corporation in social responsibility area of labour practices. This risk relates to worker dissatisfaction and worker mental health and arises due to the effects of TikTok\u2019s AI recommendation algorithm. There exists the potential that employees will become dissatisfied with their work and may even suffer with their mental health declining because of the harms, to civil and political rights and to physical health of user, being perpetuated by their employer through its AI recommendation algorithm. Employees want to work in a corporation that does not cause harms to the public and by TikTok failing to satisfy this, the organisation will suffer from dissatisfaction and declining mental health. The severity of this risk is medium; however, the probability is quite high. The high probability arises due to 76% of employees wanting to work in companies that are socially responsible (Cox, 2023). With over \u00be of employees concerned with working in a socially responsible business, the risk of dissatisfaction and mental health decline are quite high. The severity of this risk is medium, in comparison to the high risk under the human rights category, due to the potential for political interference and harm to or death of individuals. This severity is highlighted by a study stating that employees who report their employer being socially responsible, on average have better mental health (Allen, 2021). TikTok, due to acting socially irresponsibly, may cause its employees to become dissatisfied with their work and have deteriorating mental health, impacting the functioning of the corporation and quality of life of the employees. Thus, the severity is placed at a medium level as while it does cause great harm it does not lead to loss of life as the risk under human rights does. \n\nThe Environment\nTikTok\u2019s AI recommendation algorithm leads to ethical risk for a multitude of stakeholders under the social responsibility category of the environment. Firstly, there is an environmental risk due to the operation and monitoring of this AI system. As it is a self-training AI system, it is constantly training itself and harvesting new information and there is increased pollution associated with this. This affects the corporation as it is responsible for all emissions because of its AI system. Furthermore, the government, both national and EU, is affected due to the necessary monitoring of AI systems, like TikTok\u2019s, to ensure that regulations are satisfactory in a field that sees constant development. This constant monitoring also leads to increased emissions for the government. The probability of this risk is high; however, severity is quite low but has potential to increase. Probability is high for both stakeholders due to the necessity of the operation of the AI system for the corporation and the monitoring for the government. It is inevitable that carbo emissions will result from these activities. The inevitable emissions of TikTok\u2019s complex AI system are seen as TikTok emits 3.3 times more carbon emissions per minute of use than comparable websites like Facebook (L, 2023). The severity of the environmental risk of AI operation, development and monitoring is relatively low since AI currently only makes up 1% of global carbon emissions (Xing and Monck, 2023). However, there is potential for this severity to increase as AI models become more prevalent and TikTok eventually must replace or update its AI system due to increased competition, further leading to more emissions in the monitoring on behalf of the government. The potential increase in severity can be seen as the training of a single AI model can emit an estimated 284 tonnes of Co2 (Xing and Monck, 2023). It is clear to see how the severity of TikTok\u2019s environmental impact can increase in future upon considering the emissions for a single AI model and how complex TikTok\u2019s AI recommendation algorithm is required to be.\n\nFair Operating Procedure\nTikTok\u2019s AI recommendation algorithm causes potential risks under the social responsibility category of fair operating procedure. This is due to the ability to game the algorithm to undermine the political process. This risk affects users and their family, friends and colleagues, as affected stakeholders. It is possible for influence operations to occur in order to undermine the political process. Groups can excessively lobby, push out misinformation or even manipulative information to cause social divide and sway votes.  Furthermore, deepfakes can be used to deceive individuals into thinking a political figure has made promises that in reality they have not. These influence operations are possible due to TikTok\u2019s AI system recommending videos to users based on perceived engagement without considering the validity of these videos. This affects users as they are recommended these manipulative videos on their \u201cFor you page\u201d but also affects family, friends, and colleagues of users as they may share the videos to them believing them to be accurate. The probability of this risk is high, and the severity is equally high. The high probability of influence operations to undermine the fair political process can be seen with numerous examples in US politics but also on a local level, where TikTok recently dismantled a covert influence operation targeting the Irish public with divisive content intended to intensify social conflict. This influence operation was only dismantled after amassing 94,743 followers and 2,440,995 views (O\u2019Leary, 2023). The high severity relates to the potential for these influence operations to go undiscovered and to seriously undermine the fair political process by manipulating and providing misinformation to users and their peers. This severity is added to by the lack of fact-checking in TikTok\u2019s AI system and how quickly it recommends these videos to millions of users, allowing influence operations to flood TikTok with videos that are spread rapidly before any validity can be established. This may result in extremist candidates being elected or others losing support due to false claims.\n\nConsumer Issues\nTikTok\u2019s AI recommendation algorithm has created ethical risks under the social responsibility heading of consumer issues. These risks affect users primarily. The risks arise due to the ability to create videos on TikTok that promote a product or service but without any checks as to the validity of these claims. It is possible to create a video advertising a product full of fake claims and to include an external link to a website to buy this fake product. This is due to TikTok promoting videos to users the algorithm feels will be engaged by the video and users may feel safe to purchase the product as TikTok has recommended it to them by placing it on their \u201cFor you page\u201d. The probability of this risk is at a medium level and the severity is also at a medium level. The probability being a medium level is due to the main users of TikTok being children and teenagers who may not be able to identify whether an ad is legitimate or not, however it is expected that a reasonable adult would be able to distinguish this. Furthermore, countless examples of fake or misleading ads can be found on TikTok, with one report highlighting numerous fake mobile apps, diet pills and other fake products and services appearing on the app (Wong, 2020). The medium level severity arises because users are being scammed out of their money by purchasing products that are not as advertised but there is no threat of death or immense financial harm. The products are often lower cost in comparison to if a fake car or house was being sold. However, there remains a health risk as products are unlikely to have undergone safety tests and some cosmetic items may contain toxic ingredients leading to a medium severity rating of the risk.\n\nCommunity involvement and development\nTikTok\u2019s AI recommendation algorithm has created ethical risks for users and their family, friends and colleagues under the social responsibility heading of community involvement and development. This risk is due to the ability of the recommendation algorithm to spread misinformation and hateful views designed to tear communities apart. The algorithm pushes videos that it feels will garner engagement and fails to account for whether the information in a video is accurate or designed to spread hate. This poses a risk to community involvement and development as TikTok can perpetuate harmful views against community members causing a divide in the community. The probability of this risk is high, and the severity is equally high. The high probability can be seen in an ISD (Institute for Strategic Dialogue) report on anti-immigrant videos on TikTok highlighted how followers are not necessary for high view counts as of 20 accounts in the study, followers ranged from 15 to 10,000 but views ranged from 4,000 to 1,200,000 (ISD, 2023). Furthermore, the severity is high due to the possibility of divide within communities but also of violence due to hateful views and misinformation. An example being seen in a video of a migrant entering a hospital, with overlaid text accusing him of committing sexual crimes against children, parents, and staff, which led to the innocent man being identified and assaulted on the streets of Dublin (ISD, 2023). \n\nSection 4: Discussion of Mitigation Measures for Risk\nThe most poignant risk arising from TikTok\u2019s AI recommendation algorithm is responsible for many ethical risks under the social responsibility headings and is that of the recommendation algorithms inability to identify misinformation and the spread of hateful views within videos. This is a severe risk that affects a variety of areas from political rights, individual\u2019s mental and physical health, and harmony within communities. Therefore, this is the risk that I believe is the most important to address. \n\nMy proposed mitigation measure involves collaboration from the government, as a regulatory body, the corporation itself and users. Firstly, the government must outline high-risk categories that require additional screening before being able to be shared with the public. A move which would reduce the effectiveness of influence operations by preventing the rapid sharing of misinformation and harmful views. These categories would cover the areas that have the potential to have the largest negative impact. Potential categories to be outlined could be mental and physical health, news stories, politics and deepfakes. Additionally, the government would need to establish an effective regulatory system to encourage compliance. Primarily, this involves government departments tasked with testing the levels of misinformation or harmful views or advice on each social media platform or the ability to spread misinformation on the site, work already being done by watchdog NGOs. The government could establish potential fines greater than those currently in place to act as a greater deterrence against poor self-regulation from TikTok. Furthermore, a rewarding system may encourage greater and stricter compliance. Rewards could consist of tax incentives for those social media companies that prevent misinformation and harmful views most effectively.\n\nSecondly, the corporation plays a significant role once the high-risk categories are outlined. The mitigation measure involves the development of a similar AI model to that of the self-training recommendation system that would also scan the content of the videos but instead of using that information to recommend videos to users, it would flag these videos for additional checks to verify information or scan for potential harmful advice or views e.g., romanticising suicide or anti-migrant views. This helps mitigate the risk as all videos on high-risk topics undergo additional screening which prevents the rapid spread of misinformation and harmful views or advice. Furthermore, it is practical for the corporation as not every video would require additional screening but only those that are flagged as being high-risk by the AI model, allowing for the continuation of the recommendation algorithm while removing harmful videos at the source. I understand that the development of the screening AI model would be costly, but the hope is that the prevention of fines and the potential tax incentives outweigh this cost.\n\nFinally, the user is necessary for the effective functioning of the self-training AI detection model. The corporation needs to provide an effective user reporting system where the user can report any videos they feel may have slipped through the cracks. These videos would then undergo additional screening. Further preventing the spread of misinformation and harmful views or advice as any videos that slip by the rigorous pre-upload checks can be reported by users before amassing large view counts. These mitigation measures from the government, corporation and users would help prevent the risks that are currently seen under many social responsibility headings. \n\nReferences\nSection 1\nD\u2019souza, D. (2023).\u00a0TikTok: What It Is, How It Works, and Why It\u2019s Popular. [online] Investopedia. Available at: https://www.investopedia.com/what-is-tiktok-4588933. (Accessed: 29/02/2024)\n\nMosby, A. (2023).\u00a0TikTok Ad Revenue From 2020 To 2025 (Reports & Analysis). [online] Yaguara. Available at: https://www.yaguara.co/tiktok-ad-revenue/. (Accessed 29/02/2024)\n\nMcLellan, K. (2023).\u00a0How Much Does TikTok Pay You? (+Free TikTok Money Calculator). [online] The Leap. Available at: https://www.theleap.co/blog/how-much-does-tiktok-pay/. (Accessed 29/02/2024)\n\n\u200cLa Jeunesse, W. (2023).\u00a0Here\u2019s the data that TikTok collects on its users. [online] FOXBusiness. Available at: https://www.foxbusiness.com/technology/heres-data-tiktok-collects-its-users. (Accessed 01/03/2024)\n\nsupport.tiktok.com. (n.d.).\u00a0How TikTok recommends content | TikTok Help Centre. [online] Available at: https://support.tiktok.com/en/using-tiktok/exploring-videos/how-tiktok-recommends-content. (Accessed 01/03/2024)\nSection 2\nO\u2019Leary, N. \u201cTikTok Discovers \u201cCovert Influence Operation\u201d Targeting Ireland.\u201d The Irish Times, 28 Sept. 2024, www.irishtimes.com/technology/2023/09/28/tiktok-discovers-covert-influence-operation-targeting-ireland/. (Accessed 06/03/2024)\nLewis, D. (2024). \u2018Ethical Digital Engagement: Lecture 2\u2019 [PowerPoint Presentation] TEU00062\u00a0:Engaging in The Digital World: Today and Tomorrow (Accessed 06/03/2024)\n\nSection 3\nOrtutay, B. (2022). Report: TikTok bad at culling US election misinformation ads. [online] AP NEWS. Available at: https://apnews.com/article/technology-business-government-and-politics-misinformation-a18a35a2a2542eadfc3893983a818511. (Accessed 11/03/2024)\nAmnesty International (2023).\u00a0Global: TikTok\u2019s \u2018For You\u2019 feed risks pushing children and young people towards harmful mental health content. [online] Amnesty International. Available at: https://www.amnesty.org/en/latest/news/2023/11/tiktok-risks-pushing-children-towards-harmful-content/. (Accessed 11/03/2024)\nCox, J. (2023).\u00a0Despite Economic Headwinds, Employees Still Want To Work For Companies That Care. [online] Forbes. Available at: https://www.forbes.com/sites/josiecox/2023/02/15/despite-economic-headwinds-employees-still-want-to-work-for-companies-that-care/?sh=3db79e466d20. (Accessed 11/03/2024)\nAllen, P. (2021).\u00a0Does working for a sustainable company make you happier?\u00a0[online] Corporate Knights. Available at: https://www.corporateknights.com/health-and-lifestyle/does-working-for-a-sustainable-company-make-you-happier/. (Accessed 11/03/2024)\nL, J. (2023).\u00a0TikTok Dances toward Net Zero Emissions by 2030. [online] Carbon Credits. Available at: https://carboncredits.com/tiktok-dances-toward-net-zero-emissions-by-2030/. (Accessed 12/03/2024)\nXing, E. and Monck, A. (2023). AI emissions are fueling a new doomerism. This time it\u2019s climate change. [online] Fortune. Available at: https://fortune.com/2023/12/12/ai-emissions-doomerism-climate-change-environment-tech/. (Accessed 12/03/2024)\nO\u2019Leary, N. \u201cTikTok Discovers \u201cCovert Influence Operation\u201d Targeting Ireland.\u201d The Irish Times, 28 Sept. 2024, www.irishtimes.com/technology/2023/09/28/tiktok-discovers-covert-influence-operation-targeting-ireland/. (Accessed 14/03/2024)\nWong, Q. (2020).\u00a0TikTok ads have pushed scams about apps, diet pills, other products, report says. [online] CNET. Available at: https://www.cnet.com/news/privacy/tiktok-ads-used-to-push-scams-about-apps-diet-pills-and-other-products-report-says/. (Accessed 14/03/2024)\n\u200c\nISD. (2023).\u00a0TikTok videos of migrants being attacked or harassed trend in Ireland. [online] Available at: https://www.isdglobal.org/isd-in-the-news/tiktok-videos-of-migrants-being-attacked-or-harassed-trend-in-ireland/ (Accessed 18/03/2024)\n\u200c\n\n\n\n\n\n\u200c\n\n\n\u200c"
    },
    {
      "role": "assistant",
      "content": "{\n    \"feedback\": \"A clear, well structure and well researched report on tiktok. Note the EU Digital Service Act will be bringing in controls on content moderation and misinformation. Under labour rights, did you consider the impact on content moderator staff, doing what must be a harrowing job?\",\n    \"description / 20\": 67,\n    \"roles / 10\": 67,\n    \"risks / 40\": 67,\n    \"mitigation / 20\": 67,\n    \"refs / 10\": 67,\n    \"total score\": 67.0\n}"
    }
  ],
  [
    {
      "role": "system",
      "content": "You are a professional educator. \nYour target is to assess students' assignments based on the provided rubric (between <rubric> and <rubric> labels).\n<rubric>\nAssignment: Investigating and Analysing Ethical Risks of a Digital Engagement application \n\nThis is an INDIVIDUAL assignment on Ethical Risk Assessment for the TEU00062 module. This assessment is worth 30% of the total TEU 00062 module mark. The deadline for submission is 12.00. noon Friday 31st March 2024. \n \nLearning outcomes: Upon completion of this assignment students should:\nBe able to identify the different stakeholders and organisational roles involved in developing and using AI-driven digital engagement application, and motivations. \nBe able to identify and analyse ethical risks in a digital engagement application.\nBe equipped to identify technical and governance mitigations of these risks\n \nSteps:\n \nSelect ONE real world application that embodies one or more AI-driven digital engagement techniques you have learnt about on this course, e.g. e.g. digital engagement techniques used in one applications such as social media, newsfeeds, search, eCommerce, elearning or digital humanities., or in an application you are familiar with e.g. interactive Game, Virtual Presence (i.e. VR /AR) etc. \nIdentify the different value chain stakeholder roles involved in this application scenario and where possible the actors (i.e. companies, organisations) carrying out those roles. including those involved in: data collection; AI model development; development and use of the application. \nUse social responsibility principles (see ethic lecture week 2) to identify and analyse real or potential ethical risks of the application, including classification of the affected stakeholders (hint: see also risks classified by Latzer et al 2016 \u2013 ethics lecture week 1), how severe the risk would be and how likely it is to occur.\nIdentify one or more mitigation measures that specific value-chain stakeholders may undertake to minimise the impact of the risk you think is most harmful. These could be technical measures, organisational governance measures or measure involving additional engagement with affected individuals or groups.\n \nStructure of Assignment Report. \nStructure the Report (4000 word max excluding figures and references), under the following heading [indicative percentage of assignment mark per section]. Please provide references\n \nDescription of Application [20%]\n<describe your selected application, explaining what it does, the role AI and data collection plays in its digital engagement features, and the benefits it provides>\n\nIdentification of Stakeholder Roles involved in the application and its governance [10%]\n<describe the stakeholder roles undertaken in the development, use and governance of the application, and which organization take those roles and the benefits they derive from their involvement. Include roles that may be indirectly affected by the application. >\n\nIdentification of Ethical Risks [40%]\n<Identify the ethical risks that may be raised by your application under the social responsibility categories below (more details in ethics lecture week 2). For each risk identify which stakeholder may be affected, and assess how severe the impact would be for each stakeholder (Low, Medium, High) and the likelihood of that risk being incurred (Low, Medium, High), justifying your assessment, even if you think there is no risk under that heading.>\nHuman Rights\nLabour Practices\nThe Environment\nFair Operating Procedures\nConsumer Issues\nCommunity Involvement and Development\n \nDiscussion of Mitigations Measures for Risk [20%]\n< For one of the risks you consider more severe, describe mitigation measures that could be taken. Explain how the measure would reduce or eliminate the likelihood and impact of the risk. Explain which stakeholders would be involved, or would have to interact in implementing the measure, identifying if this is a governance measure an organization can undertake itself or in collaboration with other parties, e.g. regulators or external stakeholder..>\n\nReferences [10%]\nInclude references to source you use for you analysis, which could be academic or policy papers, blogs, information from company web site or news stories.\n\n</rubric>\nThe assignment content will be provided by user (not in system prompt)\nDo not say anything else except the marking, following the format of:\n{\n    \"feedback\": \"feedback content\",\n    \"(rubric part 1) / (rubric part 1 weight)\": score,\n    \"(rubric part 2) / (rubric part 2 weight)\": score,\n    ......other rubric parts\n    \"total score\": weighted average score of all parts\n}"
    },
    {
      "role": "user",
      "content": "Investigating and Analysing the Ethical Risks of Tinder\n\nDescription of Application\n\nWhat is Tinder?\n\nFounded in 2012 by, Sean Rad, Justin Mateen, Joe Munoz, Justin Mateen, Chris Gylczynski, Dinesh Moorjani, and Whitney Wolfe the online dating app Tinder has become a globally recognised service used by millions of people with the hopes of finding connections online. The app is now available in over 190 countries and uses over 40 languages. Originally seen by users as simply a higher tech version of the social game \u2018hot or not\u2019 Tinder has since become the most popular app for singletons to meet new people online. (1)\n\nIn order to access the service, an individual must first create an account on the application and input their phone number and email address. Once those have both been verified through confirmation messages the user is free to make a profile. The information they give the app includes their name, age, gender, general location, distance from their own location which they would like possible matches to be located within, as well as their gender preferences in a partner. In addition to this users will provide the app with photos of themselves to use when showing other users their profile and they can also any extra information they like, such as their height, hobbies and the level of seriousness they are looking for from possible matches. (2)\n\nOnce an individual has created their profile, they can then begin swiping. Users are shown a series of other users profiles that Tinder\u2019s AI powered algorithm have deemed as compatible and they have the option to say yes they like the look of the user by swiping right, or no they want to move on to the next person by swiping left. If two people both swipe right on each other they get a notification to say they\u2019ve matched with someone and they have the option to message each other. (3) This is the basic philosophy upon which the company was founded as a platform where two people could approach each other while knowing in advance that both parties have an interest in the other. (1)\n\n\nRole of AI and the Collection of User Data\n\nTinder\u2019s engagement of users is what has made it so widely successful, and their AI powered algorithm is truly the key to its continued use by individuals. Prior to 2019, as part of the apps algorithm, Tinder controversially used an Elo rating system in order to match users to each other. This system most commonly known for ranking online chess players abilities and matching them up with chess players of similar ability, worked by assigning a score to individual users based on their level of attractiveness and then it matched users up with similar scores. It generated scores by looking at the percentage of users that swiped right on a profile vs the number that swiped left and the ones who got the most swipe rights were deemed attractive by the general public. (4)\nThis process resulted in those with high scores being shown to more people and thus their profile was gaining more exposure and they were more likely to match with other users. As a result users deemed \u2018less attractive\u2019 based on other users opinion were being largely left out of the app\u2019s process. This was largely criticised as Tinder was accused of shutting out people who traditionally may not have been seen as beautiful. It resulted in what was deemed a  \u2018dating bubbles\u2019 in which attractive people were getting the most attention. Overall it seemed to defeat the purpose of the app, which was inclusivity as well as to diversify peoples dating pool and efficiently match people with people who they may have not had the chance to interact with prior. (5)\n\nDue to the issues this created, Tinder announced in a blog post in 2019 that they would no longer be using Elo as a form of matching profiles however the ways in which they would match profiles instead was simply described as the \u2018latest cutting-edge technology\u2019. (6)\n\nDespite the vagueness of this statement, we do still know other ways the algorithm works in order to match the right individuals. The algorithm itself is obviously very complex but it has a few key features which allow it to generate the right profiles for each user. One thing about each profile that the algorithm analyses is the language style of each individual. Tinder looks at language processing tools like Word2Vec, a model that finds similar words with similar contexts and groups them together in order to detect the intricacies of each users language style. It then find other users with a similar language style using this information. It looks at user\u2019s use of slang and the context in which certain words and phrases are used to find \u2018like-minded\u2019 individuals. Then the idea would be that the conversation between two such individuals will likely flow better and can enhance the quality of the conversations they have.(7)\n\nIn addition to this, Tinder looks at the photos users use for their profiles and take certain aspects of the photos and match them up with other users that possess profile pictures with similar aspects. These aspects can include both having photos playing particular sports or outdoor activities like hiking and rock-climbing, or their photos are set in similar places such as a festival or at the beach. (8)\n\nTinder also uses AI to help users curate their profiles. Their \u2018Smart Photos\u2019 feature helps users choose what their most appealing photos are to gain more people swiping right on their profiles. This is based on the type of photos that other users have tended to get the most positive feedback from. With this then the feature also assists individuals in deciding the order which they should display their photos, based on what the algorithm has deemed with garner the highest likely positive response. (7)\n\n\n\nBenefits Provided due to the use of AI\n\nThere are a number of benefits that can be seen from the use of AI by Tinder. Their AI tools and algorithm assess all profiles and the information contained in each user and they\u2019re able to then offer customers a much more curated and specific experience. This allows for both a more enjoyable experience as the other profiles that each customer sees is tailored to their specific interest and likes, as well as a more affective service as the profiles users are offered have a much better chance of being a person that they\u2019re actually compatible with and would like to pursue romantically. Without an algorithm as advanced as this users would likely just be shown profiles which are randomly generated. \nIdentification of Roles involved in the Application and its Governance \nInternal stakeholders of Tinder\nThe internal stakeholders of Tinder are those individuals who working within the company on a day to day basis. The main internal stakeholders of Tinder are: Employees, Senior Management and Shareholders. \n\nEmployees \u2013 According to their LinkedIn page, Tinder employees around 500 people. These employees hold roles that fall under a wide spectrum of categories. The different areas which someone could be employed under at Tinder are; Communications, Engineering and Security, Finance and Analytics, Marketing, Member Experience, Product and Design, and HR. Employees have it in their best interest for the company to succeed as it offers them job security as well as compensation. (9)\n\nSenior Management- Within the branch of employees, senior management is comprised of positions in the company such as Chief Executive Officer, Chief Financial Officer, Officer of marketing, technology etc. Senior Management positions such as these are in charge of providing the company and its employees with leadership as well as a guiding direction and strategy for the future of the company. \n\nShareholders \u2013 The ownership of tinder is currently held by their parent company Match Group, who are also the parent company of a number of brands including Match.com, OkCupid, and Hinge. Match Group are predominately looking for tinder to turn over a large revenue so that they can continue to make a high profit turnover. (10)\n\nIn addition to the internal stakeholders that work within the company, Tinder also has a number of external stakeholders. These are people or groups that have a connection to Tinder and are somehow affected by their operations but are not working within the organisation itself.\nExternal stakeholders of Tinder\nThe key external stakeholders of Tinder can be broken down into 3 core groups. These include the App user, Competitors and Regulatory Bodies and Government \n\nApp Users \u2013 the most important stakeholder of the entire company of Tinder is their app users. Without any users of the app itself Tinder would cease to exist. Obviously this is because the premiss of the entire concept of a dating app is matching users up with each other. With no such users the app would be useless. As a result of this fact, the primary task of Tinder employees revolves around how to draw in more users, and how to ensure that they remain active on the app. Users themselves are looking to receive the service that was promised to them by being shown profiles of individuals that they are compatible with and vice versa. \n\nCompetitors \u2013 Tinder has many different competitors, however there most prominent one is Bumble. Tinder currently holds the largest percentage of the US online dating app market with 27%. However just 1% behind them at 26% is Bumble. This is illustrated below in Figure 1, which displays a pie chart of the market in 2023. \n\n\n\n\n\n\n\n\n\n\nFigure 1 (11)\n\nThe competitor Bumble is also the most notable of Tinder\u2019s competitors because of the fact that it\u2019s the only application of the top 4; Tinder, Bumble, Hinge and Plenty of Fish, seen above that is not owned by Tinder\u2019s parent company Match Group. Along with this there a deep, messy history associated with Bumble and Tinder. One of the founding members of Tinder, Whitney Wolfe, set up Bumble after she left Tinder following a lawsuit in which she sued the company for sexual harassment  and discrimination in the workplace following her relationship with another one of the founders of Tinder, Justin Mateen. (11) While both Bumble and Tinder have their own unique features they both fall under the same category of online dating and so are constantly in competition to secure new users ahead of each other. In fact according to current projections, Bumble is estimated to overtake Tinder for number of downloads this coming year. This is following a steady down trend in Tinder downloads since 2020 where it was near its peak numbers of 2014, while Bumble has been steadily rising in downloads in that time. (12) \n\nRegulatory Bodies \u2013 The final stakeholder I will talk about in this report are regulatory bodies. Tinder is responsible for millions of people across the world\u2019s personal data. Because of this Tinder has to comply with personal data laws laid out by different regulators across the world. These include the European Commission to comply with EU data processing laws as well as National Consumer Authorities in the countries its active in. In the US any breaches of privacy laws are usually handled by the Federal Trade Commission (FTC) but it varies from state to state. (13) The primary objective of these agencies are to ensure the correct protocol is being done to protect the consumers data and in the event where an individual\u2019s data has been leaked or dealt with unlawfully these commissions will hold the company, in this case Tinder, responsible. Sometimes different agencies will launch investigations into companies based on possible complaints they\u2019ve received in regards to the company\u2019s conduct. For example in 2020 the Irish Data Protection Commission (DPC) had to launch a statutory inquiry into the way Tinder was practicing its privacy policies. (14)\n\n\n\n\nIdentification of Ethical Risks\n\nThere are many possible risks that may occur when running an application such as Tinder. Many of these risks could negatively affect the consumer, in addition Tinder\u2019s actions may affect alternative parties such as their shareholder or even the environment so it\u2019s important that Tinder can identify such risks and work to prevent any negative impact to its users, employees, shareholders and other third parties. Below I have identified some of the ethical risks that Tinder as a company may face.\n\n\nHuman Rights \nWhen evaluating the company\u2019s ethical risks and human rights Tinder should consider the privacy laws and protection of users.\n\nLegal\n\nIn regards to EU countries, applications must follow the General Data Protection Regulation, or as its more commonly known GDPR. This regulation establishes protection of personal data for privacy and security of individuals within the EU. All companies must follow these regulations within these territories regardless of where the company is based if its operating within the EU. (15) Tinder is at a much higher risk of breaching these regulations due to the high volume of personal data it collects on individuals. They store and have access to all photos, messages and matches every user makes while using the app. With that much data on millions of people Tinder have a big responsibility in ensuring each person\u2019s data remains safe and secure. Along with the risk storing so much personal data results in for the user, in the event that Tinder were to be hacked or unlawfully distribute or store peoples personal data, they also face the risk of massive damage to their brands reputation. Along with this the failure to comply with GDPR can result a company having to pay up to \u20ac20 million euros or in some case up to 4% of the company\u2019s global turnover for that year. (16)\n\n\n\nLabour Practices \nLabour Laws \n\nLabour Laws are another human rights issue that Tinder could face. In the past as previously mentioned they have faced lawsuits on sexual harassment and discrimination in the workplace. Claims like that made by a former employee is both a very troubling thing to hear from a moral standpoint, but it\u2019s also something that could be incredibly damaging to the brand reputation of a company, particularly in thar case as it was two founders of the company. The perpetrator being a leader of the company and someone that held a large amount of power in the organisation. Along with the damage to the brand\u2019s reputation due to these events, it was also a large financial burden to the company. \nIn the case of Whitney Wolfe\u2019s lawsuit against Tinder, she settled out of court with the company and was paid around $1 million and Justin Mateen the man she accused was suspended before ultimately losing his job. (11)\nA scandal like that affects almost all of the stakeholders of Tinder. The senior management of the company was affected as they lost the Vice President of Marketing and Co-founder, Wolfe, and they also lost, arguably for the better from an ethics standpoint, another Co-Founder and the CMO in the form of Justin Mateen. \nShareholders are also affected as in the event of a scandal like that share price will likely drop due to the damage to the brand\u2019s reputation as well as the instability in senior management. \nOf course finally these events would likely affect the way in which users viewed the company and in some cases users own personal moral codes would likely lead to them stopping to use Tinder and instead switch to an alternative.\n\n\n\nThe Environment \nAs Tinder is only an application and doesn\u2019t result in the buying and selling of any physical product, but instead just offers a digital service it has a limited effect on the environment overall. However that\u2019s not to say it has no effect at all. The use of the app itself results in the growth of every users carbon footprint. Every time someone swipes to match with someone or sends a message they are using energy which will results I increased carbon emissions. However one could argue that Tinder itself is not at fault for this and if it didn\u2019t exist the individual would be using a different application and increasing their emissions anyway. (17) \n\n\n\nFair Operating Procedures \nThe Fair Operating Procedures of a company refer to the transparent practices used in the running of the business to ensure that the companies ethical guidelines aren\u2019t impeded on as a result of the malpractice of a group or individual involved.  (18) One potential ethical risk that may occur as a result of this is the importance to ensure the safety of civil and political practices. \n\nCivil and Political Risks\n\nTinder are obligated to be impartial in the event of an election or referendum and have a duty to ensure that no misinformation of any kind is spread on the platform. In the event that misinformation is spread on the platform and Tinder do not act to remove it or inform users of the issues they could face legal battles.\n\n\n\nConsumer Issues \nTinder have an obligation to their users to ensure they experience limited issues with the application. They can try and ensure that the app\u2019s impact on user\u2019s mental health and self-image is limited and that the company has no part in contributing a negative impact to consumers in these areas. One way in which Tinder can ensure user safety is by confirming every count as a real person and the that the account is representing the person they are saying they are. \n\n\nFake Accounts\n\nOne way in which Tinder can ensure they do not negatively impact their user\u2019s health and wellbeing is through ensuring that app users do not experience any catfishing. Catfishing is when someone will create a fake account using someone else\u2019s pictures or information and try pass themselves off as that person to other users. Tinder work to ensure that consumers are protected from acts likes this by their account check authentication. This allows users to scan a picture of their passport or diver\u2019s license into the and Tinder will check it and confirm that the person who appears to run the account actually does. After the account is authenticated by Tinder the user will receive a blue tick next to their name similar to X (formally twitter) or Instagram. This practice ensures users can choose to only interact with other users that have been authenticated by the app and provides a higher safety and security level for app users. (19)\n\n\nDiscrimination and Harassment\n\nDue to the nature of the application of Tinder involving messaging between strangers, often times issues arise between certain users choosing to act in a hateful way towards other users. This mostly comes in the form of discriminating messages following matching with another individual. \nTinder has strict community guidelines that ban and racial discrimination or abuse. In the event that a user feels they have received abuse from another user in the form of a hateful message, they can report the user to Tinder for their behaviour and the platform will ban the account. (20) \nHowever in some case there is grey area between what one user is deeming a compliment and another user finds offensive. For example many women that have used Tinder in the past have expressed how their race is often fetishized by users and perverse assumptions are made about their heritage. One black women said she\u2019d been messaged by a male user saying he wanted a \u2018taste of jungle fever\u2019 with another user saying she\u2019s received messages saying she \u2018looks like a dominant black queen\u2019. In cases like this where it\u2019s clear why these women are offended and often disturbed, but the tone behind the messages aren\u2019t strictly hateful just ignorant, it\u2019s much harder to ban such an account or make a complaint. Along with this because the app is a dating service the level of sexual explicitness used by users when messaging other profiles is hard to regulate and so harassment is easier to achieve for those who wish to. (21)\n\n\n\nCommunity Involvement and Development \nMental Health of Communities\n\nMany recent studies done on online dating as a whole note that dating services such as Tinder can result in some users experiencing depression. This is because they are interacting with so many people in a short space of time and will often face a high volume of rejection, which will negatively affect users self-esteem. In addition to this studies suggest that constant use of dating apps like Tinder can result in people finding it more difficult to relate to people face to face and form relationships in person. (22) These issues can greatly affect users mental health and its evident that is some way for some users dating apps seem to hinder their social development.\n\n\nBiased Algorithms \n\nAnother aspect of Tinder that could affect a communities development in a negative way is a biased algorithm. As I discussed earlier Tinder\u2019s algorithm is complex and has many different aspects that affect its outcomes for users. In the event that the algorithm was deemed biased in some way this could be an issue for both consumers who its acting on as well as the company itself which may face legal disputes as a result. A biased algorithm could be biased in many ways. For example as I previously stated Tinder\u2019s algorithm was previously biased in the form of attractiveness. It resulted in less attractive people being pushed out of the service. This could negatively affect the users mental health due to the societal rejection and in addition it could inhibit social development people get from meeting new and different people that they have not had the chance to interact with in the past. \n\n\n\nDiscussion of Mitigations Measures for Risk \n\nMitigation measure are steps that a company can take in order to prevent or reduce any risk from happening. They are essential in maintaining a company\u2019s brand reputation and they can ensure that a company can always maintain their integrity on behalf of their employees and shareholders.\n\nI mentioned previously risks that relate to the misuse of consumers personal data and protecting their safety while using the app. Now I will discuss some of the mitigation measures that Tinder could implement to prevent and reduce these ethical risks.\n\nThe risks that I have chosen to suggest mitigation measures for are fake accounts and the mistreatment of personal data of users. \n\n\nMitigation Measures for Fake Accounts\n\nCurrently its estimated that over 10% of tinder profiles are counterfeit. With 75 million global users that means that currently there are up to almost 8 million Tinder accounts that are fake. In order to protect legitimate Tinder customers from these catfish more measures need to be put in place to remove these accounts. (23)\nOne measure that Tinder could take is follow in their competitor Bumble\u2019s footsteps by introducing an AI tool to aid the removal of scam and fake profiles. Bumble recently launched their Deception Detector, and found during their testing phase that it was effective in blocking 95% of spam and scam profiles and in addition to this it was able to reduce any reports of scam and fake accounts by 45%. Implementing a tool like this into Tinder\u2019s framework would be a great way of improving user security for the consumer and it would also reflect well on the company as a whole as it would create more trust between Tinder and the consumer, thus improving brand reputation. (24)\n\nMitigation Measures for Mistreatment of Data\n\nIn the past Tinder have come under fire for mistreating users data. In 2014 it was discovered that Tinder were sharing more of app user\u2019s location data them they had previously thought. Hackers were able to access Tinder data and the had the ability to trace the location for individual users to within 100 feet. This was obviously a big issue and had to be fixed quickly to protect customers. Other issues like this have cropped up over the years where users had their privacy breached due to data leaks and hacking. One way in which this could be prevented in the future is by Tinder investing more of their money into cybersecurity and encrypting their data. This would result in more trust between the company and its users, which thus improves the brand reputation. In addition to this it would mean that consumers are better protected and it could also set Tinder apart from its competitors as a more secure platform. (25)\n\n\n\n\n\n\n\nReferences \n\n\nhttps://time.com/4837/tinder-meet-the-guys-who-turned-dating-into-an-addiction/\n\nhttps://www.scmp.com/lifestyle/entertainment/article/3120217/how-use-tinder-profiles-premium-features-swiping-what#:~:text=To%20use%20Tinder%2C%20you%20must,Then%20you%20begin%20swiping.\n\nhttps://www.help.tinder.com/hc/en-us/articles/115004647686-Tinder-Overview\n\n\nhttps://www.vox.com/2019/2/7/18210998/tinder-algorithm-swiping-tips-dating-app-science\n\nhttps://www.businessinsider.com/guides/tech/what-is-tinder#:~:text=Tinder%20previously%20used%20a%20notorious,those%20with%20a%20similar%20rank.\n\nhttps://www.vox.com/2019/2/7/18210998/tinder-algorithm-swiping-tips-dating-app-science\n\nhttps://www.analyticsvidhya.com/blog/2023/05/the-unexpected-love-affair-how-ai-transforms-tinders-dating-experience/#:~:text=Tinder's%20AI%20algorithm%20works%20as,AI%20and%20Machine%20Learning%20techniques.\n\n\nhttps://www.help.tinder.com/hc/en-us/articles/7606685697037-Powering-Tinder-The-Method-Behind-Our-Matching\n\nhttps://www.linkedin.com/company/tinder-incorporated/\n\n\nhttps://bullishbears.com/tinder-stock/#:~:text=trade%20Tinder%20stock%3F-,Match%20Group%2C%20Inc.,totaling%2045%20global%20dating%20companies.\n\n\nhttps://nypost.com/2021/02/12/how-bumble-ceo-whitney-wolfe-herd-became-a-billionaire-after-an-ipo-and-after-ending-some-awkward-relationships/#:~:text=The%2031%2Dyear%2Dold%20chief,called%20her%20a%20\u201cwhore.\n\nhttps://edition.cnn.com/2024/02/14/business/dating-apps-2024-hinge-tinder-dg/index.html#:~:text=Tinder's%20dominance%20is%20waning&text=But%20downloads%20of%20rival%20app,%2C%20according%20to%20data.ai.\n\nhttps://www.dlapiperdataprotection.com/index.html?t=authority&c=US#:~:text=Various%20entities%20enforce%20US%20national,the%20industry%20sector%20in%20question.\n\n\nhttps://www.irishlegal.com/articles/data-protection-watchdog-launches-inquiries-into-google-and-tinder\n\n\n https://www.hrpo.pitt.edu/european-union-eu-general-data-protection-regulation-gdpr#:~:text=What%20is%20the%20GDPR%3F,of%20individuals%20in%20the%20EEA\n\n\n\nhttps://gdpr-info.eu/issues/fines-penalties/#:~:text=83(5)%20GDPR%2C%20the,less%20severe%20violations%20in%20Art.\n\n\n\nhttps://medium.com/@pelinokutan/the-environmental-impact-of-tinder-swiping-for-love-leaving-a-carbon-footprint-98490c5e3751#:~:text=Every%20swipe%2C%20match%2C%20and%20message,leads%20to%20increased%20carbon%20emissions.\n\nhttps://legalblog.ie/fair-procedures/#:~:text=The%20extent%20of%20fair%20procedures,a%20possibility%20of%20an%20appeal.\n\n\nhttps://www.refinery29.com/en-gb/tinder-id-verification-catfishing\n\n\n\nhttps://www.abc.net.au/news/science/2020-07-03/indigenous-dating-app-racism-tinder-grindr/12406402#:~:text=Tinder%20and%20Grindr%20both%20have,Match%20Group%2C%20declined%20to%20comment.\n\n\n\nhttps://www.independent.co.uk/life-style/love-sex/dating-apps-racism-tinder-bumble-grindr-online-dating-a8504996.html\n\n\nhttps://www.embarkbh.com/blog/technology/the-psychological-effects-of-online-dating-for-young-adults/#:~:text=5.-,Difficulties%20forming%20in%2Dperson%20relationships,dating%20app%3F\u201d%20Foreman%20said.\n\nhttps://besedo.com/knowledge-hub/blog/welcome-to-the-age-of-fake-dating-profiles/\n\n\n\n\nhttps://mashable.com/article/bumble-deception-detector-spam-scam-fake-profiles\n\n\nhttps://www.theguardian.com/technology/2014/feb/20/tinder-app-dating-data-location-sharing"
    },
    {
      "role": "assistant",
      "content": "{\n    \"feedback\": \"A really well research analysis of tinder. One issue you perhaps touched on but could have explored further was how well the algorithm matched people for durable relationships - in once sense tinder is motivated to allow people to date, but if that results in a more durable relationship presumably those people stop being users. The mental health issue is an important one, since its not just about appearance but explicit acceptance/rejection by others.\",\n    \"description / 20\": 70,\n    \"roles / 10\": 70,\n    \"risks / 40\": 70,\n    \"mitigation / 20\": 70,\n    \"refs / 10\": 80,\n    \"total score\": 71.0\n}"
    }
  ],
  [
    {
      "role": "system",
      "content": "You are a professional educator. \nYour target is to assess students' assignments based on the provided rubric (between <rubric> and <rubric> labels).\n<rubric>\nAssignment: Investigating and Analysing Ethical Risks of a Digital Engagement application \n\nThis is an INDIVIDUAL assignment on Ethical Risk Assessment for the TEU00062 module. This assessment is worth 30% of the total TEU 00062 module mark. The deadline for submission is 12.00. noon Friday 31st March 2024. \n \nLearning outcomes: Upon completion of this assignment students should:\nBe able to identify the different stakeholders and organisational roles involved in developing and using AI-driven digital engagement application, and motivations. \nBe able to identify and analyse ethical risks in a digital engagement application.\nBe equipped to identify technical and governance mitigations of these risks\n \nSteps:\n \nSelect ONE real world application that embodies one or more AI-driven digital engagement techniques you have learnt about on this course, e.g. e.g. digital engagement techniques used in one applications such as social media, newsfeeds, search, eCommerce, elearning or digital humanities., or in an application you are familiar with e.g. interactive Game, Virtual Presence (i.e. VR /AR) etc. \nIdentify the different value chain stakeholder roles involved in this application scenario and where possible the actors (i.e. companies, organisations) carrying out those roles. including those involved in: data collection; AI model development; development and use of the application. \nUse social responsibility principles (see ethic lecture week 2) to identify and analyse real or potential ethical risks of the application, including classification of the affected stakeholders (hint: see also risks classified by Latzer et al 2016 \u2013 ethics lecture week 1), how severe the risk would be and how likely it is to occur.\nIdentify one or more mitigation measures that specific value-chain stakeholders may undertake to minimise the impact of the risk you think is most harmful. These could be technical measures, organisational governance measures or measure involving additional engagement with affected individuals or groups.\n \nStructure of Assignment Report. \nStructure the Report (4000 word max excluding figures and references), under the following heading [indicative percentage of assignment mark per section]. Please provide references\n \nDescription of Application [20%]\n<describe your selected application, explaining what it does, the role AI and data collection plays in its digital engagement features, and the benefits it provides>\n\nIdentification of Stakeholder Roles involved in the application and its governance [10%]\n<describe the stakeholder roles undertaken in the development, use and governance of the application, and which organization take those roles and the benefits they derive from their involvement. Include roles that may be indirectly affected by the application. >\n\nIdentification of Ethical Risks [40%]\n<Identify the ethical risks that may be raised by your application under the social responsibility categories below (more details in ethics lecture week 2). For each risk identify which stakeholder may be affected, and assess how severe the impact would be for each stakeholder (Low, Medium, High) and the likelihood of that risk being incurred (Low, Medium, High), justifying your assessment, even if you think there is no risk under that heading.>\nHuman Rights\nLabour Practices\nThe Environment\nFair Operating Procedures\nConsumer Issues\nCommunity Involvement and Development\n \nDiscussion of Mitigations Measures for Risk [20%]\n< For one of the risks you consider more severe, describe mitigation measures that could be taken. Explain how the measure would reduce or eliminate the likelihood and impact of the risk. Explain which stakeholders would be involved, or would have to interact in implementing the measure, identifying if this is a governance measure an organization can undertake itself or in collaboration with other parties, e.g. regulators or external stakeholder..>\n\nReferences [10%]\nInclude references to source you use for you analysis, which could be academic or policy papers, blogs, information from company web site or news stories.\n\n</rubric>\nThe assignment content will be provided by user (not in system prompt)\nDo not say anything else except the marking, following the format of:\n{\n    \"feedback\": \"feedback content\",\n    \"(rubric part 1) / (rubric part 1 weight)\": score,\n    \"(rubric part 2) / (rubric part 2 weight)\": score,\n    ......other rubric parts\n    \"total score\": weighted average score of all parts\n}"
    },
    {
      "role": "user",
      "content": "AI-Driven Personalization in Ecommerce\n\nDescription of Application\nThis report will discuss how the AI-driven digital engagement technique of personalisation is applied within the realm of ecommerce. The advent of AI in climate where online retail is evolving every day has raised the bar for personalisation in ecommerce. AI has revolutionised the manner in which businesses engage with consumers by enabling the creation of tailored, immersive shopping experiences that were previously unattainable. \n\u00a0\nData lies at the core of AI-driven personalisation. The amalgamation of consumer and product data fuels AI engines. Without this foundational component, the true potential of AI-driven personalisation may not be realised. Real-time consumer data offers a plethora of insights into consumer behaviours, preferences and purchasing trends. Every click, page visit, purchase, and even cart abandonment feed AI algorithms with crucial information, facilitating the creation of highly customised experiences that meet the unique preferences of every consumer. Product data introduces another layer of complexity. A comprehensive product database is comprised of elements such as product specifications, photos, reviews and ratings. AI algorithms meticulously examine this data to discern product characteristics and how they relate to consumer preferences, resulting in more accurate and pertinent recommendations. \n\u00a0\nGenerative AI pushes the boundaries of personalization further by going beyond the mere analysis of consumer and product data. By actively creating fresh content, generative AI transforms the consumer experience (Brenninkmeijer, 2024). Examples of AI-driven personalisation in ecommerce include customised recommendations, marketing campaigns, predictive analysis, and support in the form of chatbots/virtual assistance.\n\u00a0\nRecommendation engines powered by AI contribute to higher upsell revenues by monitoring consumer behaviour in real time and providing smarter, individualised product recommendations (Mileva, 2023). Studies reveal that when contemplating their first purchase from a brand, 67% of initial customers regard product recommendations as a critical factor influencing their decision to buy (Brenninkmeijer, 2024). In markets marked by fierce competition and numerous distractions, product recommendations can be transformative. They lead to higher upsell revenues by not only fostering loyalty among returning consumer buts also conveying a sense of value and understanding to first-time visitors from the brand (Seekmeai, 2023).\n\u00a0\nAI also empowers enterprises to deliver targeted marketing messages, promotions, and discounts to the appropriate customers at the opportune moments. More effective advertising reduces marketing spend and increases revenue (Mileva, 2023). It also enhances conversion rates whilst mitigating the likelihood of overwhelming consumers with irrelevant content, hence refining the overall consumer experience (Seekmeai, 2023).\n\u00a0\nAI also has the capacity to predict forthcoming consumer behaviours and trends, which helps ecommerce enterprises in forecasting demand. This gives companies adequate time to align inventories and refine pricing strategies accordingly. Operational efficiency and consumer satisfactions improves as a result (Mileva, 2023).\n\u00a0\nChatbots powered by AI transcend robotic, scripted replies by employing various techniques such Natural Language Processing (NLP) and sentiment analysis to increase the quality of support to consumers (Mileva, 2023). These virtual assistants provide instant, round-the-clock service making consumers feel supported always. Aside from addressing queries, resolving queries, and providing guidance, these chatbots also assimilate knowledge from each interaction to furnish increasingly precise responses over time (Seekmeai, 2023). Human agents are freed up and mundane issues are solved quickly and efficiently. According to Juniper Research, chatbots saved businesses approximately 2.5 billion hours and $8 billion in 2022 (Cummings, 2023).\n\nIdentification of Stakeholder Roles Involved in the Application and its Governance \nIn the domain of AI-powered personalization in e-commerce, various stakeholders hold pivotal positions in the development, utilization, and regulation of applications. An overview of these stakeholders and the roles they fulfil are outlined below.\n\u00a0\nAI-driven personalisation systems are often developed by ecommerce companies (retailers), either through internal development or partnerships with technology partners, to enhance the consumer experience and drive sales. For example, the collaboration between The Estee Lauder Companies Inc. (ELC) with Google Cloud represented a ground-breaking advancement in ecommerce personalisation. ELC offers real-time consumer sentiment analysis by incorporating generative AI, proactively resolving complaints, and improving the digital brand experience. The Retail Search and Recommendations Artificial Intelligence solutions from Google Cloud enhance customised retail experiences on ELC's websites, guaranteeing smooth customer journeys. Using Google's PaLM 2 language model, ELC can perform extensive sentiment analysis and customise plans for more than 20 high-end cosmetics companies. Utilising Google Cloud's Vertex AI platform to integrate AI-powered apps simplifies processes, lowers expenses, and increases output (Sitiatarfa, 2024). Ecommerce is also in charge of establishing rules and regulations for the moral application of AI to personalisation, guaranteeing observance of data privacy, and monitoring the correct operation of the systems. This also involves being transparent with consumers by educating them about AI usage in chatbots, personalization, and product recommendations (Hakeemat, 2023).\n\u00a0\nData scientists and AI engineers create and implement various AI models for personalisation, such as collaborative filtering and deep learning. They work to continuously ameliorate the accuracy and efficacy of personalisation by fine-tuning algorithms based on user behaviour and feedback (Yang et al., 2019). Shopee is an ecommerce platform that also employs a team of data scientists and AI engineers who are dedicated to improving the brand experience using advanced engagement tools. According to estimates from Bolt, the Shopee Beauty Cam, an augmented reality makeup try-on tool, has increased conversions for beauty businesses by up to three times. This supports research indicating that 75% of consumers are prepared to spend extra for tailored online shopping experiences, underscoring the potency of Shopee's strategy (Sitiatarfa, 2024).\n\u00a0\nOf course, consumers engage with AI-driven personalisation features such as customised recommendations, marketing campaigns, predictive analysis, and support in the form of chatbots/virtual assistance that are specifically catered to them based on their behaviour patterns and interest. For example, the implementation of Tongyi Qianwen on Taobao and Tmall ushered in a fresh era of customer engagement, marked by the debut of Ali Xiaomi, a chatbot offering personalized responses through natural language understanding (Sitiatarfa, 2024). Through their interactions with tailored experiences, customers offer feedback that shapes industry standards around data protection and user permission as well as the development of AI algorithms (Liu et al., 2021).\n\u00a0\nOther stakeholders that take on external oversight roles include regulatory bodies and government agencies, as well as third-party auditors and ethical AI consultants. For example, the European AI office established in February 2024 within the Commission, oversees the enforcement and implementation of the AI Act in member states. Its goal is to foster an atmosphere where AI technologies respect human rights, dignity, and confidence while encouraging cooperation, creativity, and research among stakeholders (European Commission, 2024). Element AI is an ethical AI consulting firm that is committed to responsible AI development. The company leverages AI for societal good and promotes equitable progress through its active involvement in AI for Good initiatives (Kueny, 2023).\n\nIdentification of Ethical Risks\nAI personalisation in e-commerce presents an array of moral dilemmas that may impact several different parties. An explanation of these risks and what they mean for stakeholder groups are described below.\n\u00a0\nOptimal AI functionality requires large volumes of customer data, which often raises privacy concerns. To profile and personalise users to increase click-through rates, advertisers and data collecting companies seek to obtain as much information as possible about consumers (Apostolov, 2019). Despite the many benefits consumers experience from AI-driven personalisation in ecommerce, studies also find that even those who are in favour of AI-powered personalisation, are also apprehensive about disclosing such personal information (Zhu and Chang, 2016). Data including browsing history, purchase behaviour and demographic information may be collected without users' explicit consent. For example, although some users regularly remove their cookies to restrict the amount of private information that websites can use, data collectors still use methods like stealth browser-based tracking, which expose personal information to advertisers even when cookies are removed (Nill and Aalberts, 2014). Even if data is collected with the users' permission, there is still a fine line between personalisation and intrusion. Of course, this risk of privacy breach because of AI personalisation in ecommerce directly affects consumers. The relationship between consumers and an ecommerce company would inevitably be compromised if consumers perceive their privacy as violated or their sensitive information exploited by third parties. These breaches may severely undermine consumer trust, leading to reduced loyalty and ultimately, a decrease in revenue for the business. In addition to experiencing decreased revenue, companies may face a huge bashing to their public reputation in light of a privacy breach scandal. For example, in 2018, Facebook encountered a significant crisis following the revelation that a political consultancy business had improperly obtained the personal information of millions of users. This episode brought to light the significance of moral data practices and the requirement for more stringent laws to safeguard user privacy (FasterCapital, n.d.). \n\u00a0\nThe impartiality of personalization algorithms hinges on the fairness of the data they utilize. Therefore, if AI algorithms are trained on inadequate datasets, biases may seep into them and lead to prejudice against particular client demographics, potentially seriously damaging a company's reputation. If biases are present in the training set, these algorithms could reinforce and amplify discriminating results. For example, if an algorithm for recommending jobs shows prejudice against groups of people, it may worsen already-existing disparities in the labour market (FasterCapital, n.d.). In 2020, Twitter's image cropping algorithm demonstrated a preference for white faces over black ones in picture previews. This bias was starkly apparent as the algorithm cropped images to prominently display white faces (Marketing, 2023). Moreover, in 2018, Amazon also faced huge criticism when it was discovered that its AI-powered hiring tool was biased against female applicants. The algorithm was trained on historical resumes, most of which came from male applicants, leading to a gender bias in the recommendations (FasterCapital, n.d.).\u00a0The development of filter bubbles as a result of AI-driven personalisation may play a role in further perpetuating bias. The term \"filter bubble\" describes the process of continually limiting a user's material through excessive personalisation to the point where they only see the same content. If people are overly reliant on online ecommerce websites, and remain ignorant to the underlying filtering mechanisms used, they risk losing arguably the greatest benefit of being an internet user - autonomy (Bozdag and Timmermans, 2011).\n\u00a0\nManipulative marketing strategies have existed for years the in the world of ecommerce. However, these strategies coupled with a collection of large volumes of data for AI algorithmic systems have dramatically increased firms' capabilities to influence consumer choices and behaviour. The use of AI with alongside clever design tactics, predatory advertising, and pervasive price discrimination may lead consumers to make inferior purchasing choices that ultimately serve the financial interests of the firm (Petropoulos, 2022). Through continual optimization for engagement metrics, algorithms have the potential to exploit psychological weaknesses, directing users towards addictive content or behaviours. For instance, the incessant flow of personalized content, notifications, and recommendations on numerous e-commerce social media platforms can foster addictive consumer behaviour and exacerbate mental health issues by encouraging excessive screen time (FasterCapital, n.d.).\n\u00a0\nThe extent of the risks highlighted above and how they affect various stakeholders is contingent upon the degree to which an individual ecommerce company relies on AI for personalization on their platforms. For example, in terms of privacy concerns, a company with minimal reliance on AI personalization may receive a few nonthreatening queries surrounding data collection. However, for those heavily dependent on AI, the stakes are considerably higher, as they may face significant legal repercussions and profound breaches of consumer trust. These threats are likely to materialise given the growing use of AI in e-commerce unless businesses put strong mitigation measures in place. Nevertheless, commercial pressures may encourage some businesses to put financial gain ahead of ethical considerations. Striking a healthy balance between AI utilization and human interaction poses as a challenge for many ecommerce companies, yet it should not be overlooked as it has knock on effects on other stakeholders involved, such as consumers (Curator, n.d.). \n\nDiscussions of Mitigation Measures for Risk\nIn order to address the ethical risks outlined above, the following mitigation measures may be implemented.\n\u00a0\nIn order to address the pressing privacy concerns surrounding with the vast amounts of user data required for machine learning, responsible data handling is imperative. Businesses must secure explicit user consent to process data, adhere to strict data protection regulations and ensure robust security measures are in place to safeguard sensitive information. Effective personalisation may still be achieved when data is anonymized to protect user privacy (FasterCapital, n.d.). Establishing a commitment to data privacy guarantees both legal compliance and the preservation of customer trust. It is also important that ecommerce companies foster a sense of transparency regarding their use of AI in personalisation. This involves openly communicating the extent of AI integration, data handling practices and security measures employed. Building confidence can be aided by offering easily reachable customer service and transparent channels for resolving AI-related problems. The relationship between consumers and e-commerce firms is strengthened when the latter displays that it is committed to ethical AI practices (Seekmeai, 2023).\n\u00a0\nAccording to a study carried out by Dr. Varsha P.S. at Presidency University concluded there two ways of reducing bias in AI algorithmic systems. Firstly AI itself may be used to detect and reduce the effect of human bias. Secondly, enhancing AI systems' ability to use data to generate and implement models may be effective in avoiding the creation of bias and or perpetuating various prejudices held by people and society. In order to implement either of these approaches successfully, firms should recognise the scenarios where AI can rectify bias, as well as those where AI may exacerbate bias. This necessitates the establishment of stringent policies and techniques to detect and mitigate bias in AI systems (Varsha, 2023). This may involve the careful curation of training datasets, regular auditing of algorithms for bias and the use of strategies such as adversarial testing. The ProPublica investigation into biased recidivism prediction algorithms emphasize the importance of addressing bias in machine learning (FasterCapital, n.d.).\n\u00a0\nNavigating ethics of AI-driven personalisation in ecommerce requires extreme caution on the part of both marketers and AI engineers. Marketing experts must be conscious of the power they possess and use it wisely, ensuring that AI is being used to improve the shopping experience rather than prey on consumer weaknesses (Seekmeai, 2023). Of course, the goal of personalisation is to improve consumer experiences, however it is crucial to honour user choices and allow individuals authority over their customised paths, so their autonomy is not removed. Users may be empowered, and the acceptance of personalised recommendations can be guaranteed by giving them the choice to accept or reject personalised experiences, letting them alter their preferences, and offering clear settings to control personalisation. By providing choices to opt-in or opt-out of personalised experiences, it is possible to empower users and receive their approval of personalised recommendations. People feel more in control over their online experiences if they can modify their preferences and adjust clear settings to govern personalisation. This cultivates sense of freedom and confidence in their shopping experience (FasterCapital, n.d.).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorks Cited\n\nApostolov, N. (2019). Does Artificial Intelligence Raise Any Ethical Issues When Used for Advertising Personalization by E-commerce Stores. University Of Twente Student Theses. [online] Available at: chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://essay.utwente.nl/78528/1/Apostolov_BA_BMS.pdf.\n\nBozdag, E. and Timmermans, J. (2011). Values in the Filter Bubble: Ethics of Personalisation Algorithms in Cloud Computing. First International Workshop on Values in DesignBuilding Bridges between RE, HCI and Ethics, 296.\n\nBrenninkmeijer, T. (2024). How AI Is Changing Ecommerce Personalization. [online] CMSWire.com. Available at: https://www.cmswire.com/ecommerce/how-ai-is-changing-ecommerce-personalization/.\n\nCummings, C. (2023). Council Post: How E-Commerce Brands Can Use AI To Deliver A More Personalized Shopping Experience. [online] Forbes. Available at: https://www.forbes.com/sites/forbestechcouncil/2023/04/21/how-e-commerce-brands-can-use-ai-to-deliver-a-more-personalized-shopping-experience/?sh=2a785c18547c [Accessed 31 Mar. 2024].\n\nCurator. (n.d.). Pros and Cons of AI for Your E-Commerce Business. [online] Available at: https://curator.io/blog/e-commerce-business-website-ai.\n\nEuropean Commission (2024). AI Act | Shaping Europe\u2019s Digital Future. [online] European Union . Available at: https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai#:~:text=The%20European%20AI%20Office%2C%20established.\n\nFasterCapital. (n.d.). Ethical Considerations In Ai Personalization. [online] Available at: https://fastercapital.com/topics/ethical-considerations-in-ai-personalization.html/1.\nHakeemat, I. (2023). Legal Implication of Artificial Intelligence in E-Commerce: Transforming the Online Shopping Experience. [online] www.linkedin.com. Available at: https://www.linkedin.com/pulse/legal-implication-artificial-intelligence-e-commerce-online-hakeemat-xb9yc/.\n\nKueny, T. (2023). Top 5 Ethical AI Consulting Firms Leading Responsible Innovation. [online] www.thetechgazette.co. Available at: https://www.thetechgazette.co/posts/ethical-ai-consulting-leaders [Accessed 31 Mar. 2024].\n\nLiu, Y., Xiong, Y., Ji, Z. and Wu, L. (2021). An Online Deep Learning Recommendation System for E-Commerce.\n\nMarketing (2023). The Significance AI Bias in Marketing: What You Need To Know. [online] Robotic Marketer. Available at: https://www.roboticmarketer.com/ai-bias-in-marketing-everything-you-need-to-know/ [Accessed 31 Mar. 2024].\n\nMileva, G. (2023). The Role of AI Personalization in eCommerce Growth. [online] Influencer Marketing Hub. Available at: https://influencermarketinghub.com/ai-personalization-ecommerce/.\n\nNill, A. and Aalberts, R.J. (2014). Legal and Ethical Challenges of Online Behavioral Targeting in Advertising. Journal of Current Issues & Research in Advertising, 35(2), pp.126\u2013146. doi:https://doi.org/10.1080/10641734.2014.899529.\n\nPetropoulos, G. (2022). The dark side of artificial intelligence: Manipulation of human behaviour. [online] Bruegel | the Brussels-based Economic Think Tank. Available at: https://www.bruegel.org/blog-post/dark-side-artificial-intelligence-manipulation-human-behaviour.\n\nSeekmeai (2023). How AI Impacted E-commerce: Personalization, Recommendations, and Customer Experience. [online] Medium. Available at: https://medium.com/@seekmeai/how-ai-impacted-e-commerce-personalization-recommendations-and-customer-experience-cdb7ec64faf8.\n\nSitiatarfa (2024). How Shopee and Others Transforming eCommerce with AI Personalisation Strategies. [online] Kitameraki. Available at: https://www.kitameraki.com/post/how-shopee-and-others-transforming-ecommerce-with-ai-personalization-strategies#:~:text=satisfaction%20and%20loyalty.-,Product%20Recommendations,to%20generate%20relevant%20product%20recommendations.\n\nVarsha, Dr.P. (2023). How can we manage biases in artificial intelligence systems \u2013 A systematic literature review. International Journal of Information Management Data Insights, [online] 3(1), p.100165. doi:https://doi.org/10.1016/j.jjimei.2023.100165.\n\nYang, J., Tang, Y., Zhang, B., Zhou, W., Ma, J. and Huang (2019). Attention-Based Collaborative Filtering with Routing-by-Agreement. 33, pp.4323\u20134330.\n\nZhu, Y.-Q. and Chang, J.-H. (2016). The key role of relevance in personalized advertisement: Examining its impact on perceptions of privacy invasion, self-awareness, and continuous use intentions. Computers in Human Behavior, 65, pp.442\u2013447. doi:https://doi.org/10.1016/j.chb.2016.08.048."
    },
    {
      "role": "assistant",
      "content": "{\n    \"feedback\": \"A well researched analysis of AI in the general field of ecommerce. However, though some specific example applications are cited it can be difficult to discuss specific mitigation in the general sense, as trade-off needed in different application context for different commercial offering are different. Nevertheless you capture the general issue well.\",\n    \"description / 20\": 60,\n    \"roles / 10\": 65,\n    \"risks / 40\": 65,\n    \"mitigation / 20\": 65,\n    \"refs / 10\": 70,\n    \"total score\": 64.5\n}"
    }
  ]
]